{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c5e621",
   "metadata": {},
   "source": [
    "### Problem (unicode1): Understanding Unicode\n",
    "\n",
    "#### (a) What Unicode character does chr(0) return?\n",
    "The null character (U+0000).\n",
    "\n",
    "#### (b) How does this character's string representation (__repr__()) differ from its printed representation?\n",
    "The string representation includes escape characters (e.g., \"'\\\\x00'\"), while the printed representation shows the character itself (which is invisible).\n",
    "\n",
    "#### (c) What happens when this character occurs in text?\n",
    "The null character can cause unexpected behavior in text processing, as it is often used as a string terminator in programming languages (such as C or C++). \n",
    "In Python, it can be included in strings, but its presence may lead to confusion or errors when manipulating or displaying the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a48eb43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"'\\\\x00'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(chr(0))\n",
    "\n",
    "display(chr(0).__repr__())\n",
    "print(chr(0))\n",
    "\n",
    "display(\"this is a test\" + chr(0) + \"string\")\n",
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bce760",
   "metadata": {},
   "source": [
    "### Problem (unicode2): Unicode Encodings\n",
    "\n",
    "#### (a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\n",
    "- UTF-8 is more space-efficient for ASCII characters, as it uses a single byte for these characters, while UTF-16 uses two bytes and UTF-32 uses four bytes.\n",
    "- Many existing text corpora and libraries are optimized for UTF-8, making it easier to integrate with other tools and datasets.\n",
    "- Training on byte sequences allows the model to learn subword representations more effectively, as it can capture byte-level patterns that may be lost in character-level representations.\n",
    "\n",
    "#### (b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\n",
    "```python\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "  return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "print(decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\")))\n",
    "```\n",
    "\n",
    "The function is incorrect because it processes each byte individually, which wouldn't make sense because some bytes need to be decoded together to make a character.\n",
    "\n",
    "#### (c) Give a two byte sequence that does not decode to any Unicode character(s).\n",
    "b'\\x80\\x80'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f9c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Error decoding bytes: 'utf-8' codec can't decode byte 0xf0 in position 0: unexpected end of data\n",
      "Error decoding bytes: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "\n",
    "print(decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\")))\n",
    "\n",
    "try:\n",
    "    print(decode_utf8_bytes_to_str_wrong(\"🎉\".encode(\"utf-8\")))\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error decoding bytes: {e}\")\n",
    "\n",
    "try:\n",
    "    b\"\\x80\\x80\".decode(\"utf-8\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error decoding bytes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e3b4a",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_tinystories): BPE Training on TinyStories\n",
    "\n",
    "#### (a) Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. Make sure to add the TinyStories `<|endoftext|>` special token to the vocabulary. Serialize the resulting vocabulary and merges to disk for further inspection. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?\n",
    "Resource requirements: ≤ 30 minutes (no GPUs), ≤ 30GB RAM  \n",
    "**Hint**  You should be able to get under 2 minutes for BPE training using multiprocessing during pretokenization and the following two facts:\n",
    "- (a) The <|endoftext|> token delimits documents in the data files.\n",
    "- (b) The <|endoftext|> token is handled as a special case before the BPE merges are applied.\n",
    "\n",
    "On MacAir M4, training took approximately 50 seconds, and peak memory usage was around 2GB.  \n",
    "The longest token in the vocabulary is: b' accomplishment'. It makes sense since it is a common word in the dataset.\n",
    "\n",
    "#### (b) Profile your code. What part of the tokenizer training process takes the most time?\n",
    "Using `scalene` to profile shows that `pretokenization` takes the most time(approximately 90%+ of the total training time).  \n",
    "The printed text(cProfile) also shows the time needed to pretokenize takes the most time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f76d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-16 18:13:09.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mtrain_bpe_tokenizer\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mTraining BPE tokenizer on data/TinyStoriesV2-GPT4-valid.txt, vocab size: 10000, special tokens: ['<|endoftext|>']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokenization took 0.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-16 18:13:10.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mtrain_bpe_tokenizer\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTrained BPE tokenizer with vocab size: 10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 10000, set vocab size: 10000\n",
      "         1984031 function calls (1983696 primitive calls) in 0.923 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       25    0.000    0.000    0.671    0.027 connection.py:1134(wait)\n",
      "        1    0.000    0.000    0.476    0.476 pretokenization.py:106(pretokenize_file)\n",
      "        1    0.000    0.000    0.468    0.468 pool.py:738(__exit__)\n",
      "        1    0.000    0.000    0.467    0.467 pool.py:654(terminate)\n",
      "       14    0.000    0.000    0.448    0.032 connection.py:253(poll)\n",
      "       15    0.000    0.000    0.441    0.029 util.py:197(__call__)\n",
      "        1    0.000    0.000    0.440    0.440 pool.py:680(_terminate_pool)\n",
      "        1    0.003    0.003    0.440    0.440 788761672.py:1(<module>)\n",
      "       25    0.000    0.000    0.438    0.018 selectors.py:385(select)\n",
      "       25    0.001    0.000    0.437    0.017 {method 'poll' of 'select.poll' objects}\n",
      "       14    0.000    0.000    0.437    0.031 connection.py:439(_poll)\n",
      "        1    0.000    0.000    0.437    0.437 pool.py:671(_help_stuff_finish)\n",
      "        1    0.000    0.000    0.437    0.437 {method 'acquire' of '_multiprocessing.SemLock' objects}\n",
      "      3/1    0.000    0.000    0.437    0.437 threading.py:998(_bootstrap)\n",
      "      3/1    0.000    0.000    0.437    0.437 threading.py:1025(_bootstrap_inner)\n",
      "      3/1    0.000    0.000    0.437    0.437 ipkernel.py:750(run_closure)\n",
      "      3/1    0.000    0.000    0.436    0.436 threading.py:981(run)\n",
      "        1    0.000    0.000    0.436    0.436 pool.py:573(_handle_results)\n",
      "        1    0.000    0.000    0.435    0.435 pool.py:369(starmap)\n",
      "       11    0.000    0.000    0.417    0.038 pool.py:500(_wait_for_updates)\n",
      "    22/18    0.000    0.000    0.233    0.013 connection.py:390(_recv)\n",
      "    30/26    0.233    0.008    0.233    0.009 {built-in method posix.read}\n",
      "     9745    0.152    0.000    0.222    0.000 trainer.py:65(_update_counter)\n",
      "    45/38    0.001    0.000    0.186    0.005 base_events.py:1954(_run_once)\n",
      "    45/38    0.005    0.000    0.175    0.005 selectors.py:540(select)\n",
      "    45/38    0.258    0.006    0.172    0.005 {method 'control' of 'select.kqueue' objects}\n",
      "        1    0.019    0.019    0.132    0.132 trainer.py:124(train)\n",
      "       46    0.000    0.000    0.062    0.001 events.py:87(_run)\n",
      "       46    0.000    0.000    0.061    0.001 {method 'run' of '_contextvars.Context' objects}\n",
      "       44    0.000    0.000    0.055    0.001 zmqstream.py:573(_handle_events)\n",
      "       44    0.000    0.000    0.050    0.001 zmqstream.py:614(_handle_recv)\n",
      "       44    0.000    0.000    0.049    0.001 zmqstream.py:546(_run_callback)\n",
      "       44    0.000    0.000    0.048    0.001 iostream.py:157(_handle_event)\n",
      "        4    0.000    0.000    0.047    0.012 iostream.py:276(<lambda>)\n",
      "        4    0.005    0.001    0.047    0.012 iostream.py:278(_really_send)\n",
      "        9    0.000    0.000    0.046    0.005 asyncio.py:206(_handle_events)\n",
      "    20788    0.020    0.000    0.037    0.000 {built-in method _heapq.heappop}\n",
      "        1    0.000    0.000    0.035    0.035 encoder.py:12(train_bpe_tokenizer)\n",
      "        1    0.004    0.004    0.031    0.031 trainer.py:212(to_files)\n",
      "    45264    0.017    0.000    0.029    0.000 __init__.py:599(__init__)\n",
      "        9    0.000    0.000    0.029    0.003 connection.py:246(recv)\n",
      "       11    0.026    0.002    0.027    0.002 {built-in method _pickle.loads}\n",
      "        1    0.000    0.000    0.026    0.026 pool.py:305(_repopulate_pool)\n",
      "        1    0.000    0.000    0.026    0.026 pool.py:314(_repopulate_pool_static)\n",
      "        8    0.000    0.000    0.025    0.003 process.py:110(start)\n",
      "        8    0.000    0.000    0.024    0.003 context.py:286(_Popen)\n",
      "        8    0.004    0.000    0.024    0.003 popen_spawn_posix.py:30(__init__)\n",
      "   370064    0.021    0.000    0.021    0.000 trainer.py:19(__lt__)\n",
      "   297941    0.019    0.000    0.019    0.000 {method 'append' of 'list' objects}\n",
      "   473338    0.019    0.000    0.019    0.000 {built-in method builtins.len}\n",
      "        1    0.005    0.005    0.017    0.017 __init__.py:120(dump)\n",
      "        8    0.000    0.000    0.016    0.002 popen_fork.py:16(__init__)\n",
      "       37    0.000    0.000    0.015    0.000 ioloop.py:750(_run_callback)\n",
      "    29520    0.010    0.000    0.015    0.000 {method 'join' of 'str' objects}\n",
      "     9745    0.002    0.000    0.014    0.000 __init__.py:734(copy)\n",
      "       13    0.000    0.000    0.012    0.001 queues.py:372(empty)\n",
      "    45264    0.005    0.000    0.011    0.000 __init__.py:673(update)\n",
      "    40004    0.003    0.000    0.011    0.000 encoder.py:414(_iterencode)\n",
      "    35868    0.006    0.000    0.010    0.000 {built-in method _heapq.heappush}\n",
      "        8    0.000    0.000    0.009    0.001 popen_spawn_posix.py:38(_launch)\n",
      "       35    0.000    0.000    0.009    0.000 zmqstream.py:684(<lambda>)\n",
      "        8    0.007    0.001    0.008    0.001 __init__.py:928(__iadd__)\n",
      "       18    0.002    0.000    0.008    0.000 iostream.py:592(flush)\n",
      "       43    0.001    0.000    0.008    0.000 iostream.py:259(schedule)\n",
      "       20    0.000    0.000    0.007    0.000 iostream.py:616(_flush)\n",
      "        8    0.000    0.000    0.007    0.001 util.py:425(_flush_std_streams)\n",
      "    40004    0.005    0.000    0.007    0.000 encoder.py:334(_iterencode_dict)\n",
      "   224321    0.007    0.000    0.007    0.000 __init__.py:613(__missing__)\n",
      "        3    0.000    0.000    0.007    0.002 session.py:754(send)\n",
      "        8    0.000    0.000    0.007    0.001 util.py:439(spawnv_passfds)\n",
      "        3    0.000    0.000    0.007    0.002 iostream.py:271(send_multipart)\n",
      "        8    0.006    0.001    0.006    0.001 {built-in method _posixsubprocess.fork_exec}\n",
      "41382/41292    0.003    0.000    0.005    0.000 {built-in method builtins.isinstance}\n",
      "       11    0.000    0.000    0.005    0.000 pool.py:333(_maintain_pool)\n",
      "    49746    0.004    0.000    0.004    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n",
      "     9798    0.004    0.000    0.004    0.000 {method 'update' of 'dict' objects}\n",
      "       16    0.000    0.000    0.003    0.000 reduction.py:58(dump)\n",
      "       37    0.002    0.000    0.003    0.000 {method 'dump' of '_pickle.Pickler' objects}\n",
      "      135    0.000    0.000    0.003    0.000 popen_fork.py:25(poll)\n",
      "       89    0.000    0.000    0.003    0.000 attrsettr.py:43(__getattr__)\n",
      "        4    0.001    0.000    0.003    0.001 socket.py:700(send_multipart)\n",
      "        3    0.000    0.000    0.003    0.001 process.py:142(join)\n",
      "        3    0.000    0.000    0.003    0.001 popen_fork.py:37(wait)\n",
      "      135    0.003    0.000    0.003    0.000 {built-in method posix.waitpid}\n",
      "    67573    0.002    0.000    0.002    0.000 trainer.py:223(<genexpr>)\n",
      "       45    0.000    0.000    0.002    0.000 zmqstream.py:653(_rebuild_io_state)\n",
      "       21    0.000    0.000    0.002    0.000 reduction.py:48(dumps)\n",
      "     9753    0.001    0.000    0.002    0.000 <frozen abc>:117(__instancecheck__)\n",
      "       89    0.002    0.000    0.002    0.000 attrsettr.py:66(_get_attr_opt)\n",
      "       37    0.000    0.000    0.002    0.000 reduction.py:38(__init__)\n",
      "       45    0.000    0.000    0.002    0.000 zmqstream.py:676(_update_handler)\n",
      "    41001    0.002    0.000    0.002    0.000 trainer.py:233(<genexpr>)\n",
      "    19553    0.002    0.000    0.002    0.000 {method 'items' of 'dict' objects}\n",
      "       16    0.000    0.000    0.002    0.000 connection.py:202(send)\n",
      "     11/9    0.000    0.000    0.001    0.000 connection.py:429(_recv_bytes)\n",
      "    35789    0.001    0.000    0.001    0.000 trainer.py:234(<genexpr>)\n",
      "    25755    0.001    0.000    0.001    0.000 <string>:2(__init__)\n",
      "       30    0.001    0.000    0.001    0.000 {method 'write' of '_io.BytesIO' objects}\n",
      "        1    0.001    0.001    0.001    0.001 tokenizer.py:15(__init__)\n",
      "        8    0.001    0.000    0.001    0.000 __init__.py:921(_keep_positive)\n",
      "       44    0.001    0.000    0.001    0.000 socket.py:771(recv_multipart)\n",
      "     9762    0.001    0.000    0.001    0.000 {method 'add' of 'set' objects}\n",
      "       11    0.001    0.000    0.001    0.000 {built-in method _io.open}\n",
      "     9753    0.001    0.000    0.001    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        5    0.000    0.000    0.001    0.000 queues.py:389(put)\n",
      "        8    0.000    0.000    0.001    0.000 spawn.py:160(get_preparation_data)\n",
      "       50    0.001    0.000    0.001    0.000 {method 'copy' of 'dict' objects}\n",
      "        9    0.000    0.000    0.001    0.000 pool.py:385(_guarded_task_generation)\n",
      "    10000    0.001    0.000    0.001    0.000 {built-in method _json.encode_basestring}\n",
      "        9    0.001    0.000    0.001    0.000 pool.py:633(_get_tasks)\n",
      "        8    0.001    0.000    0.001    0.000 {built-in method posix.getcwd}\n",
      "        2    0.000    0.000    0.001    0.000 _datetime.py:143(aware_now)\n",
      "        8    0.000    0.000    0.001    0.000 resource_tracker.py:126(getfd)\n",
      "      120    0.000    0.000    0.001    0.000 ipkernel.py:781(_clean_thread_parent_frames)\n",
      "      134    0.000    0.000    0.001    0.000 enum.py:1605(__and__)\n",
      "        6    0.000    0.000    0.001    0.000 interactiveshell.py:3043(write)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:592(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 pool.py:179(Process)\n",
      "       32    0.000    0.000    0.000    0.000 synchronize.py:100(__getstate__)\n",
      "       35    0.000    0.000    0.000    0.000 asyncio.py:231(add_callback)\n",
      "      132    0.000    0.000    0.000    0.000 typing.py:426(inner)\n",
      "        6    0.000    0.000    0.000    0.000 iostream.py:655(write)\n",
      "        3    0.000    0.000    0.000    0.000 context.py:110(SimpleQueue)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:80(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 ipkernel.py:774(init_closure)\n",
      "      124    0.000    0.000    0.000    0.000 selectors.py:340(register)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:345(_setup_queues)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:281(__init__)\n",
      "       11    0.000    0.000    0.000    0.000 pool.py:289(_join_exited_workers)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:869(__init__)\n",
      "       11    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "        3    0.000    0.000    0.000    0.000 queues.py:359(__init__)\n",
      "       15    0.000    0.000    0.000    0.000 util.py:178(__init__)\n",
      "      615    0.000    0.000    0.000    0.000 enum.py:1587(_get_value)\n",
      "       21    0.000    0.000    0.000    0.000 connection.py:406(_send_bytes)\n",
      "        6    0.000    0.000    0.000    0.000 context.py:65(Lock)\n",
      "       23    0.000    0.000    0.000    0.000 iostream.py:710(_flush_buffers)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:168(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:50(__init__)\n",
      "       45    0.000    0.000    0.000    0.000 typing.py:1374(__instancecheck__)\n",
      "       96    0.000    0.000    0.000    0.000 process.py:224(exitcode)\n",
      "      124    0.000    0.000    0.000    0.000 selectors.py:238(register)\n",
      "       35    0.000    0.000    0.000    0.000 base_events.py:817(call_soon)\n",
      "       20    0.000    0.000    0.000    0.000 iostream.py:718(_rotate_buffers)\n",
      "       32    0.000    0.000    0.000    0.000 connection.py:1184(reduce_connection)\n",
      "      338    0.000    0.000    0.000    0.000 enum.py:695(__call__)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:620(set)\n",
      "       45    0.000    0.000    0.000    0.000 typing.py:1665(__subclasscheck__)\n",
      "       54    0.000    0.000    0.000    0.000 {built-in method posix.close}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method now}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "      287    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method posix.waitstatus_to_exitcode}\n",
      "       35    0.000    0.000    0.000    0.000 base_events.py:846(_call_soon)\n",
      "        5    0.000    0.000    0.000    0.000 traitlets.py:708(__set__)\n",
      "        8    0.000    0.000    0.000    0.000 spawn.py:83(get_command_line)\n",
      "        3    0.000    0.000    0.000    0.000 session.py:690(serialize)\n",
      "        6    0.000    0.000    0.000    0.000 iostream.py:577(_schedule_flush)\n",
      "       71    0.000    0.000    0.000    0.000 enum.py:1594(__or__)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:428(notify_all)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method time.localtime}\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:725(_hooks)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _heapq.heapify}\n",
      "       12    0.000    0.000    0.000    0.000 resource_tracker.py:218(_send)\n",
      "       60    0.000    0.000    0.000    0.000 threading.py:1477(enumerate)\n",
      "       45    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3631(set)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:953(start)\n",
      "       63    0.000    0.000    0.000    0.000 threading.py:1134(is_alive)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:398(notify)\n",
      "       45    0.000    0.000    0.000    0.000 zmqstream.py:532(sending)\n",
      "        5    0.000    0.000    0.000    0.000 traitlets.py:689(set)\n",
      "        1    0.000    0.000    0.000    0.000 trainer.py:41(_init_vocab)\n",
      "       62    0.000    0.000    0.000    0.000 base_events.py:766(time)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:506(_handle_workers)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _thread.start_joinable_thread}\n",
      "        6    0.000    0.000    0.000    0.000 resource_tracker.py:210(register)\n",
      "       25    0.000    0.000    0.000    0.000 selectors.py:336(__init__)\n",
      "       12    0.000    0.000    0.000    0.000 session.py:92(json_packer)\n",
      "       97    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1390(_handle_fromlist)\n",
      "       53    0.000    0.000    0.000    0.000 {built-in method posix.write}\n",
      "        8    0.000    0.000    0.000    0.000 subprocess.py:306(_args_from_interpreter_flags)\n",
      "       46    0.000    0.000    0.000    0.000 selector_events.py:740(_process_events)\n",
      "       20    0.000    0.000    0.000    0.000 resource_tracker.py:130(ensure_running)\n",
      "       12    0.000    0.000    0.000    0.000 __init__.py:183(dumps)\n",
      "      269    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "       16    0.000    0.000    0.000    0.000 process.py:347(__reduce__)\n",
      "       88    0.000    0.000    0.000    0.000 typing.py:1443(__hash__)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:527(_handle_tasks)\n",
      "      124    0.000    0.000    0.000    0.000 selectors.py:219(_fileobj_lookup)\n",
      "       39    0.000    0.000    0.000    0.000 events.py:36(__init__)\n",
      "       32    0.000    0.000    0.000    0.000 reduction.py:191(DupFd)\n",
      "       45    0.000    0.000    0.000    0.000 <frozen abc>:121(__subclasscheck__)\n",
      "       11    0.000    0.000    0.000    0.000 pool.py:284(_get_worker_sentinels)\n",
      "      482    0.000    0.000    0.000    0.000 threading.py:1110(ident)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:61(_cleanup)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:471(_map_async)\n",
      "       32    0.000    0.000    0.000    0.000 threading.py:1427(current_thread)\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method posix.pipe}\n",
      "       45    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "       12    0.000    0.000    0.000    0.000 encoder.py:183(encode)\n",
      "      111    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "       43    0.000    0.000    0.000    0.000 iostream.py:138(_event_pipe)\n",
      "       44    0.000    0.000    0.000    0.000 threading.py:318(_is_owned)\n",
      "       45    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "        5    0.000    0.000    0.000    0.000 traitlets.py:718(_validate)\n",
      "      338    0.000    0.000    0.000    0.000 enum.py:1154(__new__)\n",
      "       25    0.000    0.000    0.000    0.000 selectors.py:213(__init__)\n",
      "       45    0.000    0.000    0.000    0.000 queue.py:115(empty)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:587(_schedule_in_thread)\n",
      "      124    0.000    0.000    0.000    0.000 selectors.py:21(_fileobj_to_fd)\n",
      "       38    0.000    0.000    0.000    0.000 {method 'getbuffer' of '_io.BytesIO' objects}\n",
      "       25    0.000    0.000    0.000    0.000 selectors.py:206(__exit__)\n",
      "       13    0.000    0.000    0.000    0.000 encoder.py:205(iterencode)\n",
      "       21    0.000    0.000    0.000    0.000 connection.py:381(_send)\n",
      "       20    0.000    0.000    0.000    0.000 resource_tracker.py:199(_check_alive)\n",
      "       16    0.000    0.000    0.000    0.000 queues.py:375(__getstate__)\n",
      "       60    0.000    0.000    0.000    0.000 traitlets.py:676(__get__)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:84(_cleanup)\n",
      "        4    0.000    0.000    0.000    0.000 ioloop.py:604(call_later)\n",
      "        8    0.000    0.000    0.000    0.000 pool.py:809(_set)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:121(_make_name)\n",
      "        3    0.000    0.000    0.000    0.000 session.py:649(msg)\n",
      "       25    0.000    0.000    0.000    0.000 selectors.py:272(close)\n",
      "        1    0.000    0.000    0.000    0.000 context.py:115(Pool)\n",
      "        2    0.000    0.000    0.000    0.000 codeop.py:113(__call__)\n",
      "       11    0.000    0.000    0.000    0.000 _weakrefset.py:85(add)\n",
      "       54    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:645(parent)\n",
      "      124    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
      "       45    0.000    0.000    0.000    0.000 threading.py:303(__enter__)\n",
      "        4    0.000    0.000    0.000    0.000 asyncio.py:216(call_at)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "        8    0.000    0.000    0.000    0.000 process.py:128(terminate)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1265(_make_invoke_excepthook)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1056(join)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        3    0.000    0.000    0.000    0.000 session.py:675(sign)\n",
      "        8    0.000    0.000    0.000    0.000 popen_fork.py:57(terminate)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:161(__delitem__)\n",
      "        6    0.000    0.000    0.000    0.000 tempfile.py:153(__next__)\n",
      "        3    0.000    0.000    0.000    0.000 session.py:645(msg_header)\n",
      "       43    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
      "        8    0.000    0.000    0.000    0.000 popen_fork.py:47(_send_signal)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'join' of '_thread._ThreadHandle' objects}\n",
      "        4    0.000    0.000    0.000    0.000 base_events.py:775(call_later)\n",
      "        6    0.000    0.000    0.000    0.000 resource_tracker.py:214(unregister)\n",
      "       68    0.000    0.000    0.000    0.000 connection.py:169(fileno)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method posix.kill}\n",
      "      176    0.000    0.000    0.000    0.000 process.py:247(sentinel)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:796(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 util.py:163(register_after_fork)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3474(validate)\n",
      "      115    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:183(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'write' of '_io.BufferedWriter' objects}\n",
      "       96    0.000    0.000    0.000    0.000 context.py:366(get_spawning_popen)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1512(_notify_trait)\n",
      "        8    0.000    0.000    0.000    0.000 subprocess.py:296(_optim_args_from_interpreter_flags)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:747(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 base_events.py:799(call_at)\n",
      "        8    0.000    0.000    0.000    0.000 spawn.py:138(_check_not_importing_main)\n",
      "        6    0.000    0.000    0.000    0.000 jsonutil.py:107(json_default)\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:164(__setitem__)\n",
      "       60    0.000    0.000    0.000    0.000 traitlets.py:629(get)\n",
      "        5    0.000    0.000    0.000    0.000 connection.py:182(send_bytes)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:145(__exit__)\n",
      "      304    0.000    0.000    0.000    0.000 process.py:99(_check_closed)\n",
      "        3    0.000    0.000    0.000    0.000 connection.py:533(Pipe)\n",
      "        1    0.000    0.000    0.000    0.000 pretokenization.py:9(find_chunk_boundaries)\n",
      "       90    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
      "       44    0.000    0.000    0.000    0.000 threading.py:306(__exit__)\n",
      "       48    0.000    0.000    0.000    0.000 context.py:372(assert_spawning)\n",
      "      240    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "        3    0.000    0.000    0.000    0.000 session.py:272(msg_header)\n",
      "       39    0.000    0.000    0.000    0.000 {built-in method _contextvars.copy_context}\n",
      "        6    0.000    0.000    0.000    0.000 random.py:458(choices)\n",
      "      133    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x102f80048}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'format_map' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "     22/3    0.000    0.000    0.000    0.000 threading.py:641(wait)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:153(is_alive)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:843(_newname)\n",
      "        5    0.000    0.000    0.000    0.000 traitlets.py:727(_cross_validate)\n",
      "      124    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "      124    0.000    0.000    0.000    0.000 {method 'register' of 'select.poll' objects}\n",
      "       89    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "       88    0.000    0.000    0.000    0.000 {built-in method time.monotonic}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'isoformat' of 'datetime.datetime' objects}\n",
      "        8    0.000    0.000    0.000    0.000 util.py:455(close_fds)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1160(daemon)\n",
      "        2    0.000    0.000    0.000    0.000 queues.py:383(get)\n",
      "        4    0.000    0.000    0.000    0.000 contextlib.py:303(helper)\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:131(__del__)\n",
      "        4    0.000    0.000    0.000    0.000 contextlib.py:136(__enter__)\n",
      "       16    0.000    0.000    0.000    0.000 context.py:369(set_spawning_popen)\n",
      "     75/6    0.001    0.000    0.000    0.000 socket.py:623(send)\n",
      "        7    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "      114    0.000    0.000    0.000    0.000 connection.py:135(_check_closed)\n",
      "       21    0.000    0.000    0.000    0.000 {built-in method _struct.pack}\n",
      "       25    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1523(notify_change)\n",
      "       45    0.000    0.000    0.000    0.000 queue.py:267(_qsize)\n",
      "     21/3    0.000    0.000    0.000    0.000 threading.py:327(wait)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'seek' of '_io.BufferedReader' objects}\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:376(_close)\n",
      "        3    0.000    0.000    0.000    0.000 session.py:198(utcnow)\n",
      "        3    0.000    0.000    0.000    0.000 hmac.py:122(copy)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:767(get)\n",
      "       93    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "       54    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:279(_get_sentinels)\n",
      "        4    0.000    0.000    0.000    0.000 events.py:113(__init__)\n",
      "       24    0.000    0.000    0.000    0.000 spawn.py:92(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 history.py:1016(_writeout_output_cache)\n",
      "       32    0.000    0.000    0.000    0.000 popen_spawn_posix.py:34(duplicate_for_child)\n",
      "       54    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "       84    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "       88    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:2304(validate)\n",
      "        4    0.000    0.000    0.000    0.000 contextlib.py:108(__init__)\n",
      "       21    0.000    0.000    0.000    0.000 threading.py:315(_acquire_restore)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1527(_notify_observers)\n",
      "        7    0.000    0.000    0.000    0.000 synchronize.py:94(__enter__)\n",
      "       45    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:764(wait)\n",
      "        4    0.000    0.000    0.000    0.000 _handler.py:110(_protected_lock)\n",
      "   108/14    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'copy' of 'list' objects}\n",
      "       42    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        9    0.000    0.000    0.000    0.000 base_events.py:1939(_add_callback)\n",
      "       89    0.000    0.000    0.000    0.000 zmqstream.py:528(receiving)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen posixpath>:166(basename)\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "        3    0.000    0.000    0.000    0.000 session.py:600(msg_id)\n",
      "        2    0.000    0.000    0.000    0.000 connection.py:208(recv_bytes)\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:347(__new__)\n",
      "       63    0.000    0.000    0.000    0.000 {method 'is_done' of '_thread._ThreadHandle' objects}\n",
      "       14    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 _datetime.py:139(__format__)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:312(_release_save)\n",
      "      183    0.000    0.000    0.000    0.000 typing.py:2371(cast)\n",
      "        3    0.000    0.000    0.000    0.000 hmac.py:161(hexdigest)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'copy' of '_hashlib.HMAC' objects}\n",
      "        1    0.000    0.000    0.000    0.000 context.py:41(cpu_count)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3624(validate_elements)\n",
      "        7    0.000    0.000    0.000    0.000 synchronize.py:97(__exit__)\n",
      "       41    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
      "       16    0.000    0.000    0.000    0.000 process.py:94(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen posixpath>:117(splitext)\n",
      "        7    0.000    0.000    0.000    0.000 {method '__enter__' of '_multiprocessing.SemLock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'put' of '_queue.SimpleQueue' objects}\n",
      "       12    0.000    0.000    0.000    0.000 hmac.py:117(update)\n",
      "        2    0.000    0.000    0.000    0.000 _datetime.py:27(_default_datetime_formatter)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:734(__enter__)\n",
      "        8    0.000    0.000    0.000    0.000 ioloop.py:549(time)\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method _multiprocessing.sem_unlink}\n",
      "        6    0.000    0.000    0.000    0.000 tempfile.py:142(rng)\n",
      "       35    0.000    0.000    0.000    0.000 {built-in method _asyncio.get_running_loop}\n",
      "       22    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
      "       28    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
      "       73    0.000    0.000    0.000    0.000 threading.py:605(is_set)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:193(name)\n",
      "        6    0.000    0.000    0.000    0.000 iostream.py:505(parent_header)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.locals}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method _struct.unpack}\n",
      "       21    0.000    0.000    0.000    0.000 {method 'remove' of 'collections.deque' objects}\n",
      "        3    0.000    0.000    0.000    0.000 session.py:281(extract_header)\n",
      "        3    0.000    0.000    0.000    0.000 traitlets.py:2635(validate)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'hexdigest' of '_hashlib.HMAC' objects}\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:352(__init__)\n",
      "       12    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
      "        5    0.000    0.000    0.000    0.000 process.py:234(ident)\n",
      "       25    0.000    0.000    0.000    0.000 {built-in method select.poll}\n",
      "       11    0.000    0.000    0.000    0.000 _weakrefset.py:39(_remove)\n",
      "        7    0.000    0.000    0.000    0.000 {method '__exit__' of '_multiprocessing.SemLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method posix.cpu_count}\n",
      "       39    0.000    0.000    0.000    0.000 base_events.py:548(_check_closed)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1047(_delete)\n",
      "       16    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1022(_set_native_id)\n",
      "       12    0.000    0.000    0.000    0.000 {method 'update' of '_hashlib.HMAC' objects}\n",
      "       20    0.000    0.000    0.000    0.000 {method '_recursion_count' of '_thread.RLock' objects}\n",
      "       19    0.000    0.000    0.000    0.000 {method 'discard' of 'set' objects}\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:105(remove)\n",
      "       32    0.000    0.000    0.000    0.000 popen_spawn_posix.py:17(__init__)\n",
      "       34    0.000    0.000    0.000    0.000 util.py:48(debug)\n",
      "       39    0.000    0.000    0.000    0.000 base_events.py:2052(get_debug)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:216(_check_mp_mode)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'index' of 'list' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'find' of 'bytes' objects}\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1018(_set_ident)\n",
      "        6    0.000    0.000    0.000    0.000 iostream.py:550(_is_master_process)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:213(authkey)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method combine}\n",
      "        2    0.000    0.000    0.000    0.000 <frozen genericpath>:157(_splitext)\n",
      "       24    0.000    0.000    0.000    0.000 process.py:37(current_process)\n",
      "       32    0.000    0.000    0.000    0.000 connection.py:164(writable)\n",
      "       13    0.000    0.000    0.000    0.000 encoder.py:105(__init__)\n",
      "       48    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n",
      "       18    0.000    0.000    0.000    0.000 process.py:189(name)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:16(__format__)\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:118(__init__)\n",
      "       25    0.000    0.000    0.000    0.000 selectors.py:63(__init__)\n",
      "       32    0.000    0.000    0.000    0.000 connection.py:159(readable)\n",
      "        8    0.000    0.000    0.000    0.000 context.py:253(get_start_method)\n",
      "        1    0.000    0.000    0.000    0.000 trainer.py:58(_finalize_vocab)\n",
      "       25    0.000    0.000    0.000    0.000 connection.py:139(_check_readable)\n",
      "       48    0.000    0.000    0.000    0.000 {built-in method math.floor}\n",
      "        6    0.000    0.000    0.000    0.000 threading.py:1145(daemon)\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "       13    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.BytesIO' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:213(_is_master_process)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen posixpath>:42(_get_sep)\n",
      "       25    0.000    0.000    0.000    0.000 selectors.py:203(__enter__)\n",
      "        4    0.000    0.000    0.000    0.000 compilerop.py:180(extra_flags)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'tell' of '_io.BufferedReader' objects}\n",
      "        8    0.000    0.000    0.000    0.000 process.py:205(daemon)\n",
      "        4    0.000    0.000    0.000    0.000 events.py:129(__lt__)\n",
      "       21    0.000    0.000    0.000    0.000 connection.py:143(_check_writable)\n",
      "        6    0.000    0.000    0.000    0.000 jsonutil.py:38(_ensure_tzinfo)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:90(_make_methods)\n",
      "        2    0.000    0.000    0.000    0.000 {method '__format__' of 'str' objects}\n",
      "       15    0.000    0.000    0.000    0.000 util.py:44(sub_debug)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'timestamp' of 'datetime.datetime' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}\n",
      "       16    0.000    0.000    0.000    0.000 spawn.py:45(get_executable)\n",
      "        2    0.000    0.000    0.000    0.000 threading.py:1094(name)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'replace' of 'datetime.time' objects}\n",
      "        1    0.000    0.000    0.000    0.000 trainer.py:32(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
      "        3    0.000    0.000    0.000    0.000 traitlets.py:2558(_validate_bounds)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _thread.daemon_threads_allowed}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method _weakref._remove_dead_weakref}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:157(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _thread.get_native_id}\n",
      "        2    0.000    0.000    0.000    0.000 interactiveshell.py:3615(compare)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'time' of 'datetime.datetime' objects}\n",
      "        6    0.000    0.000    0.000    0.000 displaypub.py:150(is_publishing)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3486(validate_elements)\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'date' of 'datetime.datetime' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'close' of '_io.StringIO' objects}\n",
      "        9    0.000    0.000    0.000    0.000 context.py:187(get_context)\n",
      "        1    0.000    0.000    0.000    0.000 encoder.py:263(_make_iterencode)\n",
      "        4    0.000    0.000    0.000    0.000 tz.py:74(utcoffset)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method sys._getframe}\n",
      "        6    0.000    0.000    0.000    0.000 context.py:197(get_start_method)\n",
      "        2    0.000    0.000    0.000    0.000 interactiveshell.py:1299(user_global_ns)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:266(__del__)\n",
      "        2    0.000    0.000    0.000    0.000 pool.py:351(_check_running)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen codecs>:189(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:756(ready)\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.ord}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
      "        6    0.000    0.000    0.000    0.000 displayhook.py:118(is_active)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:255(closed)\n",
      "        2    0.000    0.000    0.000    0.000 interactiveshell.py:685(user_ns)\n",
      "        1    0.000    0.000    0.000    0.000 context.py:237(get_context)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:23(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 hmac.py:139(_current)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:8(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:51(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:37(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 history.py:1065(hold)\n",
      "      1/0    0.000    0.000    0.000          interactiveshell.py:3663(run_code)\n",
      "      1/0    0.000    0.000    0.000          {built-in method builtins.exec}\n",
      "      1/0    0.000    0.000    0.000          _simple_sinks.py:15(write)\n",
      "      1/0    0.000    0.000    0.000          _logger.py:2076(info)\n",
      "      1/0    0.000    0.000    0.000          _handler.py:127(emit)\n",
      "      1/0    0.000    0.000    0.000          _logger.py:1931(_log)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "\n",
    "from cs336_basics.bpe_tokenizer.encoder import train_bpe_tokenizer\n",
    "\n",
    "input_path = \"data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "vocab_path = \"data/TinyStoriesV2-GPT4_vocab.json\"\n",
    "merge_path = \"data/TinyStoriesV2-GPT4_merges.txt\"\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "tokenizer = train_bpe_tokenizer(\n",
    "    input_path=input_path,\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    save=True,\n",
    "    vocab_path=vocab_path,\n",
    "    merge_path=merge_path,\n",
    ")\n",
    "\n",
    "profiler.disable()\n",
    "profiler.print_stats(sort=\"cumtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d7029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-16 21:43:31.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mtrain_bpe_tokenizer\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mTraining BPE tokenizer on data/TinyStoriesV2-GPT4-train.txt, vocab size: 10000, special tokens: ['<|endoftext|>'], num chunks: 8\u001b[0m\n",
      "\u001b[32m2025-08-16 21:44:16.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mtrain_bpe_tokenizer\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTrained BPE tokenizer with vocab size: 10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 10000, set vocab size: 10000\n",
      "         5722411 function calls (5722270 primitive calls) in 45.257 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       69    0.000    0.000   98.016    1.421 connection.py:1134(wait)\n",
      "       69    0.001    0.000   97.836    1.418 selectors.py:385(select)\n",
      "       33    0.000    0.000   87.507    2.652 pool.py:500(_wait_for_updates)\n",
      "       53    0.017    0.000   78.543    1.482 base_events.py:1954(_run_once)\n",
      "       69   10.237    0.148   54.061    0.783 {method 'poll' of 'select.poll' objects}\n",
      "      3/2    0.000    0.000   45.256   22.628 interactiveshell.py:3663(run_code)\n",
      "        2    0.000    0.000   45.256   22.628 {built-in method builtins.exec}\n",
      "        1    0.001    0.001   45.256   45.256 3166285396.py:1(<module>)\n",
      "        1    0.232    0.232   45.194   45.194 trainer.py:112(train)\n",
      "        1    0.000    0.000   43.886   43.886 pretokenization.py:109(pretokenize_file_to_counter)\n",
      "        1    0.000    0.000   43.885   43.885 pool.py:738(__exit__)\n",
      "        1    0.000    0.000   43.885   43.885 pool.py:654(terminate)\n",
      "       36    0.000    0.000   43.843    1.218 connection.py:253(poll)\n",
      "       36    0.000    0.000   43.837    1.218 connection.py:439(_poll)\n",
      "       15    0.000    0.000   43.832    2.922 util.py:197(__call__)\n",
      "        1    0.000    0.000   43.831   43.831 pool.py:680(_terminate_pool)\n",
      "        1    0.000    0.000   43.823   43.823 pool.py:671(_help_stuff_finish)\n",
      "        1    0.000    0.000   43.823   43.823 {method 'acquire' of '_multiprocessing.SemLock' objects}\n",
      "      3/1    0.000    0.000   43.823   43.823 threading.py:998(_bootstrap)\n",
      "      3/1    0.000    0.000   43.823   43.823 threading.py:1025(_bootstrap_inner)\n",
      "      3/1    0.005    0.002   43.823   43.823 ipkernel.py:750(run_closure)\n",
      "      3/1    0.000    0.000   43.823   43.823 threading.py:981(run)\n",
      "        1    0.000    0.000   43.823   43.823 pool.py:573(_handle_results)\n",
      "       33    0.000    0.000   43.096    1.306 pool.py:333(_maintain_pool)\n",
      "       33    0.000    0.000   43.089    1.306 pool.py:289(_join_exited_workers)\n",
      "       53    0.003    0.000   33.318    0.629 selectors.py:540(select)\n",
      "       53   33.306    0.628   33.306    0.628 {method 'control' of 'select.kqueue' objects}\n",
      "     9744    0.677    0.000    0.977    0.000 trainer.py:53(_update_counter)\n",
      "        1    0.000    0.000    0.698    0.698 pool.py:369(starmap)\n",
      "        1    0.000    0.000    0.698    0.698 pool.py:767(get)\n",
      "    86/82    0.179    0.002    0.179    0.002 {built-in method posix.read}\n",
      "    22/18    0.000    0.000    0.179    0.010 connection.py:390(_recv)\n",
      "    88947    0.108    0.000    0.131    0.000 __init__.py:599(__init__)\n",
      "  1372154    0.080    0.000    0.080    0.000 {method 'append' of 'list' objects}\n",
      "  2069983    0.080    0.000    0.080    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.052    0.052 pool.py:305(_repopulate_pool)\n",
      "        1    0.000    0.000    0.052    0.052 pool.py:314(_repopulate_pool_static)\n",
      "        8    0.001    0.000    0.051    0.006 process.py:110(start)\n",
      "        8    0.000    0.000    0.049    0.006 context.py:286(_Popen)\n",
      "        8    0.000    0.000    0.048    0.006 popen_spawn_posix.py:30(__init__)\n",
      "    21941    0.026    0.000    0.047    0.000 {built-in method _heapq.heappop}\n",
      "        8    0.001    0.000    0.042    0.005 popen_fork.py:16(__init__)\n",
      "      272    0.006    0.000    0.040    0.000 process.py:224(exitcode)\n",
      "      312    0.010    0.000    0.039    0.000 popen_fork.py:25(poll)\n",
      "        1    0.000    0.000    0.037    0.037 encoder.py:11(train_bpe_tokenizer)\n",
      "        1    0.005    0.005    0.035    0.035 trainer.py:200(to_files)\n",
      "        9    0.000    0.000    0.032    0.004 connection.py:246(recv)\n",
      "       11    0.030    0.003    0.031    0.003 {built-in method _pickle.loads}\n",
      "   532759    0.031    0.000    0.031    0.000 trainer.py:18(__lt__)\n",
      "   946162    0.030    0.000    0.030    0.000 __init__.py:613(__missing__)\n",
      "        8    0.001    0.000    0.029    0.004 popen_spawn_posix.py:38(_launch)\n",
      "        8    0.001    0.000    0.026    0.003 util.py:439(spawnv_passfds)\n",
      "      312    0.026    0.000    0.026    0.000 {built-in method posix.waitpid}\n",
      "    79510    0.016    0.000    0.026    0.000 {built-in method _heapq.heappush}\n",
      "        8    0.026    0.003    0.026    0.003 {built-in method _posixsubprocess.fork_exec}\n",
      "    88947    0.012    0.000    0.022    0.000 __init__.py:673(update)\n",
      "     9744    0.002    0.000    0.022    0.000 __init__.py:734(copy)\n",
      "        8    0.016    0.002    0.021    0.003 __init__.py:928(__iadd__)\n",
      "       35    0.000    0.000    0.020    0.001 queues.py:372(empty)\n",
      "        1    0.005    0.005    0.020    0.020 __init__.py:120(dump)\n",
      "    29516    0.010    0.000    0.015    0.000 {method 'join' of 'str' objects}\n",
      "        8    0.000    0.000    0.013    0.002 util.py:425(_flush_std_streams)\n",
      "       54    0.000    0.000    0.013    0.000 events.py:87(_run)\n",
      "       54    0.000    0.000    0.012    0.000 {method 'run' of '_contextvars.Context' objects}\n",
      "       18    0.001    0.000    0.012    0.001 iostream.py:592(flush)\n",
      "       43    0.000    0.000    0.011    0.000 zmqstream.py:573(_handle_events)\n",
      "    40004    0.004    0.000    0.011    0.000 encoder.py:414(_iterencode)\n",
      "        4    0.000    0.000    0.007    0.002 process.py:142(join)\n",
      "    40004    0.005    0.000    0.007    0.000 encoder.py:334(_iterencode_dict)\n",
      "        4    0.000    0.000    0.007    0.002 popen_fork.py:37(wait)\n",
      "       32    0.000    0.000    0.006    0.000 ioloop.py:750(_run_callback)\n",
      "     9797    0.006    0.000    0.006    0.000 {method 'update' of 'dict' objects}\n",
      "       31    0.000    0.000    0.006    0.000 zmqstream.py:684(<lambda>)\n",
      "41773/41687    0.003    0.000    0.006    0.000 {built-in method builtins.isinstance}\n",
      "       43    0.000    0.000    0.006    0.000 zmqstream.py:614(_handle_recv)\n",
      "       12    0.000    0.000    0.005    0.000 asyncio.py:206(_handle_events)\n",
      "        8    0.005    0.001    0.005    0.001 __init__.py:921(_keep_positive)\n",
      "       43    0.000    0.000    0.004    0.000 zmqstream.py:546(_run_callback)\n",
      "    49746    0.004    0.000    0.004    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n",
      "       43    0.000    0.000    0.004    0.000 iostream.py:157(_handle_event)\n",
      "    69419    0.003    0.000    0.003    0.000 <string>:2(__init__)\n",
      "       86    0.000    0.000    0.003    0.000 attrsettr.py:43(__getattr__)\n",
      "       43    0.000    0.000    0.003    0.000 zmqstream.py:653(_rebuild_io_state)\n",
      "       86    0.002    0.000    0.003    0.000 attrsettr.py:66(_get_attr_opt)\n",
      "    67874    0.002    0.000    0.002    0.000 trainer.py:211(<genexpr>)\n",
      "        4    0.000    0.000    0.002    0.001 iostream.py:276(<lambda>)\n",
      "        4    0.000    0.000    0.002    0.001 iostream.py:278(_really_send)\n",
      "        4    0.000    0.000    0.002    0.001 socket.py:700(send_multipart)\n",
      "       43    0.000    0.000    0.002    0.000 zmqstream.py:676(_update_handler)\n",
      "     9752    0.001    0.000    0.002    0.000 <frozen abc>:117(__instancecheck__)\n",
      "       11    0.002    0.000    0.002    0.000 {built-in method _io.open}\n",
      "       16    0.000    0.000    0.002    0.000 reduction.py:58(dump)\n",
      "    41840    0.002    0.000    0.002    0.000 trainer.py:221(<genexpr>)\n",
      "       41    0.001    0.000    0.002    0.000 iostream.py:259(schedule)\n",
      "    19553    0.002    0.000    0.002    0.000 {method 'items' of 'dict' objects}\n",
      "       37    0.001    0.000    0.001    0.000 {method 'dump' of '_pickle.Pickler' objects}\n",
      "        1    0.001    0.001    0.001    0.001 tokenizer.py:15(__init__)\n",
      "        8    0.001    0.000    0.001    0.000 process.py:61(_cleanup)\n",
      "    35251    0.001    0.000    0.001    0.000 trainer.py:222(<genexpr>)\n",
      "        2    0.000    0.000    0.001    0.001 _logger.py:2076(info)\n",
      "      344    0.001    0.000    0.001    0.000 ipkernel.py:781(_clean_thread_parent_frames)\n",
      "        2    0.000    0.000    0.001    0.001 _logger.py:1931(_log)\n",
      "        2    0.000    0.000    0.001    0.001 _handler.py:127(emit)\n",
      "      366    0.000    0.000    0.001    0.000 selectors.py:340(register)\n",
      "       19    0.000    0.000    0.001    0.000 iostream.py:616(_flush)\n",
      "       43    0.000    0.000    0.001    0.000 socket.py:771(recv_multipart)\n",
      "        2    0.000    0.000    0.001    0.001 _simple_sinks.py:15(write)\n",
      "       86    0.001    0.000    0.001    0.000 {method 'write' of '_io.BytesIO' objects}\n",
      "     9752    0.001    0.000    0.001    0.000 {built-in method _abc._abc_instancecheck}\n",
      "       69    0.001    0.000    0.001    0.000 socket.py:623(send)\n",
      "     9762    0.001    0.000    0.001    0.000 {method 'add' of 'set' objects}\n",
      "        8    0.000    0.000    0.001    0.000 resource_tracker.py:126(getfd)\n",
      "      129    0.000    0.000    0.001    0.000 enum.py:1605(__and__)\n",
      "    10000    0.001    0.000    0.001    0.000 {built-in method _json.encode_basestring}\n",
      "      366    0.000    0.000    0.001    0.000 selectors.py:238(register)\n",
      "        3    0.000    0.000    0.001    0.000 ipkernel.py:774(init_closure)\n",
      "        1    0.000    0.000    0.001    0.001 decorator.py:232(fun)\n",
      "        1    0.000    0.000    0.001    0.001 history.py:92(only_when_enabled)\n",
      "        3    0.000    0.000    0.001    0.000 threading.py:869(__init__)\n",
      "        8    0.000    0.000    0.001    0.000 pool.py:179(Process)\n",
      "       37    0.000    0.000    0.001    0.000 reduction.py:38(__init__)\n",
      "       80    0.000    0.000    0.001    0.000 base_events.py:766(time)\n",
      "        8    0.000    0.000    0.001    0.000 spawn.py:160(get_preparation_data)\n",
      "      129    0.000    0.000    0.001    0.000 typing.py:426(inner)\n",
      "       31    0.000    0.000    0.001    0.000 asyncio.py:231(add_callback)\n",
      "        8    0.000    0.000    0.001    0.000 process.py:80(__init__)\n",
      "       22    0.000    0.000    0.001    0.000 threading.py:592(__init__)\n",
      "      588    0.000    0.000    0.000    0.000 enum.py:1587(_get_value)\n",
      "       15    0.000    0.000    0.000    0.000 util.py:178(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _heapq.heapify}\n",
      "        3    0.000    0.000    0.000    0.000 context.py:110(SimpleQueue)\n",
      "       36    0.000    0.000    0.000    0.000 base_events.py:817(call_soon)\n",
      "        3    0.000    0.000    0.000    0.000 queues.py:359(__init__)\n",
      "    23/20    0.000    0.000    0.000    0.000 threading.py:641(wait)\n",
      "     11/9    0.000    0.000    0.000    0.000 connection.py:429(_recv_bytes)\n",
      "       16    0.000    0.000    0.000    0.000 connection.py:202(send)\n",
      "      172    0.000    0.000    0.000    0.000 threading.py:1477(enumerate)\n",
      "       21    0.000    0.000    0.000    0.000 reduction.py:48(dumps)\n",
      "       76    0.000    0.000    0.000    0.000 threading.py:1134(is_alive)\n",
      "       33    0.000    0.000    0.000    0.000 pool.py:284(_get_worker_sentinels)\n",
      "      366    0.000    0.000    0.000    0.000 selectors.py:219(_fileobj_lookup)\n",
      "        5    0.000    0.000    0.000    0.000 iostream.py:118(_run_event_pipe_gc)\n",
      "       11    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "        6    0.000    0.000    0.000    0.000 context.py:65(Lock)\n",
      "      325    0.000    0.000    0.000    0.000 enum.py:695(__call__)\n",
      "       21    0.000    0.000    0.000    0.000 iostream.py:710(_flush_buffers)\n",
      "       54    0.000    0.000    0.000    0.000 selector_events.py:740(_process_events)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:168(__init__)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:281(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:50(__init__)\n",
      "       36    0.000    0.000    0.000    0.000 base_events.py:846(_call_soon)\n",
      "       32    0.000    0.000    0.000    0.000 threading.py:1427(current_thread)\n",
      "       43    0.000    0.000    0.000    0.000 typing.py:1374(__instancecheck__)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:620(set)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:345(_setup_queues)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method posix.getcwd}\n",
      "       49    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "    22/19    0.000    0.000    0.000    0.000 threading.py:327(wait)\n",
      "        8    0.000    0.000    0.000    0.000 spawn.py:83(get_command_line)\n",
      "      366    0.000    0.000    0.000    0.000 selectors.py:21(_fileobj_to_fd)\n",
      "       38    0.000    0.000    0.000    0.000 {method 'getbuffer' of '_io.BytesIO' objects}\n",
      "       43    0.000    0.000    0.000    0.000 typing.py:1665(__subclasscheck__)\n",
      "       19    0.000    0.000    0.000    0.000 iostream.py:718(_rotate_buffers)\n",
      "       67    0.000    0.000    0.000    0.000 enum.py:1594(__or__)\n",
      "        5    0.000    0.000    0.000    0.000 queues.py:389(put)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:754(send)\n",
      "      439    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "      285    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:506(_handle_workers)\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:336(__init__)\n",
      "     1272    0.000    0.000    0.000    0.000 threading.py:1110(ident)\n",
      "       20    0.000    0.000    0.000    0.000 resource_tracker.py:130(ensure_running)\n",
      "       32    0.000    0.000    0.000    0.000 connection.py:1184(reduce_connection)\n",
      "       44    0.000    0.000    0.000    0.000 events.py:36(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 tasks.py:703(sleep)\n",
      "        4    0.000    0.000    0.000    0.000 interactiveshell.py:3043(write)\n",
      "      150    0.000    0.000    0.000    0.000 {built-in method time.monotonic}\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:428(notify_all)\n",
      "       43    0.000    0.000    0.000    0.000 zmqstream.py:532(sending)\n",
      "       94    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1390(_handle_fromlist)\n",
      "       54    0.000    0.000    0.000    0.000 {built-in method posix.close}\n",
      "      366    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:206(__exit__)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:690(serialize)\n",
      "       43    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "   110/80    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "      528    0.000    0.000    0.000    0.000 process.py:247(sentinel)\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method posix.pipe}\n",
      "       21    0.000    0.000    0.000    0.000 connection.py:406(_send_bytes)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:953(start)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:398(notify)\n",
      "       53    0.000    0.000    0.000    0.000 {built-in method posix.write}\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:655(write)\n",
      "       86    0.000    0.000    0.000    0.000 typing.py:1443(__hash__)\n",
      "       41    0.000    0.000    0.000    0.000 iostream.py:138(_event_pipe)\n",
      "        8    0.000    0.000    0.000    0.000 subprocess.py:306(_args_from_interpreter_flags)\n",
      "       32    0.000    0.000    0.000    0.000 synchronize.py:100(__getstate__)\n",
      "      325    0.000    0.000    0.000    0.000 enum.py:1154(__new__)\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:272(close)\n",
      "        5    0.000    0.000    0.000    0.000 iostream.py:127(_event_pipe_gc)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:128(terminate)\n",
      "        8    0.000    0.000    0.000    0.000 base_events.py:775(call_later)\n",
      "        4    0.000    0.000    0.000    0.000 traitlets.py:708(__set__)\n",
      "       39    0.000    0.000    0.000    0.000 {built-in method _contextvars.copy_context}\n",
      "        8    0.000    0.000    0.000    0.000 popen_fork.py:57(terminate)\n",
      "      224    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 history.py:1024(writeout_cache)\n",
      "       52    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        5    0.000    0.000    0.000    0.000 futures.py:310(_set_result_unless_cancelled)\n",
      "       12    0.000    0.000    0.000    0.000 resource_tracker.py:218(_send)\n",
      "       43    0.000    0.000    0.000    0.000 queue.py:115(empty)\n",
      "        8    0.000    0.000    0.000    0.000 popen_fork.py:47(_send_signal)\n",
      "      134    0.000    0.000    0.000    0.000 connection.py:169(fileno)\n",
      "       20    0.000    0.000    0.000    0.000 resource_tracker.py:199(_check_alive)\n",
      "       16    0.000    0.000    0.000    0.000 process.py:347(__reduce__)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _thread.start_joinable_thread}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method posix.kill}\n",
      "       32    0.000    0.000    0.000    0.000 reduction.py:191(DupFd)\n",
      "        8    0.000    0.000    0.000    0.000 session.py:92(json_packer)\n",
      "       43    0.000    0.000    0.000    0.000 <frozen abc>:121(__subclasscheck__)\n",
      "       43    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:577(_schedule_flush)\n",
      "        2    0.000    0.000    0.000    0.000 _datetime.py:143(aware_now)\n",
      "        8    0.000    0.000    0.000    0.000 base_events.py:799(call_at)\n",
      "        8    0.000    0.000    0.000    0.000 pool.py:809(_set)\n",
      "      117    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:121(_make_name)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:587(_schedule_in_thread)\n",
      "        8    0.000    0.000    0.000    0.000 __init__.py:183(dumps)\n",
      "       54    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:645(parent)\n",
      "        4    0.000    0.000    0.000    0.000 traitlets.py:689(set)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1056(join)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:84(_cleanup)\n",
      "      688    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "        3    0.000    0.000    0.000    0.000 ioloop.py:604(call_later)\n",
      "      374    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x102a0c048}\n",
      "      834    0.000    0.000    0.000    0.000 process.py:99(_check_closed)\n",
      "        1    0.000    0.000    0.000    0.000 context.py:115(Pool)\n",
      "      366    0.000    0.000    0.000    0.000 {method 'register' of 'select.poll' objects}\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:213(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'set_result' of '_asyncio.Future' objects}\n",
      "        6    0.000    0.000    0.000    0.000 resource_tracker.py:210(register)\n",
      "        2    0.000    0.000    0.000    0.000 codeop.py:113(__call__)\n",
      "        6    0.000    0.000    0.000    0.000 tempfile.py:153(__next__)\n",
      "        5    0.000    0.000    0.000    0.000 connection.py:182(send_bytes)\n",
      "       21    0.000    0.000    0.000    0.000 connection.py:381(_send)\n",
      "        8    0.000    0.000    0.000    0.000 encoder.py:183(encode)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3631(set)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:183(__init__)\n",
      "       69    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
      "        3    0.000    0.000    0.000    0.000 asyncio.py:216(call_at)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'join' of '_thread._ThreadHandle' objects}\n",
      "       44    0.000    0.000    0.000    0.000 traitlets.py:676(__get__)\n",
      "        4    0.000    0.000    0.000    0.000 traitlets.py:718(_validate)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:161(__delitem__)\n",
      "        9    0.000    0.000    0.000    0.000 encoder.py:205(iterencode)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:271(send_multipart)\n",
      "        8    0.000    0.000    0.000    0.000 spawn.py:138(_check_not_importing_main)\n",
      "        2    0.000    0.000    0.000    0.000 queues.py:383(get)\n",
      "        6    0.000    0.000    0.000    0.000 util.py:163(register_after_fork)\n",
      "       43    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "       96    0.000    0.000    0.000    0.000 context.py:366(get_spawning_popen)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        8    0.000    0.000    0.000    0.000 events.py:113(__init__)\n",
      "      348    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "       46    0.000    0.000    0.000    0.000 threading.py:303(__enter__)\n",
      "        6    0.000    0.000    0.000    0.000 random.py:458(choices)\n",
      "        6    0.000    0.000    0.000    0.000 resource_tracker.py:214(unregister)\n",
      "       16    0.000    0.000    0.000    0.000 context.py:369(set_spawning_popen)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'write' of '_io.BufferedWriter' objects}\n",
      "        1    0.000    0.000    0.000    0.000 decorator.py:200(fix)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:527(_handle_tasks)\n",
      "       44    0.000    0.000    0.000    0.000 threading.py:318(_is_owned)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:312(_release_save)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:649(msg)\n",
      "       48    0.000    0.000    0.000    0.000 context.py:372(assert_spawning)\n",
      "       46    0.000    0.000    0.000    0.000 threading.py:306(__exit__)\n",
      "       16    0.000    0.000    0.000    0.000 queues.py:375(__getstate__)\n",
      "      202    0.000    0.000    0.000    0.000 connection.py:135(_check_closed)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:675(sign)\n",
      "        7    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "        3    0.000    0.000    0.000    0.000 connection.py:533(Pipe)\n",
      "       11    0.000    0.000    0.000    0.000 _weakrefset.py:85(add)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:471(_map_async)\n",
      "        5    0.000    0.000    0.000    0.000 events.py:157(cancel)\n",
      "        1    0.000    0.000    0.000    0.000 trainer.py:40(_init_vocab)\n",
      "       12    0.000    0.000    0.000    0.000 base_events.py:1939(_add_callback)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:153(is_alive)\n",
      "       65    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'format_map' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 connection.py:208(recv_bytes)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method now}\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1265(_make_invoke_excepthook)\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:164(__setitem__)\n",
      "       44    0.000    0.000    0.000    0.000 traitlets.py:629(get)\n",
      "       97    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
      "       86    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 session.py:645(msg_header)\n",
      "        9    0.000    0.000    0.000    0.000 pool.py:385(_guarded_task_generation)\n",
      "        7    0.000    0.000    0.000    0.000 synchronize.py:94(__enter__)\n",
      "       69    0.000    0.000    0.000    0.000 {built-in method select.poll}\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3264(bind)\n",
      "        8    0.000    0.000    0.000    0.000 subprocess.py:296(_optim_args_from_interpreter_flags)\n",
      "       43    0.000    0.000    0.000    0.000 queue.py:267(_qsize)\n",
      "       24    0.000    0.000    0.000    0.000 spawn.py:92(<genexpr>)\n",
      "       98    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "       54    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
      "        9    0.000    0.000    0.000    0.000 {method 'seek' of '_io.BufferedReader' objects}\n",
      "       76    0.000    0.000    0.000    0.000 {method 'is_done' of '_thread._ThreadHandle' objects}\n",
      "       59    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:315(_acquire_restore)\n",
      "        4    0.000    0.000    0.000    0.000 jsonutil.py:107(json_default)\n",
      "        5    0.000    0.000    0.000    0.000 base_events.py:457(create_future)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:303(helper)\n",
      "       46    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
      "       86    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "        7    0.000    0.000    0.000    0.000 {method '__enter__' of '_multiprocessing.SemLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3122(_bind)\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:131(__del__)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:843(_newname)\n",
      "        4    0.000    0.000    0.000    0.000 traitlets.py:727(_cross_validate)\n",
      "        9    0.000    0.000    0.000    0.000 pool.py:633(_get_tasks)\n",
      "       32    0.000    0.000    0.000    0.000 popen_spawn_posix.py:34(duplicate_for_child)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method _struct.unpack}\n",
      "       80    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:145(__exit__)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3474(validate)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:136(__enter__)\n",
      "        1    0.000    0.000    0.000    0.000 context.py:41(cpu_count)\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:376(_close)\n",
      "        1    0.000    0.000    0.000    0.000 pretokenization.py:10(find_chunk_boundaries)\n",
      "        7    0.000    0.000    0.000    0.000 synchronize.py:97(__exit__)\n",
      "        8    0.000    0.000    0.000    0.000 util.py:455(close_fds)\n",
      "       41    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:108(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 hmac.py:122(copy)\n",
      "       36    0.000    0.000    0.000    0.000 {built-in method _asyncio.get_running_loop}\n",
      "       86    0.000    0.000    0.000    0.000 zmqstream.py:528(receiving)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen posixpath>:166(basename)\n",
      "        2    0.000    0.000    0.000    0.000 _datetime.py:139(__format__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method time.localtime}\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1512(_notify_trait)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:796(__init__)\n",
      "       86    0.000    0.000    0.000    0.000 threading.py:605(is_set)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method posix.cpu_count}\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1160(daemon)\n",
      "      147    0.000    0.000    0.000    0.000 typing.py:2371(cast)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:272(msg_header)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:764(wait)\n",
      "        4    0.000    0.000    0.000    0.000 {method 'isoformat' of 'datetime.datetime' objects}\n",
      "       16    0.000    0.000    0.000    0.000 process.py:94(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 _datetime.py:27(_default_datetime_formatter)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1523(notify_change)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'tell' of '_io.BufferedReader' objects}\n",
      "       21    0.000    0.000    0.000    0.000 {built-in method _struct.pack}\n",
      "        8    0.000    0.000    0.000    0.000 process.py:193(name)\n",
      "        6    0.000    0.000    0.000    0.000 threading.py:1145(daemon)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'execute' of 'sqlite3.Connection' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'copy' of 'list' objects}\n",
      "       12    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
      "        6    0.000    0.000    0.000    0.000 process.py:234(ident)\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:63(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:747(__init__)\n",
      "       12    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method '__exit__' of '_multiprocessing.SemLock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'put' of '_queue.SimpleQueue' objects}\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:105(remove)\n",
      "       21    0.000    0.000    0.000    0.000 {method 'remove' of 'collections.deque' objects}\n",
      "        4    0.000    0.000    0.000    0.000 _handler.py:110(_protected_lock)\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:203(__enter__)\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method _multiprocessing.sem_unlink}\n",
      "        2    0.000    0.000    0.000    0.000 <frozen posixpath>:117(splitext)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:213(authkey)\n",
      "        6    0.000    0.000    0.000    0.000 tempfile.py:142(rng)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'copy' of '_hashlib.HMAC' objects}\n",
      "       44    0.000    0.000    0.000    0.000 base_events.py:548(_check_closed)\n",
      "       20    0.000    0.000    0.000    0.000 {method '_recursion_count' of '_thread.RLock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:2635(validate)\n",
      "       54    0.000    0.000    0.000    0.000 base_events.py:2052(get_debug)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1047(_delete)\n",
      "        6    0.000    0.000    0.000    0.000 ioloop.py:549(time)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:600(msg_id)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1527(_notify_observers)\n",
      "       22    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
      "       23    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
      "       47    0.000    0.000    0.000    0.000 connection.py:139(_check_readable)\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:347(__new__)\n",
      "       18    0.000    0.000    0.000    0.000 process.py:189(name)\n",
      "        2    0.000    0.000    0.000    0.000 hmac.py:161(hexdigest)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3624(validate_elements)\n",
      "        1    0.000    0.000    0.000    0.000 history.py:1008(_writeout_input_cache)\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:352(__init__)\n",
      "       35    0.000    0.000    0.000    0.000 util.py:48(debug)\n",
      "        8    0.000    0.000    0.000    0.000 hmac.py:117(update)\n",
      "       48    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2936(apply_defaults)\n",
      "        5    0.000    0.000    0.000    0.000 events.py:73(cancel)\n",
      "       14    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "       19    0.000    0.000    0.000    0.000 {method 'discard' of 'set' objects}\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:725(_hooks)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:216(_check_mp_mode)\n",
      "        8    0.000    0.000    0.000    0.000 context.py:253(get_start_method)\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:118(__init__)\n",
      "       24    0.000    0.000    0.000    0.000 process.py:37(current_process)\n",
      "        2    0.000    0.000    0.000    0.000 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "       32    0.000    0.000    0.000    0.000 popen_spawn_posix.py:17(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1022(_set_native_id)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'index' of 'list' objects}\n",
      "       48    0.000    0.000    0.000    0.000 {built-in method math.floor}\n",
      "       11    0.000    0.000    0.000    0.000 _weakrefset.py:39(_remove)\n",
      "        7    0.000    0.000    0.000    0.000 {method 'find' of 'bytes' objects}\n",
      "        1    0.000    0.000    0.000    0.000 history.py:1016(_writeout_output_cache)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.locals}\n",
      "       13    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.BytesIO' objects}\n",
      "       32    0.000    0.000    0.000    0.000 connection.py:159(readable)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:505(parent_header)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:281(extract_header)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1018(_set_ident)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:198(utcnow)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method math.isnan}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method combine}\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:16(__format__)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:90(_make_methods)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'hexdigest' of '_hashlib.HMAC' objects}\n",
      "       32    0.000    0.000    0.000    0.000 connection.py:164(writable)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method posix.waitstatus_to_exitcode}\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2883(args)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:213(_is_master_process)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen genericpath>:157(_splitext)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:734(__enter__)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:550(_is_master_process)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:2304(validate)\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method _weakref._remove_dead_weakref}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'update' of '_hashlib.HMAC' objects}\n",
      "        4    0.000    0.000    0.000    0.000 compilerop.py:180(extra_flags)\n",
      "       21    0.000    0.000    0.000    0.000 connection.py:143(_check_writable)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen posixpath>:42(_get_sep)\n",
      "        1    0.000    0.000    0.000    0.000 trainer.py:46(_finalize_vocab)\n",
      "        2    0.000    0.000    0.000    0.000 {method '__format__' of 'str' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'cancelled' of '_asyncio.Future' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'timestamp' of 'datetime.datetime' objects}\n",
      "       16    0.000    0.000    0.000    0.000 spawn.py:45(get_executable)\n",
      "        6    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:631(clear)\n",
      "        3    0.000    0.000    0.000    0.000 events.py:129(__lt__)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'replace' of 'datetime.time' objects}\n",
      "        9    0.000    0.000    0.000    0.000 encoder.py:105(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}\n",
      "        8    0.000    0.000    0.000    0.000 process.py:205(daemon)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:157(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2906(kwargs)\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3486(validate_elements)\n",
      "        1    0.000    0.000    0.000    0.000 trainer.py:31(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 base_events.py:1949(_timer_handle_cancelled)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}\n",
      "        4    0.000    0.000    0.000    0.000 jsonutil.py:38(_ensure_tzinfo)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'items' of 'mappingproxy' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:279(_get_sentinels)\n",
      "       15    0.000    0.000    0.000    0.000 util.py:44(sub_debug)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:2558(_validate_bounds)\n",
      "        9    0.000    0.000    0.000    0.000 context.py:187(get_context)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'time' of 'datetime.datetime' objects}\n",
      "        6    0.000    0.000    0.000    0.000 context.py:197(get_start_method)\n",
      "        2    0.000    0.000    0.000    0.000 interactiveshell.py:3615(compare)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "        2    0.000    0.000    0.000    0.000 history.py:1065(hold)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'date' of 'datetime.datetime' objects}\n",
      "        1    0.000    0.000    0.000    0.000 encoder.py:263(_make_iterencode)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:756(ready)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:255(closed)\n",
      "       10    0.000    0.000    0.000    0.000 inspect.py:2793(kind)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'close' of '_io.StringIO' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _thread.daemon_threads_allowed}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _thread.get_native_id}\n",
      "        3    0.000    0.000    0.000    0.000 tz.py:74(utcoffset)\n",
      "        2    0.000    0.000    0.000    0.000 interactiveshell.py:1299(user_global_ns)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
      "        4    0.000    0.000    0.000    0.000 displaypub.py:150(is_publishing)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen codecs>:189(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 context.py:237(get_context)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:23(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 interactiveshell.py:685(user_ns)\n",
      "        4    0.000    0.000    0.000    0.000 displayhook.py:118(is_active)\n",
      "        2    0.000    0.000    0.000    0.000 pool.py:351(_check_running)\n",
      "        4    0.000    0.000    0.000    0.000 inspect.py:2781(name)\n",
      "        2    0.000    0.000    0.000    0.000 threading.py:1094(name)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2875(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 inspect.py:3076(parameters)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method sys._getframe}\n",
      "        2    0.000    0.000    0.000    0.000 hmac.py:139(_current)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:8(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:37(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:51(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:266(__del__)\n",
      "\n",
      "\n",
      "         5722411 function calls (5722270 primitive calls) in 45.257 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       53   33.306    0.628   33.306    0.628 {method 'control' of 'select.kqueue' objects}\n",
      "       69   10.237    0.148   54.061    0.783 {method 'poll' of 'select.poll' objects}\n",
      "     9744    0.677    0.000    0.977    0.000 trainer.py:53(_update_counter)\n",
      "        1    0.232    0.232   45.194   45.194 trainer.py:112(train)\n",
      "    86/82    0.179    0.002    0.179    0.002 {built-in method posix.read}\n",
      "    88947    0.108    0.000    0.131    0.000 __init__.py:599(__init__)\n",
      "  1372154    0.080    0.000    0.080    0.000 {method 'append' of 'list' objects}\n",
      "  2069983    0.080    0.000    0.080    0.000 {built-in method builtins.len}\n",
      "   532759    0.031    0.000    0.031    0.000 trainer.py:18(__lt__)\n",
      "   946162    0.030    0.000    0.030    0.000 __init__.py:613(__missing__)\n",
      "       11    0.030    0.003    0.031    0.003 {built-in method _pickle.loads}\n",
      "    21941    0.026    0.000    0.047    0.000 {built-in method _heapq.heappop}\n",
      "      312    0.026    0.000    0.026    0.000 {built-in method posix.waitpid}\n",
      "        8    0.026    0.003    0.026    0.003 {built-in method _posixsubprocess.fork_exec}\n",
      "       53    0.017    0.000   78.543    1.482 base_events.py:1954(_run_once)\n",
      "    79510    0.016    0.000    0.026    0.000 {built-in method _heapq.heappush}\n",
      "        8    0.016    0.002    0.021    0.003 __init__.py:928(__iadd__)\n",
      "    88947    0.012    0.000    0.022    0.000 __init__.py:673(update)\n",
      "      312    0.010    0.000    0.039    0.000 popen_fork.py:25(poll)\n",
      "    29516    0.010    0.000    0.015    0.000 {method 'join' of 'str' objects}\n",
      "     9797    0.006    0.000    0.006    0.000 {method 'update' of 'dict' objects}\n",
      "      272    0.006    0.000    0.040    0.000 process.py:224(exitcode)\n",
      "    40004    0.005    0.000    0.007    0.000 encoder.py:334(_iterencode_dict)\n",
      "        1    0.005    0.005    0.020    0.020 __init__.py:120(dump)\n",
      "      3/1    0.005    0.002   43.823   43.823 ipkernel.py:750(run_closure)\n",
      "        8    0.005    0.001    0.005    0.001 __init__.py:921(_keep_positive)\n",
      "        1    0.005    0.005    0.035    0.035 trainer.py:200(to_files)\n",
      "    49746    0.004    0.000    0.004    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n",
      "    40004    0.004    0.000    0.011    0.000 encoder.py:414(_iterencode)\n",
      "    69419    0.003    0.000    0.003    0.000 <string>:2(__init__)\n",
      "41773/41687    0.003    0.000    0.006    0.000 {built-in method builtins.isinstance}\n",
      "       53    0.003    0.000   33.318    0.629 selectors.py:540(select)\n",
      "       86    0.002    0.000    0.003    0.000 attrsettr.py:66(_get_attr_opt)\n",
      "    67874    0.002    0.000    0.002    0.000 trainer.py:211(<genexpr>)\n",
      "       11    0.002    0.000    0.002    0.000 {built-in method _io.open}\n",
      "     9744    0.002    0.000    0.022    0.000 __init__.py:734(copy)\n",
      "    41840    0.002    0.000    0.002    0.000 trainer.py:221(<genexpr>)\n",
      "    19553    0.002    0.000    0.002    0.000 {method 'items' of 'dict' objects}\n",
      "    35251    0.001    0.000    0.001    0.000 trainer.py:222(<genexpr>)\n",
      "        1    0.001    0.001    0.001    0.001 tokenizer.py:15(__init__)\n",
      "       18    0.001    0.000    0.012    0.001 iostream.py:592(flush)\n",
      "        8    0.001    0.000    0.029    0.004 popen_spawn_posix.py:38(_launch)\n",
      "        8    0.001    0.000    0.001    0.000 process.py:61(_cleanup)\n",
      "     9752    0.001    0.000    0.002    0.000 <frozen abc>:117(__instancecheck__)\n",
      "        1    0.001    0.001   45.256   45.256 3166285396.py:1(<module>)\n",
      "       86    0.001    0.000    0.001    0.000 {method 'write' of '_io.BytesIO' objects}\n",
      "     9752    0.001    0.000    0.001    0.000 {built-in method _abc._abc_instancecheck}\n",
      "       69    0.001    0.000    0.001    0.000 socket.py:623(send)\n",
      "       37    0.001    0.000    0.001    0.000 {method 'dump' of '_pickle.Pickler' objects}\n",
      "     9762    0.001    0.000    0.001    0.000 {method 'add' of 'set' objects}\n",
      "        8    0.001    0.000    0.042    0.005 popen_fork.py:16(__init__)\n",
      "    10000    0.001    0.000    0.001    0.000 {built-in method _json.encode_basestring}\n",
      "      344    0.001    0.000    0.001    0.000 ipkernel.py:781(_clean_thread_parent_frames)\n",
      "       41    0.001    0.000    0.002    0.000 iostream.py:259(schedule)\n",
      "       69    0.001    0.000   97.836    1.418 selectors.py:385(select)\n",
      "        8    0.001    0.000    0.026    0.003 util.py:439(spawnv_passfds)\n",
      "        8    0.001    0.000    0.051    0.006 process.py:110(start)\n",
      "       19    0.000    0.000    0.001    0.000 iostream.py:616(_flush)\n",
      "       86    0.000    0.000    0.003    0.000 attrsettr.py:43(__getattr__)\n",
      "        4    0.000    0.000    0.002    0.001 socket.py:700(send_multipart)\n",
      "       43    0.000    0.000    0.001    0.000 socket.py:771(recv_multipart)\n",
      "       80    0.000    0.000    0.001    0.000 base_events.py:766(time)\n",
      "        8    0.000    0.000    0.001    0.000 process.py:80(__init__)\n",
      "        3    0.000    0.000    0.001    0.000 threading.py:869(__init__)\n",
      "       15    0.000    0.000    0.000    0.000 util.py:178(__init__)\n",
      "      129    0.000    0.000    0.001    0.000 typing.py:426(inner)\n",
      "        8    0.000    0.000    0.049    0.006 context.py:286(_Popen)\n",
      "       69    0.000    0.000   98.016    1.421 connection.py:1134(wait)\n",
      "        1    0.000    0.000    0.052    0.052 pool.py:314(_repopulate_pool_static)\n",
      "      129    0.000    0.000    0.001    0.000 enum.py:1605(__and__)\n",
      "       11    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "       76    0.000    0.000    0.000    0.000 threading.py:1134(is_alive)\n",
      "      588    0.000    0.000    0.000    0.000 enum.py:1587(_get_value)\n",
      "      366    0.000    0.000    0.001    0.000 selectors.py:238(register)\n",
      "       37    0.000    0.000    0.001    0.000 reduction.py:38(__init__)\n",
      "       43    0.000    0.000    0.011    0.000 zmqstream.py:573(_handle_events)\n",
      "        1    0.000    0.000   43.823   43.823 {method 'acquire' of '_multiprocessing.SemLock' objects}\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:281(__init__)\n",
      "       32    0.000    0.000    0.000    0.000 threading.py:1427(current_thread)\n",
      "       54    0.000    0.000    0.000    0.000 selector_events.py:740(_process_events)\n",
      "      172    0.000    0.000    0.000    0.000 threading.py:1477(enumerate)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method posix.getcwd}\n",
      "       49    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "        8    0.000    0.000    0.013    0.002 util.py:425(_flush_std_streams)\n",
      "       54    0.000    0.000    0.013    0.000 events.py:87(_run)\n",
      "      366    0.000    0.000    0.001    0.000 selectors.py:340(register)\n",
      "       38    0.000    0.000    0.000    0.000 {method 'getbuffer' of '_io.BytesIO' objects}\n",
      "       54    0.000    0.000    0.012    0.000 {method 'run' of '_contextvars.Context' objects}\n",
      "       43    0.000    0.000    0.004    0.000 zmqstream.py:546(_run_callback)\n",
      "       22    0.000    0.000    0.001    0.000 threading.py:592(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _heapq.heapify}\n",
      "      285    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "       31    0.000    0.000    0.001    0.000 asyncio.py:231(add_callback)\n",
      "       43    0.000    0.000    0.004    0.000 iostream.py:157(_handle_event)\n",
      "      325    0.000    0.000    0.000    0.000 enum.py:695(__call__)\n",
      "     1272    0.000    0.000    0.000    0.000 threading.py:1110(ident)\n",
      "       43    0.000    0.000    0.006    0.000 zmqstream.py:614(_handle_recv)\n",
      "       19    0.000    0.000    0.000    0.000 iostream.py:718(_rotate_buffers)\n",
      "       43    0.000    0.000    0.002    0.000 zmqstream.py:676(_update_handler)\n",
      "      150    0.000    0.000    0.000    0.000 {built-in method time.monotonic}\n",
      "   110/80    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        8    0.000    0.000    0.001    0.000 spawn.py:160(get_preparation_data)\n",
      "        1    0.000    0.000    0.037    0.037 encoder.py:11(train_bpe_tokenizer)\n",
      "       33    0.000    0.000   43.089    1.306 pool.py:289(_join_exited_workers)\n",
      "       12    0.000    0.000    0.005    0.000 asyncio.py:206(_handle_events)\n",
      "      439    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        1    0.000    0.000   43.885   43.885 pool.py:654(terminate)\n",
      "       54    0.000    0.000    0.000    0.000 {built-in method posix.close}\n",
      "       43    0.000    0.000    0.003    0.000 zmqstream.py:653(_rebuild_io_state)\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method posix.pipe}\n",
      "        1    0.000    0.000    0.698    0.698 pool.py:767(get)\n",
      "       53    0.000    0.000    0.000    0.000 {built-in method posix.write}\n",
      "      366    0.000    0.000    0.000    0.000 selectors.py:21(_fileobj_to_fd)\n",
      "       16    0.000    0.000    0.002    0.000 reduction.py:58(dump)\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:336(__init__)\n",
      "       36    0.000    0.000    0.000    0.000 base_events.py:846(_call_soon)\n",
      "    22/19    0.000    0.000    0.000    0.000 threading.py:327(wait)\n",
      "       41    0.000    0.000    0.000    0.000 iostream.py:138(_event_pipe)\n",
      "       33    0.000    0.000    0.000    0.000 pool.py:284(_get_worker_sentinels)\n",
      "      325    0.000    0.000    0.000    0.000 enum.py:1154(__new__)\n",
      "       86    0.000    0.000    0.000    0.000 typing.py:1443(__hash__)\n",
      "       39    0.000    0.000    0.000    0.000 {built-in method _contextvars.copy_context}\n",
      "      224    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "      3/1    0.000    0.000   43.823   43.823 threading.py:1025(_bootstrap_inner)\n",
      "      528    0.000    0.000    0.000    0.000 process.py:247(sentinel)\n",
      "        4    0.000    0.000    0.007    0.002 process.py:142(join)\n",
      "    23/20    0.000    0.000    0.000    0.000 threading.py:641(wait)\n",
      "       33    0.000    0.000   87.507    2.652 pool.py:500(_wait_for_updates)\n",
      "    22/18    0.000    0.000    0.179    0.010 connection.py:390(_recv)\n",
      "       94    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1390(_handle_fromlist)\n",
      "       32    0.000    0.000    0.000    0.000 synchronize.py:100(__getstate__)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:50(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _thread.start_joinable_thread}\n",
      "        8    0.000    0.000    0.000    0.000 spawn.py:83(get_command_line)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method posix.kill}\n",
      "       67    0.000    0.000    0.000    0.000 enum.py:1594(__or__)\n",
      "        1    0.000    0.000   43.831   43.831 pool.py:680(_terminate_pool)\n",
      "      366    0.000    0.000    0.000    0.000 selectors.py:219(_fileobj_lookup)\n",
      "       44    0.000    0.000    0.000    0.000 events.py:36(__init__)\n",
      "      366    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:620(set)\n",
      "       43    0.000    0.000    0.000    0.000 typing.py:1665(__subclasscheck__)\n",
      "       43    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
      "       32    0.000    0.000    0.000    0.000 connection.py:1184(reduce_connection)\n",
      "      117    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "        8    0.000    0.000    0.000    0.000 subprocess.py:306(_args_from_interpreter_flags)\n",
      "       33    0.000    0.000   43.096    1.306 pool.py:333(_maintain_pool)\n",
      "      134    0.000    0.000    0.000    0.000 connection.py:169(fileno)\n",
      "       36    0.000    0.000   43.837    1.218 connection.py:439(_poll)\n",
      "       15    0.000    0.000   43.832    2.922 util.py:197(__call__)\n",
      "      688    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "      374    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x102a0c048}\n",
      "        8    0.000    0.000    0.048    0.006 popen_spawn_posix.py:30(__init__)\n",
      "       36    0.000    0.000   43.843    1.218 connection.py:253(poll)\n",
      "       36    0.000    0.000    0.000    0.000 base_events.py:817(call_soon)\n",
      "        8    0.000    0.000    0.001    0.000 pool.py:179(Process)\n",
      "      834    0.000    0.000    0.000    0.000 process.py:99(_check_closed)\n",
      "       43    0.000    0.000    0.000    0.000 queue.py:115(empty)\n",
      "       16    0.000    0.000    0.000    0.000 process.py:347(__reduce__)\n",
      "      366    0.000    0.000    0.000    0.000 {method 'register' of 'select.poll' objects}\n",
      "       20    0.000    0.000    0.000    0.000 resource_tracker.py:130(ensure_running)\n",
      "       43    0.000    0.000    0.000    0.000 zmqstream.py:532(sending)\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:213(__init__)\n",
      "       43    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:272(close)\n",
      "       21    0.000    0.000    0.000    0.000 iostream.py:710(_flush_buffers)\n",
      "       69    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
      "       21    0.000    0.000    0.000    0.000 connection.py:406(_send_bytes)\n",
      "        5    0.000    0.000    0.000    0.000 iostream.py:127(_event_pipe_gc)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'join' of '_thread._ThreadHandle' objects}\n",
      "       32    0.000    0.000    0.000    0.000 reduction.py:191(DupFd)\n",
      "       43    0.000    0.000    0.000    0.000 typing.py:1374(__instancecheck__)\n",
      "       10    0.000    0.000    0.000    0.000 tasks.py:703(sleep)\n",
      "       54    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:645(parent)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "       21    0.000    0.000    0.000    0.000 reduction.py:48(dumps)\n",
      "     11/9    0.000    0.000    0.000    0.000 connection.py:429(_recv_bytes)\n",
      "      3/1    0.000    0.000   43.823   43.823 threading.py:981(run)\n",
      "       32    0.000    0.000    0.006    0.000 ioloop.py:750(_run_callback)\n",
      "       43    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        5    0.000    0.000    0.000    0.000 iostream.py:118(_run_event_pipe_gc)\n",
      "      348    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:398(notify)\n",
      "        8    0.000    0.000    0.000    0.000 spawn.py:138(_check_not_importing_main)\n",
      "       16    0.000    0.000    0.000    0.000 context.py:369(set_spawning_popen)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'write' of '_io.BufferedWriter' objects}\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:428(notify_all)\n",
      "        2    0.000    0.000    0.001    0.001 _logger.py:1931(_log)\n",
      "       43    0.000    0.000    0.000    0.000 <frozen abc>:121(__subclasscheck__)\n",
      "        3    0.000    0.000    0.001    0.000 ipkernel.py:774(init_closure)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:655(write)\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:206(__exit__)\n",
      "      202    0.000    0.000    0.000    0.000 connection.py:135(_check_closed)\n",
      "        5    0.000    0.000    0.000    0.000 futures.py:310(_set_result_unless_cancelled)\n",
      "        7    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "        4    0.000    0.000    0.000    0.000 interactiveshell.py:3043(write)\n",
      "        1    0.000    0.000    0.000    0.000 trainer.py:40(_init_vocab)\n",
      "        2    0.000    0.000    0.000    0.000 _datetime.py:143(aware_now)\n",
      "        6    0.000    0.000    0.000    0.000 random.py:458(choices)\n",
      "        9    0.000    0.000    0.000    0.000 encoder.py:205(iterencode)\n",
      "        8    0.000    0.000    0.000    0.000 base_events.py:775(call_later)\n",
      "        4    0.000    0.000    0.000    0.000 traitlets.py:708(__set__)\n",
      "       65    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "       16    0.000    0.000    0.000    0.000 connection.py:202(send)\n",
      "       31    0.000    0.000    0.006    0.000 zmqstream.py:684(<lambda>)\n",
      "       52    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        9    0.000    0.000    0.032    0.004 connection.py:246(recv)\n",
      "       46    0.000    0.000    0.000    0.000 threading.py:306(__exit__)\n",
      "       35    0.000    0.000    0.020    0.001 queues.py:372(empty)\n",
      "       96    0.000    0.000    0.000    0.000 context.py:366(get_spawning_popen)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method now}\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1265(_make_invoke_excepthook)\n",
      "        5    0.000    0.000    0.000    0.000 events.py:157(cancel)\n",
      "       11    0.000    0.000    0.000    0.000 _weakrefset.py:85(add)\n",
      "      3/2    0.000    0.000   45.256   22.628 interactiveshell.py:3663(run_code)\n",
      "        8    0.000    0.000    0.000    0.000 base_events.py:799(call_at)\n",
      "       97    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
      "       44    0.000    0.000    0.000    0.000 traitlets.py:676(__get__)\n",
      "       86    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "        5    0.000    0.000    0.000    0.000 queues.py:389(put)\n",
      "       69    0.000    0.000    0.000    0.000 {built-in method select.poll}\n",
      "       46    0.000    0.000    0.000    0.000 threading.py:303(__enter__)\n",
      "        1    0.000    0.000    0.052    0.052 pool.py:305(_repopulate_pool)\n",
      "       12    0.000    0.000    0.000    0.000 resource_tracker.py:218(_send)\n",
      "       44    0.000    0.000    0.000    0.000 traitlets.py:629(get)\n",
      "        8    0.000    0.000    0.000    0.000 subprocess.py:296(_optim_args_from_interpreter_flags)\n",
      "       44    0.000    0.000    0.000    0.000 threading.py:318(_is_owned)\n",
      "       24    0.000    0.000    0.000    0.000 spawn.py:92(<genexpr>)\n",
      "       16    0.000    0.000    0.000    0.000 queues.py:375(__getstate__)\n",
      "        8    0.000    0.000    0.001    0.000 resource_tracker.py:126(getfd)\n",
      "       98    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "       54    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
      "        9    0.000    0.000    0.000    0.000 {method 'seek' of '_io.BufferedReader' objects}\n",
      "       76    0.000    0.000    0.000    0.000 {method 'is_done' of '_thread._ThreadHandle' objects}\n",
      "       59    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        5    0.000    0.000    0.000    0.000 connection.py:182(send_bytes)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:312(_release_save)\n",
      "        4    0.000    0.000    0.007    0.002 popen_fork.py:37(wait)\n",
      "       46    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
      "       86    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "        5    0.000    0.000    0.000    0.000 base_events.py:457(create_future)\n",
      "        7    0.000    0.000    0.000    0.000 {method '__enter__' of '_multiprocessing.SemLock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 session.py:754(send)\n",
      "        6    0.000    0.000    0.000    0.000 util.py:163(register_after_fork)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:843(_newname)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'set_result' of '_asyncio.Future' objects}\n",
      "       48    0.000    0.000    0.000    0.000 context.py:372(assert_spawning)\n",
      "        8    0.000    0.000    0.000    0.000 pool.py:809(_set)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:471(_map_async)\n",
      "        9    0.000    0.000    0.000    0.000 pool.py:633(_get_tasks)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1056(join)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:506(_handle_workers)\n",
      "       43    0.000    0.000    0.000    0.000 queue.py:267(_qsize)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:84(_cleanup)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method _struct.unpack}\n",
      "       80    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "       20    0.000    0.000    0.000    0.000 resource_tracker.py:199(_check_alive)\n",
      "      3/1    0.000    0.000   43.823   43.823 threading.py:998(_bootstrap)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:953(start)\n",
      "        6    0.000    0.000    0.000    0.000 context.py:65(Lock)\n",
      "        4    0.000    0.000    0.000    0.000 traitlets.py:727(_cross_validate)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:183(__init__)\n",
      "        1    0.000    0.000   43.886   43.886 pretokenization.py:109(pretokenize_file_to_counter)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3122(_bind)\n",
      "       41    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
      "        2    0.000    0.000    0.000    0.000 session.py:690(serialize)\n",
      "        8    0.000    0.000    0.000    0.000 __init__.py:183(dumps)\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:164(__setitem__)\n",
      "       36    0.000    0.000    0.000    0.000 {built-in method _asyncio.get_running_loop}\n",
      "       86    0.000    0.000    0.000    0.000 zmqstream.py:528(receiving)\n",
      "        8    0.000    0.000    0.000    0.000 events.py:113(__init__)\n",
      "       21    0.000    0.000    0.000    0.000 connection.py:381(_send)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method time.localtime}\n",
      "        3    0.000    0.000    0.000    0.000 context.py:110(SimpleQueue)\n",
      "       86    0.000    0.000    0.000    0.000 threading.py:605(is_set)\n",
      "        4    0.000    0.000    0.002    0.001 iostream.py:278(_really_send)\n",
      "        2    0.000    0.000    0.001    0.001 _logger.py:2076(info)\n",
      "        3    0.000    0.000    0.000    0.000 connection.py:533(Pipe)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method posix.cpu_count}\n",
      "      147    0.000    0.000    0.000    0.000 typing.py:2371(cast)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:315(_acquire_restore)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:153(is_alive)\n",
      "        4    0.000    0.000    0.000    0.000 traitlets.py:718(_validate)\n",
      "       32    0.000    0.000    0.000    0.000 popen_spawn_posix.py:34(duplicate_for_child)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:108(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 encoder.py:183(encode)\n",
      "        6    0.000    0.000    0.000    0.000 tempfile.py:153(__next__)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'format_map' of 'str' objects}\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1160(daemon)\n",
      "        2    0.000    0.000    0.000    0.000 codeop.py:113(__call__)\n",
      "        3    0.000    0.000    0.000    0.000 asyncio.py:216(call_at)\n",
      "       16    0.000    0.000    0.000    0.000 process.py:94(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 _datetime.py:27(_default_datetime_formatter)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'tell' of '_io.BufferedReader' objects}\n",
      "       21    0.000    0.000    0.000    0.000 {built-in method _struct.pack}\n",
      "        2    0.000    0.000    0.001    0.001 _handler.py:127(emit)\n",
      "        4    0.000    0.000    0.000    0.000 {method 'isoformat' of 'datetime.datetime' objects}\n",
      "        1    0.000    0.000    0.000    0.000 context.py:115(Pool)\n",
      "        6    0.000    0.000    0.000    0.000 threading.py:1145(daemon)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'execute' of 'sqlite3.Connection' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'copy' of 'list' objects}\n",
      "        4    0.000    0.000    0.000    0.000 traitlets.py:689(set)\n",
      "       12    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 <frozen posixpath>:166(basename)\n",
      "        1    0.000    0.000   43.823   43.823 pool.py:573(_handle_results)\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:63(__init__)\n",
      "       12    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method '__exit__' of '_multiprocessing.SemLock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'put' of '_queue.SimpleQueue' objects}\n",
      "        8    0.000    0.000    0.000    0.000 popen_fork.py:47(_send_signal)\n",
      "        8    0.000    0.000    0.000    0.000 session.py:92(json_packer)\n",
      "        3    0.000    0.000    0.000    0.000 queues.py:359(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:121(_make_name)\n",
      "       12    0.000    0.000    0.000    0.000 base_events.py:1939(_add_callback)\n",
      "       21    0.000    0.000    0.000    0.000 {method 'remove' of 'collections.deque' objects}\n",
      "        3    0.000    0.000    0.000    0.000 ioloop.py:604(call_later)\n",
      "       69    0.000    0.000    0.000    0.000 selectors.py:203(__enter__)\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method _multiprocessing.sem_unlink}\n",
      "        1    0.000    0.000   43.823   43.823 pool.py:671(_help_stuff_finish)\n",
      "        6    0.000    0.000    0.000    0.000 process.py:234(ident)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:193(name)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:213(authkey)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'copy' of '_hashlib.HMAC' objects}\n",
      "       44    0.000    0.000    0.000    0.000 base_events.py:548(_check_closed)\n",
      "       20    0.000    0.000    0.000    0.000 {method '_recursion_count' of '_thread.RLock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3631(set)\n",
      "       54    0.000    0.000    0.000    0.000 base_events.py:2052(get_debug)\n",
      "        8    0.000    0.000    0.000    0.000 popen_fork.py:57(terminate)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:675(sign)\n",
      "        4    0.000    0.000    0.002    0.001 iostream.py:276(<lambda>)\n",
      "        9    0.000    0.000    0.000    0.000 pool.py:385(_guarded_task_generation)\n",
      "       22    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
      "       23    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
      "       47    0.000    0.000    0.000    0.000 connection.py:139(_check_readable)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:649(msg)\n",
      "       18    0.000    0.000    0.000    0.000 process.py:189(name)\n",
      "        4    0.000    0.000    0.000    0.000 _handler.py:110(_protected_lock)\n",
      "        1    0.000    0.000    0.000    0.000 decorator.py:200(fix)\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:352(__init__)\n",
      "       35    0.000    0.000    0.000    0.000 util.py:48(debug)\n",
      "        2    0.000    0.000    0.001    0.001 _simple_sinks.py:15(write)\n",
      "        7    0.000    0.000    0.000    0.000 synchronize.py:94(__enter__)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:2635(validate)\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:105(remove)\n",
      "        1    0.000    0.000    0.000    0.000 pretokenization.py:10(find_chunk_boundaries)\n",
      "        4    0.000    0.000    0.000    0.000 jsonutil.py:107(json_default)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:587(_schedule_in_thread)\n",
      "        6    0.000    0.000    0.000    0.000 ioloop.py:549(time)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3474(validate)\n",
      "        1    0.000    0.000    0.000    0.000 history.py:1024(writeout_cache)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1047(_delete)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:747(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 process.py:128(terminate)\n",
      "        7    0.000    0.000    0.000    0.000 synchronize.py:97(__exit__)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:303(helper)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:145(__exit__)\n",
      "        2    0.000    0.000    0.000    0.000 queues.py:383(get)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3264(bind)\n",
      "       48    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:161(__delitem__)\n",
      "       14    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "       19    0.000    0.000    0.000    0.000 {method 'discard' of 'set' objects}\n",
      "        2    0.000    0.000    0.000    0.000 hmac.py:122(copy)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1527(_notify_observers)\n",
      "        8    0.000    0.000    0.000    0.000 context.py:253(get_start_method)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:168(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:118(__init__)\n",
      "       24    0.000    0.000    0.000    0.000 process.py:37(current_process)\n",
      "        2    0.000    0.000   45.256   22.628 {built-in method builtins.exec}\n",
      "        2    0.000    0.000    0.000    0.000 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "       32    0.000    0.000    0.000    0.000 popen_spawn_posix.py:17(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 events.py:73(cancel)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3624(validate_elements)\n",
      "        5    0.000    0.000    0.000    0.000 contextlib.py:136(__enter__)\n",
      "        6    0.000    0.000    0.000    0.000 tempfile.py:142(rng)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'index' of 'list' objects}\n",
      "        2    0.000    0.000    0.000    0.000 connection.py:208(recv_bytes)\n",
      "       48    0.000    0.000    0.000    0.000 {built-in method math.floor}\n",
      "        2    0.000    0.000    0.000    0.000 <frozen posixpath>:117(splitext)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:600(msg_id)\n",
      "        7    0.000    0.000    0.000    0.000 {method 'find' of 'bytes' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.locals}\n",
      "       13    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.BytesIO' objects}\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:577(_schedule_flush)\n",
      "        1    0.000    0.000   43.885   43.885 pool.py:738(__exit__)\n",
      "       32    0.000    0.000    0.000    0.000 connection.py:159(readable)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1022(_set_native_id)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:281(extract_header)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method math.isnan}\n",
      "        6    0.000    0.000    0.000    0.000 resource_tracker.py:210(register)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method combine}\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:90(_make_methods)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2936(apply_defaults)\n",
      "        6    0.000    0.000    0.000    0.000 weakref.py:347(__new__)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'hexdigest' of '_hashlib.HMAC' objects}\n",
      "        2    0.000    0.000    0.000    0.000 session.py:645(msg_header)\n",
      "        1    0.000    0.000    0.000    0.000 context.py:41(cpu_count)\n",
      "       32    0.000    0.000    0.000    0.000 connection.py:164(writable)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method posix.waitstatus_to_exitcode}\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:505(parent_header)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:796(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:345(_setup_queues)\n",
      "        8    0.000    0.000    0.000    0.000 hmac.py:117(update)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1523(notify_change)\n",
      "        1    0.000    0.000    0.001    0.001 history.py:92(only_when_enabled)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1018(_set_ident)\n",
      "        8    0.000    0.000    0.000    0.000 util.py:455(close_fds)\n",
      "        2    0.000    0.000    0.000    0.000 _datetime.py:139(__format__)\n",
      "       11    0.000    0.000    0.000    0.000 _weakrefset.py:39(_remove)\n",
      "        2    0.000    0.000    0.000    0.000 hmac.py:161(hexdigest)\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method _weakref._remove_dead_weakref}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'update' of '_hashlib.HMAC' objects}\n",
      "        4    0.000    0.000    0.000    0.000 compilerop.py:180(extra_flags)\n",
      "        6    0.000    0.000    0.000    0.000 resource_tracker.py:214(unregister)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:734(__enter__)\n",
      "        1    0.000    0.000    0.698    0.698 pool.py:369(starmap)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:550(_is_master_process)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen genericpath>:157(_splitext)\n",
      "       21    0.000    0.000    0.000    0.000 connection.py:143(_check_writable)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:2304(validate)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2883(args)\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:131(__del__)\n",
      "        2    0.000    0.000    0.000    0.000 {method '__format__' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:527(_handle_tasks)\n",
      "        1    0.000    0.000    0.000    0.000 history.py:1016(_writeout_output_cache)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:272(msg_header)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'cancelled' of '_asyncio.Future' objects}\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:213(_is_master_process)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'timestamp' of 'datetime.datetime' objects}\n",
      "       16    0.000    0.000    0.000    0.000 spawn.py:45(get_executable)\n",
      "        6    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1512(_notify_trait)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'replace' of 'datetime.time' objects}\n",
      "        9    0.000    0.000    0.000    0.000 encoder.py:105(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:725(_hooks)\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:216(_check_mp_mode)\n",
      "        6    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}\n",
      "        8    0.000    0.000    0.000    0.000 process.py:205(daemon)\n",
      "        3    0.000    0.000    0.000    0.000 events.py:129(__lt__)\n",
      "        1    0.000    0.000    0.001    0.001 decorator.py:232(fun)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:157(__init__)\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
      "        1    0.000    0.000    0.000    0.000 trainer.py:31(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 trainer.py:46(_finalize_vocab)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen posixpath>:42(_get_sep)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:16(__format__)\n",
      "        5    0.000    0.000    0.000    0.000 base_events.py:1949(_timer_handle_cancelled)\n",
      "        1    0.000    0.000    0.000    0.000 history.py:1008(_writeout_input_cache)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3486(validate_elements)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:376(_close)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2906(kwargs)\n",
      "        4    0.000    0.000    0.000    0.000 jsonutil.py:38(_ensure_tzinfo)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:198(utcnow)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'items' of 'mappingproxy' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:279(_get_sentinels)\n",
      "       15    0.000    0.000    0.000    0.000 util.py:44(sub_debug)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:764(wait)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:2558(_validate_bounds)\n",
      "        9    0.000    0.000    0.000    0.000 context.py:187(get_context)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:271(send_multipart)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:631(clear)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'time' of 'datetime.datetime' objects}\n",
      "        6    0.000    0.000    0.000    0.000 context.py:197(get_start_method)\n",
      "        2    0.000    0.000    0.000    0.000 interactiveshell.py:3615(compare)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "        2    0.000    0.000    0.000    0.000 history.py:1065(hold)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'date' of 'datetime.datetime' objects}\n",
      "        1    0.000    0.000    0.000    0.000 encoder.py:263(_make_iterencode)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}\n",
      "        4    0.000    0.000    0.000    0.000 iostream.py:255(closed)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:756(ready)\n",
      "       10    0.000    0.000    0.000    0.000 inspect.py:2793(kind)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'close' of '_io.StringIO' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _thread.daemon_threads_allowed}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _thread.get_native_id}\n",
      "        3    0.000    0.000    0.000    0.000 tz.py:74(utcoffset)\n",
      "        2    0.000    0.000    0.000    0.000 interactiveshell.py:1299(user_global_ns)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
      "        4    0.000    0.000    0.000    0.000 displaypub.py:150(is_publishing)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen codecs>:189(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 context.py:237(get_context)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:23(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 interactiveshell.py:685(user_ns)\n",
      "        4    0.000    0.000    0.000    0.000 displayhook.py:118(is_active)\n",
      "        2    0.000    0.000    0.000    0.000 pool.py:351(_check_running)\n",
      "        4    0.000    0.000    0.000    0.000 inspect.py:2781(name)\n",
      "        2    0.000    0.000    0.000    0.000 threading.py:1094(name)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2875(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 inspect.py:3076(parameters)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method sys._getframe}\n",
      "        2    0.000    0.000    0.000    0.000 hmac.py:139(_current)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:8(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:37(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 _recattrs.py:51(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 pool.py:266(__del__)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "\n",
    "from cs336_basics.bpe_tokenizer.encoder import train_bpe_tokenizer\n",
    "\n",
    "input_path = \"data/TinyStoriesV2-GPT4-train.txt\"\n",
    "vocab_path = \"data/TinyStoriesV2-GPT4_vocab.json\"\n",
    "merge_path = \"data/TinyStoriesV2-GPT4_merges.txt\"\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "tokenizer = train_bpe_tokenizer(\n",
    "    input_path=input_path,\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    save=True,\n",
    "    vocab_path=vocab_path,\n",
    "    merge_path=merge_path,\n",
    ")\n",
    "\n",
    "profiler.disable()\n",
    "profiler.print_stats(sort=\"cumtime\")\n",
    "profiler.print_stats(sort=\"tottime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ae7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest token in the vocabulary is: b' accomplishment'\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.bpe_tokenizer.encoder import load_bpe_tokenizer\n",
    "\n",
    "vocab_path = \"data/TinyStoriesV2-GPT4_vocab.json\"\n",
    "merge_path = \"data/TinyStoriesV2-GPT4_merges.txt\"\n",
    "# find the longest vocab in tokenizer\n",
    "tokenizer = load_bpe_tokenizer(\n",
    "    vocab_path=vocab_path,\n",
    "    merge_path=merge_path,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "longest_token = max(tokenizer.vocab.values(), key=len)\n",
    "print(f\"The longest token in the vocabulary is: {longest_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2b35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-16 21:51:06.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mEncoding file data/TinyStoriesV2-GPT4-train.txt using 8 processes...\u001b[0m\n",
      "\u001b[32m2025-08-16 21:52:16.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mEncoding completed. Saving to data/TinyStoriesV2-GPT4-train_encoded_10k.npy...\u001b[0m\n",
      "\u001b[32m2025-08-16 21:52:29.206\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m104\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/TinyStoriesV2-GPT4-train_encoded_10k.npy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.bpe_tokenizer.encoder import (\n",
    "    encode_file,\n",
    "    load_bpe_tokenizer,\n",
    ")\n",
    "\n",
    "input_path = f\"data/TinyStoriesV2-GPT4-train.txt\"\n",
    "output_file = f\"data/TinyStoriesV2-GPT4-train_encoded_10k.npy\"\n",
    "\n",
    "vocab_filepath = f\"data/TinyStoriesV2-GPT4_vocab.json\"\n",
    "merge_filepath = f\"data/TinyStoriesV2-GPT4_merges.txt\"\n",
    "\n",
    "tokenizer = load_bpe_tokenizer(\n",
    "    vocab_path=vocab_filepath,\n",
    "    merge_path=merge_filepath,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "encode_file(input_path, tokenizer, output_file, use_memmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ce1cf",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_expts_owt): BPE Training on OpenWebText\n",
    "\n",
    "#### (a) Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection. What is the longest token in the vocabulary? Does it make sense?\n",
    "Resource requirements: ≤ 12 hours (no GPUs), ≤ 100GB RAM  \n",
    "\n",
    "The longest token in the vocabulary is: `b'---------------------------'`. This makes sense, as long runs of hyphens are common as separators in web text, and BPE merges frequent patterns into single tokens.\n",
    "\n",
    "#### (b) Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.\n",
    "- The tokenizer trained on TinyStories has a smaller vocabulary size (10,000) compared to the one trained on OpenWebText (32,000). \n",
    "- TinyStories has a more specialized vocabulary, focusing on the specific language and patterns found in children's stories, while OpenWebText has a broader vocabulary that captures a wider range of topics and styles found in web text.\n",
    "- The TinyStories tokenizer tends to merge frequent, simple words and short phrases, resulting in tokens that are often common words or short expressions. In contrast, the OpenWebText tokenizer merges frequent patterns found on the web, such as long runs of hyphens, URLs, or special symbols, so the longest tokens may be web separators or long English words.\n",
    "- In summary, the TinyStories tokenizer is simpler and more domain-specific, while the OpenWebText tokenizer is larger and more general-purpose, better suited for handling the diversity of real-world web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d60ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-16 20:31:41.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mtrain_bpe_tokenizer\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mTraining BPE tokenizer on data/owt_train.txt, vocab size: 32000, special tokens: ['<|endoftext|>']\u001b[0m\n",
      "\u001b[32m2025-08-16 20:40:56.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mtrain_bpe_tokenizer\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mTrained BPE tokenizer with vocab size: 32000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 32000, set vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.bpe_tokenizer.encoder import train_bpe_tokenizer\n",
    "\n",
    "input_path = \"data/owt_train.txt\"\n",
    "vocab_path = \"data/owt_vocab.json\"\n",
    "merge_path = \"data/owt_merges.txt\"\n",
    "\n",
    "tokenizer = train_bpe_tokenizer(\n",
    "    input_path=input_path,\n",
    "    vocab_size=32_000,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    save=True,\n",
    "    vocab_path=vocab_path,\n",
    "    merge_path=merge_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0536b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest token in the vocabulary is: b'---------------------------'\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.bpe_tokenizer.encoder import load_bpe_tokenizer\n",
    "\n",
    "vocab_path = \"data/owt_vocab.json\"\n",
    "merge_path = \"data/owt_merges.txt\"\n",
    "# find the longest vocab in tokenizer\n",
    "tokenizer = load_bpe_tokenizer(\n",
    "    vocab_path=vocab_path,\n",
    "    merge_path=merge_path,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "longest_token = max(tokenizer.vocab.values(), key=len)\n",
    "print(f\"The longest token in the vocabulary is: {longest_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc2dbe",
   "metadata": {},
   "source": [
    "### Memory considerations.\n",
    "> Suppose we want to tokenize a large text file that we cannot fit in memory. To efficiently tokenize this large file (or any other stream of data), we need to break it up into manageable chunks and process each chunk in-turn, so that the memory complexity is constant as opposed to linear in the size of the text. In doing so, we need to make sure that a token doesn't cross chunk boundaries, else we'll get a different tokenization than the naïve method of tokenizing the entire sequence in-memory.\n",
    "\n",
    "To verify chunking strategy is working correctly, compare the tokenization results of the chunked approach with the naïve approach on smaller, manageable text files. By ensuring that both methods produce the same tokenization for these smaller files, we can gain confidence that our chunking strategy will work for larger files as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c80f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-17 11:19:08.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mEncoding file data/TinyStoriesV2-GPT4-valid.txt using 8 chunks...\u001b[0m\n",
      "2025-08-17 11:19:08.755 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 2813541-5625758, got 682990 tokens\n",
      "2025-08-17 11:19:08.788 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 0-2813541, got 683867 tokens\n",
      "2025-08-17 11:19:08.796 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 19690315-22502601, got 682743 tokens\n",
      "2025-08-17 11:19:08.798 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 5625758-8438541, got 683132 tokens\n",
      "2025-08-17 11:19:08.812 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 11252559-14064356, got 681943 tokens\n",
      "2025-08-17 11:19:08.815 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 8438541-11252559, got 682702 tokens\n",
      "2025-08-17 11:19:08.817 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 14064356-16877372, got 684968 tokens\n",
      "2025-08-17 11:19:08.823 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 16877372-19690315, got 683618 tokens\n",
      "\u001b[32m2025-08-17 11:19:08.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEncoding completed. Saving to data/TinyStoriesV2-GPT4-valid_encoded_10k1.npy...\u001b[0m\n",
      "100%|██████████| 8/8 [00:00<00:00, 91.13it/s]\n",
      "\u001b[32m2025-08-17 11:19:08.961\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m114\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/TinyStoriesV2-GPT4-valid_encoded_10k1.npy\u001b[0m\n",
      "\u001b[32m2025-08-17 11:19:09.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mEncoding file data/TinyStoriesV2-GPT4-valid.txt using 8 chunks...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:19:09.422 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 0-2813541, got 683867 tokens\n",
      "2025-08-17 11:19:09.425 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 2813541-5625758, got 682990 tokens\n",
      "2025-08-17 11:19:09.446 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 16877372-19690315, got 683618 tokens\n",
      "2025-08-17 11:19:09.448 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 14064356-16877372, got 684968 tokens\n",
      "2025-08-17 11:19:09.453 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 8438541-11252559, got 682702 tokens\n",
      "2025-08-17 11:19:09.455 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 5625758-8438541, got 683132 tokens\n",
      "2025-08-17 11:19:09.475 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 19690315-22502601, got 682743 tokens\n",
      "2025-08-17 11:19:09.476 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 11252559-14064356, got 681943 tokens\n",
      "\u001b[32m2025-08-17 11:19:09.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEncoding completed. Saving to data/TinyStoriesV2-GPT4-valid_encoded_10k2.npy...\u001b[0m\n",
      "100%|██████████| 8/8 [00:00<00:00, 76.39it/s]\n",
      "\u001b[32m2025-08-17 11:19:09.638\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m114\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/TinyStoriesV2-GPT4-valid_encoded_10k2.npy\u001b[0m\n",
      "\u001b[32m2025-08-17 11:19:09.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_naive\u001b[0m:\u001b[36m168\u001b[0m - \u001b[1mEncoding file data/TinyStoriesV2-GPT4-valid.txt serially...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-17 11:19:11.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_naive\u001b[0m:\u001b[36m172\u001b[0m - \u001b[1mEncoding completed. Saving to data/TinyStoriesV2-GPT4-valid_encoded_10k3.npy...\u001b[0m\n",
      "\u001b[32m2025-08-17 11:19:11.241\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_naive\u001b[0m:\u001b[36m188\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/TinyStoriesV2-GPT4-valid_encoded_10k3.npy\u001b[0m\n",
      "\u001b[32m2025-08-17 11:19:11.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_naive\u001b[0m:\u001b[36m168\u001b[0m - \u001b[1mEncoding file data/TinyStoriesV2-GPT4-valid.txt serially...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 1.59 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-17 11:19:12.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_naive\u001b[0m:\u001b[36m172\u001b[0m - \u001b[1mEncoding completed. Saving to data/TinyStoriesV2-GPT4-valid_encoded_10k4.npy...\u001b[0m\n",
      "\u001b[32m2025-08-17 11:19:12.758\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_naive\u001b[0m:\u001b[36m188\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/TinyStoriesV2-GPT4-valid_encoded_10k4.npy\u001b[0m\n",
      "\u001b[32m2025-08-17 11:19:12.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_streaming\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mEncoding file data/TinyStoriesV2-GPT4-valid.txt using 1 chunks...\u001b[0m\n",
      "\u001b[32m2025-08-17 11:19:12.768\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_streaming\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mProcessing chunks 0 to 1 / 1...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 1.52 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 11:19:14.253 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:66 - Tokenized chunk 0-22502601, got 5465963 tokens\n",
      "\u001b[32m2025-08-17 11:19:14.457\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_streaming\u001b[0m:\u001b[36m158\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/TinyStoriesV2-GPT4-valid_encoded_10k5.npy\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 1.71 seconds\n",
      "✅ [117 861 491 ... 377 375  46] and [117 861 491 ... 377 375  46] are identical!\n",
      "✅ [117 861 491 ... 377 375  46] and [117 861 491 ... 377 375  46] are identical!\n",
      "✅ [117 861 491 ... 377 375  46] and [117 861 491 ... 377 375  46] are identical!\n",
      "✅ [117 861 491 ... 377 375  46] and [117 861 491 ... 377 375  46] are identical!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from cs336_basics.bpe_tokenizer.encoder import (\n",
    "    encode_file,\n",
    "    encode_file_streaming,\n",
    "    encode_file_naive,\n",
    "    load_bpe_tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "def check_equal(array1, array2):\n",
    "    if np.array_equal(array1, array2):\n",
    "        print(f\"✅ {array1} and {array2} are identical!\")\n",
    "    else:\n",
    "        print(f\"❌ {array1} and {array2} are different!\")\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.time()\n",
    "        print(f\"Elapsed: {self.end - self.start:.2f} seconds\")\n",
    "\n",
    "\n",
    "train_or_valid = \"valid\"\n",
    "input_path = f\"data/TinyStoriesV2-GPT4-{train_or_valid}.txt\"\n",
    "output_file1 = f\"data/TinyStoriesV2-GPT4-{train_or_valid}_encoded_10k1.npy\"\n",
    "output_file2 = f\"data/TinyStoriesV2-GPT4-{train_or_valid}_encoded_10k2.npy\"\n",
    "output_file3 = f\"data/TinyStoriesV2-GPT4-{train_or_valid}_encoded_10k3.npy\"\n",
    "output_file4 = f\"data/TinyStoriesV2-GPT4-{train_or_valid}_encoded_10k4.npy\"\n",
    "output_file5 = f\"data/TinyStoriesV2-GPT4-{train_or_valid}_encoded_10k5.npy\"\n",
    "\n",
    "vocab_filepath = \"data/TinyStoriesV2-GPT4_vocab.json\"\n",
    "merge_filepath = \"data/TinyStoriesV2-GPT4_merges.txt\"\n",
    "\n",
    "tokenizer = load_bpe_tokenizer(\n",
    "    vocab_path=vocab_filepath,\n",
    "    merge_path=merge_filepath,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "with Timer():\n",
    "    encode_file(input_path, tokenizer, output_file1)\n",
    "with Timer():\n",
    "    encode_file(input_path, tokenizer, output_file2, use_memmap=True)\n",
    "with Timer():\n",
    "    encode_file_naive(input_path, tokenizer, output_file3)\n",
    "with Timer():\n",
    "    encode_file_naive(input_path, tokenizer, output_file4, use_memmap=True)\n",
    "with Timer():\n",
    "    encode_file_streaming(input_path, tokenizer, output_file5)\n",
    "\n",
    "array1 = np.load(output_file1)\n",
    "array2 = np.memmap(output_file2, dtype=np.uint16, mode=\"r\")\n",
    "array3 = np.load(output_file3)\n",
    "array4 = np.memmap(output_file4, dtype=np.uint16, mode=\"r\")\n",
    "array5 = np.memmap(output_file5, dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "check_equal(array1, array2)\n",
    "check_equal(array3, array4)\n",
    "check_equal(array1, array3)\n",
    "check_equal(array1, array5)\n",
    "\n",
    "os.remove(output_file1)\n",
    "os.remove(output_file2)\n",
    "os.remove(output_file3)\n",
    "os.remove(output_file4)\n",
    "os.remove(output_file5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7250a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-17 12:32:06.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mEncoding file data/TinyStoriesV2-GPT4-train.txt using 8 chunks...\u001b[0m\n",
      "2025-08-17 12:32:06.610 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 278469249-556938507, total 265.57 MB...\n",
      "2025-08-17 12:32:06.629 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 0-278469249, total 265.57 MB...\n",
      "2025-08-17 12:32:06.632 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1113876594-1392345974, total 265.57 MB...\n",
      "2025-08-17 12:32:06.633 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 556938507-835407773, total 265.57 MB...\n",
      "2025-08-17 12:32:06.635 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 835407773-1113876594, total 265.57 MB...\n",
      "2025-08-17 12:32:06.638 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1949284164-2227753162, total 265.57 MB...\n",
      "2025-08-17 12:32:06.638 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1392345974-1670815354, total 265.57 MB...\n",
      "2025-08-17 12:32:06.648 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1670815354-1949284164, total 265.57 MB...\n",
      "2025-08-17 12:33:04.955 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 556938507-835407773, got 67647604 tokens\n",
      "2025-08-17 12:33:11.168 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1113876594-1392345974, got 67648334 tokens\n",
      "2025-08-17 12:33:12.753 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 835407773-1113876594, got 67658531 tokens\n",
      "2025-08-17 12:33:13.088 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1949284164-2227753162, got 67646897 tokens\n",
      "2025-08-17 12:33:13.652 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1670815354-1949284164, got 67665251 tokens\n",
      "2025-08-17 12:33:13.699 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1392345974-1670815354, got 67649538 tokens\n",
      "2025-08-17 12:33:13.888 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 278469249-556938507, got 67661088 tokens\n",
      "2025-08-17 12:33:14.068 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 0-278469249, got 67657785 tokens\n",
      "\u001b[32m2025-08-17 12:33:20.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mEncoding completed. Saving to data/TinyStoriesV2-GPT4-train_encoded_10k1.npy...\u001b[0m\n",
      "100%|██████████| 8/8 [00:10<00:00,  1.32s/it]\n",
      "\u001b[32m2025-08-17 12:33:31.388\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m116\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/TinyStoriesV2-GPT4-train_encoded_10k1.npy\u001b[0m\n",
      "\u001b[32m2025-08-17 12:33:34.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_streaming\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mEncoding file data/TinyStoriesV2-GPT4-train.txt using 21 chunks...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 87.73 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-17 12:33:34.213\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_streaming\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mProcessing chunks 0 to 16 / 21...\u001b[0m\n",
      "2025-08-17 12:33:34.353 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 106084188-212167479, total 101.17 MB...\n",
      "2025-08-17 12:33:34.353 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 424334498-530417596, total 101.17 MB...\n",
      "2025-08-17 12:33:34.353 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 636501360-742584451, total 101.17 MB...\n",
      "2025-08-17 12:33:34.353 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 0-106084188, total 101.17 MB...\n",
      "2025-08-17 12:33:34.354 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 318250584-424334498, total 101.17 MB...\n",
      "2025-08-17 12:33:34.355 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 742584451-848668118, total 101.17 MB...\n",
      "2025-08-17 12:33:34.355 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 212167479-318250584, total 101.17 MB...\n",
      "2025-08-17 12:33:34.355 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 530417596-636501360, total 101.17 MB...\n",
      "2025-08-17 12:33:46.681 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 530417596-636501360, got 25773720 tokens\n",
      "2025-08-17 12:33:46.737 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 106084188-212167479, got 25775807 tokens\n",
      "2025-08-17 12:33:46.755 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 424334498-530417596, got 25776973 tokens\n",
      "2025-08-17 12:33:46.869 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 318250584-424334498, got 25773335 tokens\n",
      "2025-08-17 12:33:46.924 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 212167479-318250584, got 25775375 tokens\n",
      "2025-08-17 12:33:47.096 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 636501360-742584451, got 25771486 tokens\n",
      "2025-08-17 12:33:47.112 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 742584451-848668118, got 25767877 tokens\n",
      "2025-08-17 12:33:47.252 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 848668118-954751634, total 101.17 MB...\n",
      "2025-08-17 12:33:47.691 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 0-106084188, got 25772062 tokens\n",
      "2025-08-17 12:33:47.729 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 954751634-1060835441, total 101.17 MB...\n",
      "2025-08-17 12:33:48.218 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1060835441-1166918485, total 101.17 MB...\n",
      "2025-08-17 12:33:48.704 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1166918485-1273002260, total 101.17 MB...\n",
      "2025-08-17 12:33:49.283 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1273002260-1379085804, total 101.17 MB...\n",
      "2025-08-17 12:33:49.858 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1379085804-1485169260, total 101.17 MB...\n",
      "2025-08-17 12:33:50.483 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1485169260-1591252734, total 101.17 MB...\n",
      "2025-08-17 12:33:51.176 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1591252734-1697337309, total 101.17 MB...\n",
      "2025-08-17 12:33:59.206 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 848668118-954751634, got 25773497 tokens\n",
      "2025-08-17 12:34:00.356 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 954751634-1060835441, got 25773356 tokens\n",
      "2025-08-17 12:34:01.019 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1060835441-1166918485, got 25779698 tokens\n",
      "2025-08-17 12:34:01.480 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1166918485-1273002260, got 25763624 tokens\n",
      "2025-08-17 12:34:02.290 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1273002260-1379085804, got 25773959 tokens\n",
      "2025-08-17 12:34:02.748 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1379085804-1485169260, got 25773862 tokens\n",
      "2025-08-17 12:34:03.297 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1485169260-1591252734, got 25774116 tokens\n",
      "2025-08-17 12:34:04.217 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1591252734-1697337309, got 25770602 tokens\n",
      "\u001b[32m2025-08-17 12:34:12.477\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_streaming\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mProcessing chunks 16 to 21 / 21...\u001b[0m\n",
      "2025-08-17 12:34:12.482 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1697337309-1803419637, total 101.17 MB...\n",
      "2025-08-17 12:34:12.484 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1803419637-1909503273, total 101.17 MB...\n",
      "2025-08-17 12:34:12.486 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 1909503273-2015586460, total 101.17 MB...\n",
      "2025-08-17 12:34:12.488 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 2015586460-2121671486, total 101.17 MB...\n",
      "2025-08-17 12:34:12.490 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:60 - Tokenizing bytes 2121671486-2227753162, total 101.17 MB...\n",
      "2025-08-17 12:34:23.230 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1697337309-1803419637, got 25772135 tokens\n",
      "2025-08-17 12:34:23.737 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 2015586460-2121671486, got 25770124 tokens\n",
      "2025-08-17 12:34:23.858 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1803419637-1909503273, got 25776586 tokens\n",
      "2025-08-17 12:34:23.904 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 1909503273-2015586460, got 25776549 tokens\n",
      "2025-08-17 12:34:24.070 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:70 - Tokenized chunk 2121671486-2227753162, got 25770285 tokens\n",
      "\u001b[32m2025-08-17 12:34:30.781\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file_streaming\u001b[0m:\u001b[36m160\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/TinyStoriesV2-GPT4-train_encoded_10k5.npy\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 57.12 seconds\n",
      "✅ [ 10 429 438 ... 316  89 111] and [ 10 429 438 ... 316  89 111] are identical!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from cs336_basics.bpe_tokenizer.encoder import (\n",
    "    encode_file,\n",
    "    encode_file_streaming,\n",
    "    encode_file_naive,\n",
    "    load_bpe_tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "def check_equal(array1, array2):\n",
    "    if np.array_equal(array1, array2):\n",
    "        print(f\"✅ {array1} and {array2} are identical!\")\n",
    "    else:\n",
    "        print(f\"❌ {array1} and {array2} are different!\")\n",
    "        diff = np.setdiff1d(array1, array2)\n",
    "        print(f\"Difference (in array1 but not in array2): {diff}\")\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.time()\n",
    "        print(f\"Elapsed: {self.end - self.start:.2f} seconds\")\n",
    "\n",
    "\n",
    "train_or_valid = \"train\"\n",
    "input_path = f\"data/TinyStoriesV2-GPT4-{train_or_valid}.txt\"\n",
    "output_file1 = f\"data/TinyStoriesV2-GPT4-{train_or_valid}_encoded_10k1.npy\"\n",
    "output_file5 = f\"data/TinyStoriesV2-GPT4-{train_or_valid}_encoded_10k5.npy\"\n",
    "\n",
    "vocab_filepath = \"data/TinyStoriesV2-GPT4_vocab.json\"\n",
    "merge_filepath = \"data/TinyStoriesV2-GPT4_merges.txt\"\n",
    "\n",
    "tokenizer = load_bpe_tokenizer(\n",
    "    vocab_path=vocab_filepath,\n",
    "    merge_path=merge_filepath,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "with Timer():\n",
    "    encode_file(input_path, tokenizer, output_file1)\n",
    "with Timer():\n",
    "    encode_file_streaming(input_path, tokenizer, output_file5, 100)\n",
    "\n",
    "array1 = np.load(output_file1)\n",
    "array5 = np.memmap(output_file5, dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "check_equal(array1, array5)\n",
    "\n",
    "os.remove(output_file1)\n",
    "os.remove(output_file5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd392730",
   "metadata": {},
   "source": [
    "### Problem (tokenizer_experiments): Experiments with tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb3a52",
   "metadata": {},
   "source": [
    "#### (a) Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer's compression ratio (bytes/token)?\n",
    "TinyStories tokenizers compression ratio: 4.03\n",
    "OpenWebText tokenizers compression ratio: 4.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyStories compression ratio: 4.03\n",
      "OpenWebText compression ratio: 4.20\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.bpe_tokenizer.encoder import load_bpe_tokenizer, sample_from_file\n",
    "\n",
    "\n",
    "def calculate_compression_ratio(document_name, desired_num_chunks: int):\n",
    "    if document_name == \"TinyStoriesV2-GPT4\":\n",
    "        input_path = f\"data/{document_name}-train.txt\"\n",
    "    elif document_name == \"owt\":\n",
    "        input_path = f\"data/{document_name}_train.txt\"\n",
    "    vocab_path = f\"data/{document_name}_vocab.json\"\n",
    "    merge_path = f\"data/{document_name}_merges.txt\"\n",
    "\n",
    "    sample_size = 10\n",
    "    special_tokens = [\"<|endoftext|>\"]\n",
    "\n",
    "    tokenizer = load_bpe_tokenizer(vocab_path, merge_path, special_tokens)\n",
    "    samples = sample_from_file(input_path, sample_size, special_tokens, desired_num_chunks)\n",
    "    token_ids = tokenizer.encode_iterable(samples)\n",
    "\n",
    "    original_size = sum(len(sample.encode(\"utf-8\")) for sample in samples)\n",
    "    compressed_size = 0\n",
    "    for _ in token_ids:\n",
    "        # Each token ID is a single integer, so we count each as 1 byte\n",
    "        compressed_size += 1\n",
    "    return original_size / compressed_size if compressed_size > 0 else 0\n",
    "\n",
    "\n",
    "tiny_stories_ratio = calculate_compression_ratio(\"TinyStoriesV2-GPT4\", 8)\n",
    "print(f\"TinyStories compression ratio: {tiny_stories_ratio:.2f}\")\n",
    "\n",
    "owt_ratio = calculate_compression_ratio(\"owt\", 64)\n",
    "print(f\"OpenWebText compression ratio: {owt_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57f0c0",
   "metadata": {},
   "source": [
    "#### (b) What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Compare the compression ratio and/or qualitatively describe what happens.\n",
    "OpenWebText compression ratio using TinyStories tokenizer: 3.22. The compression ratio drops significantly, indicating that the TinyStories tokenizer is less effective at compressing the OpenWebText dataset compared to its native tokenizer. That is because the TinyStories tokenizer is more specialized for shorter, more narrative texts, while OpenWebText contains a wider variety of content and styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41e5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenWebText compression ratio using TinyStories tokenizer: 3.22\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.bpe_tokenizer.encoder import load_bpe_tokenizer, sample_from_file\n",
    "\n",
    "\n",
    "input_path = f\"data/owt_train.txt\"\n",
    "vocab_path = f\"data/TinyStoriesV2-GPT4_vocab.json\"\n",
    "merge_path = f\"data/TinyStoriesV2-GPT4_merges.txt\"\n",
    "\n",
    "sample_size = 10\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "\n",
    "tokenizer = load_bpe_tokenizer(vocab_path, merge_path, special_tokens)\n",
    "samples = sample_from_file(input_path, sample_size, special_tokens, 64)\n",
    "token_ids = tokenizer.encode_iterable(samples)\n",
    "\n",
    "original_size = sum(len(sample.encode(\"utf-8\")) for sample in samples)\n",
    "compressed_size = 0\n",
    "for _ in token_ids:\n",
    "    # Each token ID is a single integer, so we count each as 1 byte\n",
    "    compressed_size += 1\n",
    "compressed_ratio = original_size / compressed_size if compressed_size > 0 else 0\n",
    "\n",
    "print(f\"OpenWebText compression ratio using TinyStories tokenizer: {compressed_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0e2d9",
   "metadata": {},
   "source": [
    "#### (c) Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text)?\n",
    "Mean throughputs for TinyStories: 16170108.27 bytes/second. Time needed to process 825GB with TinyStories: 54782.38 seconds(roughly 15.2 hours).  \n",
    "Mean throughputs for OpenWebText: 12334632.05 bytes/second. Time needed to process 825GB with OpenWebText: 71817.06 seconds(roughly 19.9 hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af98e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyStories throughput: 15806786.31 bytes/second\n",
      "OpenWebText throughput: 11361867.14 bytes/second\n",
      "TinyStories throughput: 16391106.94 bytes/second\n",
      "OpenWebText throughput: 12586252.07 bytes/second\n",
      "TinyStories throughput: 16312431.57 bytes/second\n",
      "OpenWebText throughput: 13055776.93 bytes/second\n",
      "Mean throughputs for TinyStories: 16170108.27 bytes/second\n",
      "Time needed to process 825GB with TinyStories: 54782.38 seconds\n",
      "Mean throughputs for OpenWebText: 12334632.05 bytes/second\n",
      "Time needed to process 825GB with OpenWebText: 71817.06 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from cs336_basics.bpe_tokenizer.encoder import load_bpe_tokenizer\n",
    "\n",
    "\n",
    "def calculate_throughput(document_name, read_mb: int = 128):\n",
    "    if document_name == \"TinyStoriesV2-GPT4\":\n",
    "        input_path = f\"data/{document_name}-train.txt\"\n",
    "    elif document_name == \"owt\":\n",
    "        input_path = f\"data/{document_name}_train.txt\"\n",
    "    vocab_path = f\"data/{document_name}_vocab.json\"\n",
    "    merge_path = f\"data/{document_name}_merges.txt\"\n",
    "\n",
    "    special_tokens = [\"<|endoftext|>\"]\n",
    "    tokenizer = load_bpe_tokenizer(vocab_path, merge_path, special_tokens)\n",
    "\n",
    "    read_bytes = read_mb * 1024 * 1024  # Read specified MB\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read(read_bytes)\n",
    "\n",
    "    start_time = time.time()\n",
    "    _ = list(tokenizer.encode_iterable([text]))\n",
    "    end_time = time.time()\n",
    "\n",
    "    return read_bytes / (end_time - start_time) if end_time - start_time > 0 else 0\n",
    "\n",
    "\n",
    "tiny_stories_throughputs = []\n",
    "owt_throughputs = []\n",
    "\n",
    "for mb in [128, 256, 512]:\n",
    "    tiny_stories_throughput = calculate_throughput(\"TinyStoriesV2-GPT4\", read_mb=mb)\n",
    "    tiny_stories_throughputs.append(tiny_stories_throughput)\n",
    "    print(f\"TinyStories throughput: {tiny_stories_throughput:.2f} bytes/second\")\n",
    "\n",
    "    owt_throughput = calculate_throughput(\"owt\", read_mb=mb)\n",
    "    owt_throughputs.append(owt_throughput)\n",
    "    print(f\"OpenWebText throughput: {owt_throughput:.2f} bytes/second\")\n",
    "\n",
    "mean_tiny_stories_throughput = np.mean(tiny_stories_throughputs)\n",
    "print(f\"Mean throughputs for TinyStories: {mean_tiny_stories_throughput:.2f} bytes/second\")\n",
    "time_needed_for_825GB = 825 * 1024 * 1024 * 1024 / mean_tiny_stories_throughput\n",
    "print(f\"Time needed to process 825GB with TinyStories: {time_needed_for_825GB:.2f} seconds\")\n",
    "\n",
    "mean_owt_throughput = np.mean(owt_throughputs)\n",
    "print(f\"Mean throughputs for OpenWebText: {mean_owt_throughput:.2f} bytes/second\")\n",
    "time_needed_for_825GB = 825 * 1024 * 1024 * 1024 / mean_owt_throughput\n",
    "print(f\"Time needed to process 825GB with OpenWebText: {time_needed_for_825GB:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebfa9a",
   "metadata": {},
   "source": [
    "#### (d) Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token IDs. We'll use this later to train our language model. We recommend serializing the token IDs as a NumPy array of datatype uint16. Why is uint16 an appropriate choice?\n",
    "Using `uint16` is appropriate because:\n",
    "1. It provides a sufficient range to represent all possible token IDs (up to 65535), which is more than enough for most tokenizers.\n",
    "2. It uses less memory compared to larger integer types (like `int32` or `int64`), making it more efficient for storing large arrays of token IDs.\n",
    "3. Many NLP tasks involve working with large datasets, and reducing memory usage can lead to faster training times and lower resource consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac2cfd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-16 23:59:06.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mEncoding file data/TinyStoriesV2-GPT4-valid.txt using 8 chunks...\u001b[0m\n",
      "2025-08-16 23:59:07.280 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 5625758-8438541, got 683132 tokens\n",
      "2025-08-16 23:59:07.292 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 0-2813541, got 683867 tokens\n",
      "2025-08-16 23:59:07.295 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 14064356-16877372, got 684968 tokens\n",
      "2025-08-16 23:59:07.301 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 8438541-11252559, got 682702 tokens\n",
      "2025-08-16 23:59:07.309 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 11252559-14064356, got 681943 tokens\n",
      "2025-08-16 23:59:07.311 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 2813541-5625758, got 682990 tokens\n",
      "2025-08-16 23:59:07.326 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 16877372-19690315, got 683618 tokens\n",
      "2025-08-16 23:59:07.326 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 19690315-22502601, got 682743 tokens\n",
      "\u001b[32m2025-08-16 23:59:07.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mEncoding completed. Saving to data/TinyStoriesV2-GPT4-valid_encoded_10k.npy...\u001b[0m\n",
      "\u001b[32m2025-08-16 23:59:07.526\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m113\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/TinyStoriesV2-GPT4-valid_encoded_10k.npy\u001b[0m\n",
      "\u001b[32m2025-08-16 23:59:07.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mEncoding file data/TinyStoriesV2-GPT4-train.txt using 8 chunks...\u001b[0m\n",
      "2025-08-16 23:59:49.600 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 556938507-835407773, got 67647604 tokens\n",
      "2025-08-17 00:00:08.067 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 0-278469249, got 67657785 tokens\n",
      "2025-08-17 00:00:14.128 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 278469249-556938507, got 67661088 tokens\n",
      "2025-08-17 00:00:15.077 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 1113876594-1392345974, got 67648334 tokens\n",
      "2025-08-17 00:00:15.200 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 1949284164-2227753162, got 67646897 tokens\n",
      "2025-08-17 00:00:15.298 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 835407773-1113876594, got 67658531 tokens\n",
      "2025-08-17 00:00:15.337 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 1392345974-1670815354, got 67649538 tokens\n",
      "2025-08-17 00:00:15.346 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 1670815354-1949284164, got 67665251 tokens\n",
      "\u001b[32m2025-08-17 00:00:22.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mEncoding completed. Saving to data/TinyStoriesV2-GPT4-train_encoded_10k.npy...\u001b[0m\n",
      "\u001b[32m2025-08-17 00:00:36.638\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m113\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/TinyStoriesV2-GPT4-train_encoded_10k.npy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.bpe_tokenizer.encoder import (\n",
    "    encode_file,\n",
    "    load_bpe_tokenizer,\n",
    ")\n",
    "\n",
    "vocab_filepath = f\"data/TinyStoriesV2-GPT4_vocab.json\"\n",
    "merge_filepath = f\"data/TinyStoriesV2-GPT4_merges.txt\"\n",
    "tokenizer = load_bpe_tokenizer(\n",
    "    vocab_path=vocab_filepath,\n",
    "    merge_path=merge_filepath,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "for train_or_valid in [\"valid\", \"train\"]:\n",
    "    input_path = f\"data/TinyStoriesV2-GPT4-{train_or_valid}.txt\"\n",
    "    output_file = f\"data/TinyStoriesV2-GPT4-{train_or_valid}_encoded_10k.npy\"\n",
    "\n",
    "    encode_file(input_path, tokenizer, output_file, use_memmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-17 00:03:06.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mEncoding file data/owt_valid.txt using 64 chunks...\u001b[0m\n",
      "2025-08-17 00:03:07.702 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 0-4534646, got 1028482 tokens\n",
      "2025-08-17 00:03:07.733 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 18128078-22660565, got 1028150 tokens\n",
      "2025-08-17 00:03:07.733 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 45313693-49843604, got 1034177 tokens\n",
      "2025-08-17 00:03:07.757 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 9069135-13595080, got 1045635 tokens\n",
      "2025-08-17 00:03:07.758 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 27188872-31721170, got 1040461 tokens\n",
      "2025-08-17 00:03:07.815 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 54375317-58913104, got 1037925 tokens\n",
      "2025-08-17 00:03:07.825 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 36335216-40783338, got 1011621 tokens\n",
      "2025-08-17 00:03:07.837 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 63437663-67969883, got 1043999 tokens\n",
      "2025-08-17 00:03:08.447 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 4534646-9069135, got 1067188 tokens\n",
      "2025-08-17 00:03:08.501 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 22660565-27188872, got 1032424 tokens\n",
      "2025-08-17 00:03:08.503 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 31721170-36335216, got 1062444 tokens\n",
      "2025-08-17 00:03:08.516 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 13595080-18128078, got 1049751 tokens\n",
      "2025-08-17 00:03:08.524 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 49843604-54375317, got 1041034 tokens\n",
      "2025-08-17 00:03:08.551 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 58913104-63437663, got 1028784 tokens\n",
      "2025-08-17 00:03:08.589 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 40783338-45313693, got 1025814 tokens\n",
      "2025-08-17 00:03:08.599 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 67969883-72505172, got 1035285 tokens\n",
      "2025-08-17 00:03:09.333 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 72505172-77067267, got 1044226 tokens\n",
      "2025-08-17 00:03:09.450 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 81566659-86098153, got 1028400 tokens\n",
      "2025-08-17 00:03:09.475 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 90631022-95158220, got 1022523 tokens\n",
      "2025-08-17 00:03:09.550 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 99688142-104219393, got 1032682 tokens\n",
      "2025-08-17 00:03:09.597 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 108752143-113288220, got 1051212 tokens\n",
      "2025-08-17 00:03:09.677 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 117820142-122346510, got 1057710 tokens\n",
      "2025-08-17 00:03:09.728 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 126907149-131405991, got 1026097 tokens\n",
      "2025-08-17 00:03:09.791 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 135951845-140480446, got 1025940 tokens\n",
      "2025-08-17 00:03:10.134 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 77067267-81566659, got 1029391 tokens\n",
      "2025-08-17 00:03:10.241 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 95158220-99688142, got 1035905 tokens\n",
      "2025-08-17 00:03:10.262 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 86098153-90631022, got 1032362 tokens\n",
      "2025-08-17 00:03:10.326 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 104219393-108752143, got 1044658 tokens\n",
      "2025-08-17 00:03:10.360 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 113288220-117820142, got 1028872 tokens\n",
      "2025-08-17 00:03:10.469 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 122346510-126907149, got 1047143 tokens\n",
      "2025-08-17 00:03:10.551 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 131405991-135951845, got 1034417 tokens\n",
      "2025-08-17 00:03:10.629 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 140480446-145027268, got 1061690 tokens\n",
      "2025-08-17 00:03:11.114 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 145027268-149531541, got 1040820 tokens\n",
      "2025-08-17 00:03:11.219 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 154062360-158596478, got 1036109 tokens\n",
      "2025-08-17 00:03:11.343 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 172189059-176721827, got 1032685 tokens\n",
      "2025-08-17 00:03:11.377 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 163130645-167656197, got 1036126 tokens\n",
      "2025-08-17 00:03:11.385 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 181256470-185783171, got 1029433 tokens\n",
      "2025-08-17 00:03:11.455 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 190330317-194844995, got 1038076 tokens\n",
      "2025-08-17 00:03:11.551 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 199377433-203908570, got 1032600 tokens\n",
      "2025-08-17 00:03:11.667 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 208438569-212975889, got 1041480 tokens\n",
      "2025-08-17 00:03:11.932 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 149531541-154062360, got 1047819 tokens\n",
      "2025-08-17 00:03:11.984 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 158596478-163130645, got 1036604 tokens\n",
      "2025-08-17 00:03:12.139 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 176721827-181256470, got 1037073 tokens\n",
      "2025-08-17 00:03:12.141 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 185783171-190330317, got 1027090 tokens\n",
      "2025-08-17 00:03:12.205 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 194844995-199377433, got 1022642 tokens\n",
      "2025-08-17 00:03:12.210 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 167656197-172189059, got 1058687 tokens\n",
      "2025-08-17 00:03:12.389 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 203908570-208438569, got 1039799 tokens\n",
      "2025-08-17 00:03:12.482 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 212975889-217499287, got 1038039 tokens\n",
      "2025-08-17 00:03:12.948 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 217499287-222033931, got 1083131 tokens\n",
      "2025-08-17 00:03:13.043 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 226591681-231141997, got 1080313 tokens\n",
      "2025-08-17 00:03:13.164 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 235626620-240171975, got 1034165 tokens\n",
      "2025-08-17 00:03:13.249 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 244692695-249221446, got 1035273 tokens\n",
      "2025-08-17 00:03:13.335 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 253752435-258281502, got 1031525 tokens\n",
      "2025-08-17 00:03:13.365 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 262813107-267342754, got 1031366 tokens\n",
      "2025-08-17 00:03:13.431 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 271879422-276407548, got 1026560 tokens\n",
      "2025-08-17 00:03:13.438 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 280942347-285470218, got 1017671 tokens\n",
      "2025-08-17 00:03:13.702 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 222033931-226591681, got 1032930 tokens\n",
      "2025-08-17 00:03:13.797 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 231141997-235626620, got 1026146 tokens\n",
      "2025-08-17 00:03:13.963 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 240171975-244692695, got 1039724 tokens\n",
      "2025-08-17 00:03:14.033 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 249221446-253752435, got 1028222 tokens\n",
      "2025-08-17 00:03:14.116 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 267342754-271879422, got 1032075 tokens\n",
      "2025-08-17 00:03:14.119 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 258281502-262813107, got 1048816 tokens\n",
      "2025-08-17 00:03:14.163 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 276407548-280942347, got 1036206 tokens\n",
      "2025-08-17 00:03:14.164 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 285470218-289998753, got 1040547 tokens\n",
      "\u001b[32m2025-08-17 00:03:14.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mEncoding completed. Saving to data/owt_valid_encoded_32k.npy...\u001b[0m\n",
      "\u001b[32m2025-08-17 00:03:15.404\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m113\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/owt_valid_encoded_32k.npy\u001b[0m\n",
      "\u001b[32m2025-08-17 00:03:15.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mEncoding file data/owt_train.txt using 64 chunks...\u001b[0m\n",
      "2025-08-17 00:03:58.671 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 0-186264074, got 42685045 tokens\n",
      "2025-08-17 00:04:00.185 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 745033759-931290897, got 42750272 tokens\n",
      "2025-08-17 00:04:06.068 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 1117547968-1303809074, got 42574558 tokens\n",
      "2025-08-17 00:04:09.200 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 372525189-558778316, got 42624949 tokens\n",
      "2025-08-17 00:04:10.423 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 1862581364-2048842027, got 42686508 tokens\n",
      "2025-08-17 00:04:10.538 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 2235103536-2421360901, got 42625852 tokens\n",
      "2025-08-17 00:04:10.538 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 1490070394-1676321928, got 42683329 tokens\n",
      "2025-08-17 00:04:11.645 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 2607613592-2793870054, got 42717783 tokens\n",
      "2025-08-17 00:05:03.160 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 186264074-372525189, got 42648647 tokens\n",
      "2025-08-17 00:05:06.626 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 931290897-1117547968, got 42596734 tokens\n",
      "2025-08-17 00:05:27.800 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 1303809074-1490070394, got 42680361 tokens\n",
      "2025-08-17 00:05:27.855 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 2421360901-2607613592, got 42604954 tokens\n",
      "2025-08-17 00:05:27.863 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 2793870054-2980128270, got 42644359 tokens\n",
      "2025-08-17 00:05:27.871 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 1676321928-1862581364, got 42661345 tokens\n",
      "2025-08-17 00:05:27.870 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 558778316-745033759, got 42698791 tokens\n",
      "2025-08-17 00:05:27.880 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 2048842027-2235103536, got 42528943 tokens\n",
      "2025-08-17 00:06:02.944 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 2980128270-3166387668, got 42667792 tokens\n",
      "2025-08-17 00:06:05.898 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 3352643934-3538948062, got 42670233 tokens\n",
      "2025-08-17 00:06:39.668 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 3725160533-3911419547, got 42692802 tokens\n",
      "2025-08-17 00:06:54.137 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 4097677743-4283934349, got 42686112 tokens\n",
      "2025-08-17 00:06:54.295 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 4470223005-4656454687, got 42688698 tokens\n",
      "2025-08-17 00:06:57.123 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 4842707817-5028967053, got 42845251 tokens\n",
      "2025-08-17 00:07:15.713 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 3538948062-3725160533, got 42697420 tokens\n",
      "2025-08-17 00:07:18.609 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 5215236910-5401483870, got 42595431 tokens\n",
      "2025-08-17 00:07:21.437 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 3166387668-3352643934, got 42552239 tokens\n",
      "2025-08-17 00:07:32.669 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 5587774742-5774001743, got 42625668 tokens\n",
      "2025-08-17 00:08:04.353 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 3911419547-4097677743, got 42555264 tokens\n",
      "2025-08-17 00:08:20.340 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 5028967053-5215236910, got 42626272 tokens\n",
      "2025-08-17 00:08:20.344 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 4656454687-4842707817, got 42638415 tokens\n",
      "2025-08-17 00:08:32.127 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 4283934349-4470223005, got 42620213 tokens\n",
      "2025-08-17 00:08:44.464 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 5960269363-6146532219, got 42743669 tokens\n",
      "2025-08-17 00:08:48.451 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 6332776447-6519030430, got 42767805 tokens\n",
      "2025-08-17 00:08:56.450 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 5774001743-5960269363, got 42616624 tokens\n",
      "2025-08-17 00:08:59.864 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 5401483870-5587774742, got 42719786 tokens\n",
      "2025-08-17 00:09:27.882 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 6705307054-6891548899, got 42651501 tokens\n",
      "2025-08-17 00:09:53.299 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 7450321387-7636593636, got 42620438 tokens\n",
      "2025-08-17 00:09:54.748 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 7077805242-7264078758, got 42741000 tokens\n",
      "2025-08-17 00:10:01.807 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 6146532219-6332776447, got 42669693 tokens\n",
      "2025-08-17 00:10:04.805 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 7822836862-8009093929, got 42617096 tokens\n",
      "2025-08-17 00:10:17.579 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 6519030430-6705307054, got 42690247 tokens\n",
      "2025-08-17 00:10:20.109 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 8195351909-8381624875, got 42543030 tokens\n",
      "2025-08-17 00:10:22.145 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 8567875847-8754139195, got 42659120 tokens\n",
      "2025-08-17 00:10:47.660 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 6891548899-7077805242, got 42634502 tokens\n",
      "2025-08-17 00:11:41.568 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 9312899502-9499161249, got 42536074 tokens\n",
      "2025-08-17 00:11:44.991 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 8009093929-8195351909, got 42772514 tokens\n",
      "2025-08-17 00:11:48.368 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 8940385101-9126642442, got 42586874 tokens\n",
      "2025-08-17 00:11:58.500 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 7636593636-7822836862, got 42539175 tokens\n",
      "2025-08-17 00:11:59.865 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 7264078758-7450321387, got 42544172 tokens\n",
      "2025-08-17 00:12:04.667 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 8754139195-8940385101, got 42586799 tokens\n",
      "2025-08-17 00:12:05.476 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 8381624875-8567875847, got 42628780 tokens\n",
      "2025-08-17 00:12:24.839 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 9685416176-9871702170, got 42557493 tokens\n",
      "2025-08-17 00:13:18.982 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 10057946906-10244190271, got 42676317 tokens\n",
      "2025-08-17 00:13:39.447 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 9126642442-9312899502, got 42470964 tokens\n",
      "2025-08-17 00:13:53.936 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 10430448049-10616709913, got 42535914 tokens\n",
      "2025-08-17 00:14:00.032 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 9499161249-9685416176, got 42633418 tokens\n",
      "2025-08-17 00:14:00.844 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 10802971165-10989230078, got 42642565 tokens\n",
      "2025-08-17 00:14:00.872 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 9871702170-10057946906, got 42679546 tokens\n",
      "2025-08-17 00:14:00.985 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 11547998454-11734253098, got 42629728 tokens\n",
      "2025-08-17 00:14:07.109 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 11175481314-11361744763, got 42486650 tokens\n",
      "2025-08-17 00:14:32.350 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 10244190271-10430448049, got 42608553 tokens\n",
      "2025-08-17 00:14:46.458 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 10616709913-10802971165, got 42541490 tokens\n",
      "2025-08-17 00:14:47.058 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 11734253098-11920511059, got 42644689 tokens\n",
      "2025-08-17 00:14:48.931 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 10989230078-11175481314, got 42579116 tokens\n",
      "2025-08-17 00:14:49.953 | DEBUG    | cs336_basics.bpe_tokenizer.encoder:tokenize_chunk:65 - Tokenized chunk 11361744763-11547998454, got 42574705 tokens\n",
      "\u001b[32m2025-08-17 00:14:55.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1mEncoding completed. Saving to data/owt_train_encoded_32k.npy...\u001b[0m\n",
      "\u001b[32m2025-08-17 00:19:35.590\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mcs336_basics.bpe_tokenizer.encoder\u001b[0m:\u001b[36mencode_file\u001b[0m:\u001b[36m113\u001b[0m - \u001b[32m\u001b[1mSaved token array to data/owt_train_encoded_32k.npy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.bpe_tokenizer.encoder import (\n",
    "    encode_file,\n",
    "    load_bpe_tokenizer,\n",
    ")\n",
    "\n",
    "vocab_filepath = f\"data/owt_vocab.json\"\n",
    "merge_filepath = f\"data/owt_merges.txt\"\n",
    "tokenizer = load_bpe_tokenizer(\n",
    "    vocab_path=vocab_filepath,\n",
    "    merge_path=merge_filepath,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "for train_or_valid in [\"valid\", \"train\"]:\n",
    "    input_path = f\"data/owt_{train_or_valid}.txt\"\n",
    "    output_file = f\"data/owt_{train_or_valid}_encoded_32k.npy\"\n",
    "\n",
    "    encode_file(input_path, tokenizer, output_file, use_memmap=True, desired_num_chunks=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b457a",
   "metadata": {},
   "source": [
    "### Einsum\n",
    "> It turns out almost all operations in machine learning are some combination of dimension juggling and tensor contraction with the occasional (usually pointwise) nonlinear function. This means that a lot of your code can be more readable and flexible when using einsum notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 128])\n",
      "torch.Size([32, 128, 128])\n",
      "torch.Size([32, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Example (einstein_example1): Batched matrix multiplication with einops.einsum\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "D = torch.randn(32, 128, 64)\n",
    "A = torch.randn(128, 64)\n",
    "\n",
    "## Basic implementation\n",
    "Y = D @ A.T\n",
    "print(Y.shape)\n",
    "# Hard to tell the input and output shapes and what they mean.\n",
    "# What shapes can D and A have, and do any of these have unexpected behavior?\n",
    "## Einsum is self-documenting and robust\n",
    "# D A -> Y\n",
    "Y = einsum(D, A, \"batch sequence d_in, d_out d_in -> batch sequence d_out\")\n",
    "print(Y.shape)\n",
    "## Or, a batched version where D can have any leading dimensions but A is constrained.\n",
    "Y = einsum(D, A, \"... d_in, d_out d_in -> ... d_out\")\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d4079f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 128, 128, 3])\n",
      "torch.Size([64, 10, 128, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example (einstein_example2): Broadcasted operations with einops.rearrange\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "\"\"\"\n",
    "We have a batch of images, and for each image we want to generate 10 dimmed versions based on some scaling factor:\n",
    "\"\"\"\n",
    "images = torch.randn(64, 128, 128, 3)  # (batch, height, width, channel)\n",
    "dim_by = torch.linspace(start=0.0, end=1.0, steps=10)\n",
    "## Reshape and multiply\n",
    "dim_value = rearrange(dim_by, \"dim_value -> 1 dim_value 1 1 1\")\n",
    "images_rearr = rearrange(images, \"b height width channel -> b 1 height width channel\")\n",
    "dimmed_images = images_rearr * dim_value\n",
    "print(dimmed_images.shape)\n",
    "## Or in one go:\n",
    "dimmed_images = einsum(\n",
    "    images,\n",
    "    dim_by,\n",
    "    \"batch height width channel, dim_value -> batch dim_value height width channel\",\n",
    ")\n",
    "print(dimmed_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748e741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 32, 3]) torch.Size([1024, 1024])\n",
      "torch.Size([64, 1024, 3])\n",
      "torch.Size([64, 3, 1024])\n",
      "torch.Size([64, 3, 1024])\n",
      "torch.Size([64, 1024, 3])\n",
      "torch.Size([64, 32, 32, 3])\n",
      "torch.Size([64, 3, 1024])\n",
      "torch.Size([64, 3, 1024])\n",
      "torch.Size([64, 32, 32, 3])\n",
      "torch.Size([64, 32, 32, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example (einstein_example3): Pixel mixing with einops.rearrange\n",
    "\n",
    "import torch\n",
    "import einx\n",
    "from einops import rearrange, einsum\n",
    "\n",
    "\"\"\"\n",
    "Suppose we have a batch of images represented as a tensor of shape (batch, height, width,\n",
    "channel), and we want to perform a linear transformation across all pixels of the image, but this\n",
    "transformation should happen independently for each channel. Our linear transformation is\n",
    "represented as a matrix B of shape (height * width, height * width).\n",
    "\"\"\"\n",
    "channels_last = torch.randn(64, 32, 32, 3)  # (batch, height, width, channel)\n",
    "B = torch.randn(32 * 32, 32 * 32)  # (pixel_out, pixel_in)\n",
    "print(channels_last.shape, B.shape)\n",
    "## Rearrange an image tensor for mixing across all pixels\n",
    "# (batch, height * width, channel)\n",
    "channels_last_flat = channels_last.view(-1, channels_last.size(1) * channels_last.size(2), channels_last.size(3))\n",
    "print(channels_last_flat.shape)\n",
    "# (batch, channel, height * width)\n",
    "channels_first_flat = channels_last_flat.transpose(1, 2)\n",
    "print(channels_first_flat.shape)\n",
    "# (batch, channel, height * width)\n",
    "channels_first_flat_transformed = channels_first_flat @ B.T\n",
    "print(channels_first_flat_transformed.shape)\n",
    "# (batch, height * width, channel)\n",
    "channels_last_flat_transformed = channels_first_flat_transformed.transpose(1, 2)\n",
    "print(channels_last_flat_transformed.shape)\n",
    "# (batch, height, width, channel)\n",
    "channels_last_transformed = channels_last_flat_transformed.view(*channels_last.shape)\n",
    "print(channels_last_transformed.shape)\n",
    "\n",
    "\"\"\"\n",
    "Instead, using einops:\n",
    "\"\"\"\n",
    "height = width = 32\n",
    "## Rearrange replaces clunky torch view + transpose\n",
    "channels_first = rearrange(channels_last, \"batch height width channel -> batch channel (height width)\")\n",
    "print(channels_first.shape)\n",
    "channels_first_transformed = einsum(\n",
    "    channels_first,\n",
    "    B,\n",
    "    \"batch channel pixel_in, pixel_out pixel_in -> batch channel pixel_out\",\n",
    ")\n",
    "print(channels_first_transformed.shape)\n",
    "channels_last_transformed = rearrange(\n",
    "    channels_first_transformed,\n",
    "    \"batch channel (height width) -> batch height width channel\",\n",
    "    height=height,\n",
    "    width=width,\n",
    ")\n",
    "print(channels_last_transformed.shape)\n",
    "\"\"\"\n",
    "Or, if you're feeling crazy: all in one go using einx.dot (einx equivalent of einops.einsum)\n",
    "\"\"\"\n",
    "height = width = 32\n",
    "channels_last_transformed = einx.dot(\n",
    "    \"batch row_in col_in channel, (row_out col_out) (row_in col_in)\" \"-> batch row_out col_out channel\",\n",
    "    channels_last,\n",
    "    B,\n",
    "    col_in=width,\n",
    "    col_out=width,\n",
    ")\n",
    "print(channels_last_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23640d2b",
   "metadata": {},
   "source": [
    "### Problem (transformer_accounting): Transformer LM resource accounting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f456e",
   "metadata": {},
   "source": [
    "#### (a) Consider GPT-2 XL, which has the following configuration:\n",
    "- $vocab_size: 50,257$  \n",
    "- $context_length: 1,024$  \n",
    "- $num_layers: 48$  \n",
    "- $d_model: 1,600$  \n",
    "- $num_heads: 25$  \n",
    "- $d_ff: 6,400$  \n",
    "Suppose we constructed our model using this configuration. How many trainable parameters\n",
    "would our model have? Assuming each parameter is represented using single-precision floating point, how much memory is required to just load this model?\n",
    "\n",
    "Embedding: $50257 \\times 1600[wte] + 1024 \\times 1600[wpe]$  \n",
    "Transformer blocks: $48 \\times ((1600 + 1600)[ln\\_1] + (4800 \\times 1600 + 4800)[c\\_attn] + (1600 \\times 1600 + 1600)[c\\_proj] + (1600 + 1600)[ln\\_2] + (6400 \\times 1600 + 6400)[c\\_fc] + (1600 \\times 6400 + 1600)[c\\_proj])$  \n",
    "LayerNorm: $1600 + 1600$  \n",
    "Linear: $1600 \\times 50257$  \n",
    "Total trainable parameters: $1,557,611,200$  (Linear parameters shares weights with wte, so only count once)\n",
    "Memory required: $1,557,611,200 \\times 4$ bytes = $6,230,444,800$ bytes (approximately 5.80 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e770a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1,638,022,400, Trainable parameters: 1,557,611,200\n",
      "<bound method Module.modules of GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1600)\n",
      "    (wpe): Embedding(1024, 1600)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-47): 48 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=4800, nx=1600)\n",
      "          (c_proj): Conv1D(nf=1600, nx=1600)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=6400, nx=1600)\n",
      "          (c_proj): Conv1D(nf=1600, nx=6400)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
      ")>\n",
      "Model has 1,557,611,200 parameters.\n",
      "Memory required: 6,230,444,800 bytes = 5.80 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "embedding_parameters = 50257 * 1600 + 1024 * 1600\n",
    "transformer_block_parameters = 48 * (\n",
    "    (1600 + 1600)\n",
    "    + (4800 * 1600 + 4800)\n",
    "    + (1600 * 1600 + 1600)\n",
    "    + (1600 + 1600)\n",
    "    + (6400 * 1600 + 6400)\n",
    "    + (1600 * 6400 + 1600)\n",
    ")\n",
    "layernorm_parameters = 1600 + 1600\n",
    "linear_parameters = 1600 * 50257\n",
    "total_parameters = embedding_parameters + transformer_block_parameters + layernorm_parameters + linear_parameters\n",
    "# Exclude the final linear layer since it shares weights with the embedding layer\n",
    "total_trainable_parameters = total_parameters - linear_parameters\n",
    "\n",
    "print(f\"Total parameters: {total_parameters:,}, Trainable parameters: {total_trainable_parameters:,}\")\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=50257,\n",
    "    n_positions=1024,  # context length\n",
    "    n_layer=48,  # num_layers\n",
    "    n_embd=1600,  # d_model\n",
    "    n_head=25,  # num_heads\n",
    "    n_inner=6400,  # d_ff (feed-forward dimension, usually 4 * d_model for GPT-2 models)\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "print(model.modules)\n",
    "print(f\"Model has {model.num_parameters(only_trainable=True):,} parameters.\")\n",
    "\n",
    "print(\n",
    "    f\"Memory required: {model.num_parameters(only_trainable=True) * 4:,} bytes = {model.num_parameters(only_trainable=True) * 4 / (1024 ** 3):.2f} GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e170e6e",
   "metadata": {},
   "source": [
    "#### (b) Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped model. How many FLOPs do these matrix multiplies require in total? Assume that our input sequence has context_length tokens.\n",
    "Deliverable: A list of matrix multiplies (with descriptions), and the total number of FLOPs required.  \n",
    "\n",
    "> **Resource accounting**. It is useful to be able to understand how the various parts of the Transformer consume compute and memory. We will go through the steps to do some basic \"FLOPs accounting\". The vast majority of FLOPS in a Transformer are matrix multiplies, so our core approach is simple:  \n",
    "> \n",
    "> 1. Write down all the matrix multiplies in a Transformer forward pass.  \n",
    "> 2. Convert each matrix multiply into FLOPs required.  \n",
    "> \n",
    "> For this second step, the following fact will be useful:  \n",
    "> **Rule:** $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$, the matrix-matrix product $AB$ requires $2mnp$ FLOPs.  \n",
    "> To see this, note that $(AB)[i, j] = A[i, :] · B[:, j]$, and that this dot product requires $n$ additions and $n$ multiplications ($2n$ FLOPs). Then, since the matrix-matrix product $AB$ has $m \\times p$ entries, the total number of FLOPS is $(2n)(mp) = 2mnp$.  \n",
    "\n",
    "Embedding layer requires no matrix multiplications but only look up the table.  \n",
    "**Main matrix multiplies per Transformer block:**\n",
    "1. **QKV projection (`c_attn`)**:  \n",
    "   - Shape: (context_length, d_model) × (d_model, 3 × d_model)  \n",
    "   - FLOPs: $2 \\times 1024 \\times 1600 \\times 4800$\n",
    "\n",
    "2. **QK^T (attention scores)**:  \n",
    "   - Shape: (context_length, d_k) × (d_k, context_length), per head, d_k = d_model / num_heads = 64  \n",
    "   - FLOPs: $2 \\times 25 \\times 1024 \\times 64 \\times 1024$\n",
    "\n",
    "3. **Attention weights × V**:  \n",
    "   - Shape: (context_length, context_length) × (context_length, d_k), per head  \n",
    "   - FLOPs: $2 \\times 25 \\times 1024 \\times 1024 \\times 64$\n",
    "\n",
    "4. **Output projection (`c_proj`)**:  \n",
    "   - Shape: (context_length, d_model) × (d_model, d_model)  \n",
    "   - FLOPs: $2 \\times 1024 \\times 1600 \\times 1600$\n",
    "\n",
    "5. **MLP first layer (`c_fc`)**:  \n",
    "   - Shape: (context_length, d_model) × (d_model, d_ff)  \n",
    "   - FLOPs: $2 \\times 1024 \\times 1600 \\times 6400$\n",
    "\n",
    "6. **MLP second layer (`c_proj`)**:  \n",
    "   - Shape: (context_length, d_ff) × (d_ff, d_model)  \n",
    "   - FLOPs: $2 \\times 1024 \\times 6400 \\times 1600$\n",
    "\n",
    "**Output layer:**\n",
    "- Shape: (context_length, d_model) × (d_model, vocab_size)  \n",
    "- FLOPs: $2 \\times 1024 \\times 1600 \\times 50257$\n",
    "\n",
    "---\n",
    "**Grand total:**  \n",
    "$\\boxed{3,506,703,564,800}$ FLOPs per forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153f30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ModelConfig(name='GPT2_XL', vocab_size=50257, context_length=1024, num_layers=48, d_model=1600, num_heads=25, d_ff=6400)\n",
      "QKV projection            : 15,728,640,000\n",
      "QK^T                      : 3,355,443,200\n",
      "Attention score           : 3,355,443,200\n",
      "C_proj_1                  : 5,242,880,000\n",
      "[Attention mechanism]     : 22,439,526,400\n",
      "C_fc                      : 20,971,520,000\n",
      "C_proj_2                  : 20,971,520,000\n",
      "[Transformer block]       : 69,625,446,400\n",
      "[Total Transformer blocks]: 3,342,021,427,200\n",
      "Output layer              : 164,682,137,600\n",
      "Each token: 3,506,703,564,800\n",
      "Proportions:\n",
      "Attention: 37.89% [QKV projection: 21.53%, QK^T: 4.59%, Attention score: 4.59%, (Attention mechanism: 30.72%) C_proj_1: 7.18%]\n",
      "MLP: 57.41% [C_fc: 28.71%, C_proj_2: 28.71%]\n",
      "Output layer: 4.70%\n",
      "\n",
      "==================================================\n",
      "\n",
      "Config: ModelConfig(name='GPT2_small', vocab_size=50257, context_length=1024, num_layers=12, d_model=768, num_heads=12, d_ff=3072)\n",
      "QKV projection            : 3,623,878,656\n",
      "QK^T                      : 1,610,612,736\n",
      "Attention score           : 1,610,612,736\n",
      "C_proj_1                  : 1,207,959,552\n",
      "[Attention mechanism]     : 6,845,104,128\n",
      "C_fc                      : 4,831,838,208\n",
      "C_proj_2                  : 4,831,838,208\n",
      "[Transformer block]       : 17,716,740,096\n",
      "[Total Transformer blocks]: 212,600,881,152\n",
      "Output layer              : 79,047,426,048\n",
      "Each token: 291,648,307,200\n",
      "Proportions:\n",
      "Attention: 33.13% [QKV projection: 14.91%, QK^T: 6.63%, Attention score: 6.63%, (Attention mechanism: 28.16%) C_proj_1: 4.97%]\n",
      "MLP: 39.76% [C_fc: 19.88%, C_proj_2: 19.88%]\n",
      "Output layer: 27.10%\n",
      "\n",
      "==================================================\n",
      "\n",
      "Config: ModelConfig(name='GPT2_medium', vocab_size=50257, context_length=1024, num_layers=24, d_model=1024, num_heads=16, d_ff=4096)\n",
      "QKV projection            : 6,442,450,944\n",
      "QK^T                      : 2,147,483,648\n",
      "Attention score           : 2,147,483,648\n",
      "C_proj_1                  : 2,147,483,648\n",
      "[Attention mechanism]     : 10,737,418,240\n",
      "C_fc                      : 8,589,934,592\n",
      "C_proj_2                  : 8,589,934,592\n",
      "[Transformer block]       : 30,064,771,072\n",
      "[Total Transformer blocks]: 721,554,505,728\n",
      "Output layer              : 105,396,568,064\n",
      "Each token: 826,951,073,792\n",
      "Proportions:\n",
      "Attention: 37.39% [QKV projection: 18.70%, QK^T: 6.23%, Attention score: 6.23%, (Attention mechanism: 31.16%) C_proj_1: 6.23%]\n",
      "MLP: 49.86% [C_fc: 24.93%, C_proj_2: 24.93%]\n",
      "Output layer: 12.75%\n",
      "\n",
      "==================================================\n",
      "\n",
      "Config: ModelConfig(name='GPT2_large', vocab_size=50257, context_length=1024, num_layers=36, d_model=1280, num_heads=20, d_ff=5120)\n",
      "QKV projection            : 10,066,329,600\n",
      "QK^T                      : 2,684,354,560\n",
      "Attention score           : 2,684,354,560\n",
      "C_proj_1                  : 3,355,443,200\n",
      "[Attention mechanism]     : 15,435,038,720\n",
      "C_fc                      : 13,421,772,800\n",
      "C_proj_2                  : 13,421,772,800\n",
      "[Transformer block]       : 45,634,027,520\n",
      "[Total Transformer blocks]: 1,642,824,990,720\n",
      "Output layer              : 131,745,710,080\n",
      "Each token: 1,774,570,700,800\n",
      "Proportions:\n",
      "Attention: 38.12% [QKV projection: 20.42%, QK^T: 5.45%, Attention score: 5.45%, (Attention mechanism: 31.31%) C_proj_1: 6.81%]\n",
      "MLP: 54.46% [C_fc: 27.23%, C_proj_2: 27.23%]\n",
      "Output layer: 7.42%\n",
      "\n",
      "==================================================\n",
      "\n",
      "Config: ModelConfig(name='GPT2_XL_modified', vocab_size=50257, context_length=16384, num_layers=48, d_model=1600, num_heads=25, d_ff=6400)\n",
      "QKV projection            : 251,658,240,000\n",
      "QK^T                      : 858,993,459,200\n",
      "Attention score           : 858,993,459,200\n",
      "C_proj_1                  : 83,886,080,000\n",
      "[Attention mechanism]     : 1,969,645,158,400\n",
      "C_fc                      : 335,544,320,000\n",
      "C_proj_2                  : 335,544,320,000\n",
      "[Transformer block]       : 2,724,619,878,400\n",
      "[Total Transformer blocks]: 130,781,754,163,200\n",
      "Output layer              : 2,634,914,201,600\n",
      "Each token: 133,416,668,364,800\n",
      "Proportions:\n",
      "Attention: 73.88% [QKV projection: 9.05%, QK^T: 30.90%, Attention score: 30.90%, (Attention mechanism: 70.86%) C_proj_1: 3.02%]\n",
      "MLP: 24.14% [C_fc: 12.07%, C_proj_2: 12.07%]\n",
      "Output layer: 1.97%\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "ModelConfig = namedtuple(\n",
    "    \"ModelConfig\", [\"name\", \"vocab_size\", \"context_length\", \"num_layers\", \"d_model\", \"num_heads\", \"d_ff\"]\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_flops(vocab_size, context_length, num_layers, d_model, num_heads, d_ff):\n",
    "    flops = []\n",
    "    # (context_length, d_model) × (d_model, 3 × d_model)\n",
    "    qkv_projection = 2 * context_length * d_model * (3 * d_model)\n",
    "    flops.append((\"QKV projection\", qkv_projection))\n",
    "\n",
    "    # (context_length, d_k) × (d_k, context_length), per head, d_k = d_model / num_heads\n",
    "    qk_t = 2 * num_heads * context_length * (d_model // num_heads) * context_length\n",
    "    flops.append((\"QK^T\", qk_t))\n",
    "\n",
    "    # (context_length, context_length) × (context_length, d_k), per head\n",
    "    attention_score = 2 * num_heads * context_length * context_length * (d_model // num_heads)\n",
    "    flops.append((\"Attention score\", attention_score))\n",
    "\n",
    "    # (context_length, d_model) × (d_model, d_model)\n",
    "    c_proj_1 = 2 * context_length * d_model * d_model\n",
    "    flops.append((\"C_proj_1\", c_proj_1))\n",
    "\n",
    "    flops.append((\"[Attention mechanism]\", qkv_projection + qk_t + attention_score))\n",
    "\n",
    "    # (context_length, d_model) × (d_model, d_ff)\n",
    "    c_fc = 2 * context_length * d_model * d_ff\n",
    "    flops.append((\"C_fc\", c_fc))\n",
    "\n",
    "    # (context_length, d_ff) × (d_ff, d_model)\n",
    "    c_proj_2 = 2 * context_length * d_ff * d_model\n",
    "    flops.append((\"C_proj_2\", c_proj_2))\n",
    "\n",
    "    transformer_block = qkv_projection + qk_t + attention_score + c_proj_1 + c_fc + c_proj_2\n",
    "    flops.append((\"[Transformer block]\", transformer_block))\n",
    "\n",
    "    flops.append((\"[Total Transformer blocks]\", transformer_block * num_layers))\n",
    "\n",
    "    # (context_length, d_model) × (d_model, vocab_size)\n",
    "    output_layer = 2 * context_length * d_model * vocab_size\n",
    "    flops.append((\"Output layer\", output_layer))\n",
    "\n",
    "    maxlen = max(len(name) for name, _ in flops)\n",
    "    for name, count in flops:\n",
    "        print(f\"{name:<{maxlen}}: {count:,}\")\n",
    "\n",
    "    total_flops = transformer_block * num_layers + output_layer\n",
    "    print(f\"Each token: {total_flops:,}\")\n",
    "    print(\"Proportions:\")\n",
    "    print(\n",
    "        f\"Attention: {100 * (qkv_projection + qk_t + attention_score + c_proj_1) * num_layers / total_flops:.2f}% [QKV projection: {100 * qkv_projection * num_layers / total_flops:.2f}%, QK^T: {100 * qk_t * num_layers / total_flops:.2f}%, Attention score: {100 * attention_score * num_layers / total_flops:.2f}%, (Attention mechanism: {100 * (qkv_projection + qk_t + attention_score) * num_layers / total_flops:.2f}%) C_proj_1: {100 * c_proj_1 * num_layers / total_flops:.2f}%]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"MLP: {100 * (c_fc + c_proj_2) * num_layers / total_flops:.2f}% [C_fc: {100 * c_fc * num_layers / total_flops:.2f}%, C_proj_2: {100 * c_proj_2 * num_layers / total_flops:.2f}%]\"\n",
    "    )\n",
    "    print(f\"Output layer: {100 * output_layer / total_flops:.2f}%\")\n",
    "\n",
    "\n",
    "GPT2_XL = ModelConfig(\n",
    "    name=\"GPT2_XL\", vocab_size=50257, context_length=1024, num_layers=48, d_model=1600, num_heads=25, d_ff=6400\n",
    ")\n",
    "GPT2_small = ModelConfig(\n",
    "    name=\"GPT2_small\", vocab_size=50257, context_length=1024, num_layers=12, d_model=768, num_heads=12, d_ff=3072\n",
    ")\n",
    "GPT2_medium = ModelConfig(\n",
    "    name=\"GPT2_medium\", vocab_size=50257, context_length=1024, num_layers=24, d_model=1024, num_heads=16, d_ff=4096\n",
    ")\n",
    "GPT2_large = ModelConfig(\n",
    "    name=\"GPT2_large\", vocab_size=50257, context_length=1024, num_layers=36, d_model=1280, num_heads=20, d_ff=5120\n",
    ")\n",
    "\n",
    "GPT2_XL_modified = ModelConfig(\n",
    "    name=\"GPT2_XL_modified\",\n",
    "    vocab_size=50257,\n",
    "    context_length=16384,\n",
    "    num_layers=48,\n",
    "    d_model=1600,\n",
    "    num_heads=25,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "for config in [GPT2_XL, GPT2_small, GPT2_medium, GPT2_large, GPT2_XL_modified]:\n",
    "    print(f\"Config: {config}\")\n",
    "    calculate_flops(\n",
    "        config.vocab_size, config.context_length, config.num_layers, config.d_model, config.num_heads, config.d_ff\n",
    "    )\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9425a6",
   "metadata": {},
   "source": [
    "#### (c) Based on your analysis above, which parts of the model require the most FLOPs?\n",
    "The majority of FLOPs in GPT-2 XL are consumed by the MLP (feed-forward) layers and the attention mechanism, especially the QKV projections and the large matrix multiplications in the MLP. Among these, the two MLP linear layers (c_fc and c_proj) together account for the largest share of total FLOPs per forward pass.\n",
    "\n",
    "#### (d) Repeat your analysis with GPT-2 small (12 layers, 768 d_model, 12 heads), GPT-2 medium (24 layers, 1024 d_model, 16 heads), and GPT-2 large (36 layers, 1280 d_model, 20 heads). As the model size increases, which parts of the Transformer LM take up proportionally more or less of the total FLOPs?\n",
    "Deliverable: For each model, provide a breakdown of model components and its associated FLOPs (as a proportion of the total FLOPs required for a forward pass). In addition, provide a one-to-two sentence description of how varying the model size changes the proportional FLOPs of each component.\n",
    "\n",
    "**Assume context_length = 1024, vocab_size = 50257**\n",
    "\n",
    "| Model         | Layers | d_model | d_ff   | heads | d_k   |\n",
    "|---------------|--------|---------|--------|-------|-------|\n",
    "| GPT-2 small   | 12     | 768     | 3072   | 12    | 64    |\n",
    "| GPT-2 medium  | 24     | 1024    | 4096   | 16    | 64    |\n",
    "| GPT-2 large   | 36     | 1280    | 5120   | 20    | 64    |\n",
    "| GPT-2 XL      | 48     | 1600    | 6400   | 25    | 64    |\n",
    "\n",
    "\n",
    "GPT-2 small (12 × 768, 12 heads, d_ff=3072)  \n",
    "- Attention (QKV+QK^T+Attn×V+Output proj): 33.13% [QKV projection: 14.91%, QK^T: 6.63%, Attention score: 6.63%, (Attention mechanism: 28.16%) C_proj_1: 4.97%]\n",
    "- MLP (fc+proj): 39.76% [C_fc: 19.88%, C_proj_2: 19.88%]\n",
    "- Output layer: 27.10%\n",
    "---\n",
    "\n",
    "GPT-2 medium (24 × 1024, 16 heads, d_ff=4096)  \n",
    "- Attention: 37.39% [QKV projection: 18.70%, QK^T: 6.23%, Attention score: 6.23%, (Attention mechanism: 31.16%) C_proj_1: 6.23%]\n",
    "- MLP: 49.86% [C_fc: 24.93%, C_proj_2: 24.93%]\n",
    "- Output layer: 12.75%\n",
    "---\n",
    "\n",
    "GPT-2 large (36 × 1280, 20 heads, d_ff=5120)  \n",
    "- Attention: 38.12% [QKV projection: 20.42%, QK^T: 5.45%, Attention score: 5.45%, (Attention mechanism: 31.31%) C_proj_1: 6.81%]\n",
    "- MLP: 54.46% [C_fc: 27.23%, C_proj_2: 27.23%]\n",
    "- Output layer: 7.42%\n",
    "---\n",
    "\n",
    "GPT-2 XL (48 × 1600, 25 heads, d_ff=6400)  \n",
    "- Attention: 37.89% [QKV projection: 21.53%, QK^T: 4.59%, Attention score: 4.59%, (Attention mechanism: 30.72%) C_proj_1: 7.18%]\n",
    "- MLP: 57.41% [C_fc: 28.71%, C_proj_2: 28.71%]\n",
    "- Output layer: 4.70%\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "As model size increases, the proportion of FLOPs spent in the MLP (feed-forward) layers grows, while the relative share of output layer decreases. This is because the MLP layers scale with the product of d_model and d_ff, which grows faster than the attention components as models get wider and deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdbed4e",
   "metadata": {},
   "source": [
    "#### (e) Take GPT-2 XL and increase the context length to 16,384. How does the total FLOPs for one forward pass change? How do the relative contribution of FLOPs of the model components change?\n",
    "Total FLOPs per forward pass to increase dramatically(3.507 TFLOPS vs 133.417 TFLOPS), especially for the attention components whose complexity scales quadratically with context length. As a result, the attention mechanism dominates the total FLOPs, while the relative proportion of FLOPs spent in the MLP and output layers decreases.  \n",
    "\n",
    "Config: ModelConfig(name='GPT2_XL', vocab_size=50257, context_length=1024, num_layers=48, d_model=1600, num_heads=25, d_ff=6400)\n",
    "- Each token: 3,506,703,564,800  \n",
    "- Attention: 37.89% [QKV projection: 21.53%, QK^T: 4.59%, Attention score: 4.59%, (Attention mechanism: 30.72%) C_proj_1: 7.18%]\n",
    "- MLP: 57.41% [C_fc: 28.71%, C_proj_2: 28.71%]\n",
    "- Output layer: 4.70%\n",
    "\n",
    "---\n",
    "\n",
    "Config: ModelConfig(name='GPT2_XL_modified', vocab_size=50257, context_length=16384, num_layers=48, d_model=1600, num_heads=25, d_ff=6400)  \n",
    "- Each token: 133,416,668,364,800  \n",
    "- Attention: 73.88% [QKV projection: 9.05%, QK^T: 30.90%, Attention score: 30.90%, (Attention mechanism: 70.86%) C_proj_1: 3.02%]\n",
    "- MLP: 24.14% [C_fc: 12.07%, C_proj_2: 12.07%]\n",
    "- Output layer: 1.97%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46adb23d",
   "metadata": {},
   "source": [
    "### Problem (learning_rate_tuning): Tuning the learning rate\n",
    "As we will see, one of the hyperparameters that affects training the most is the learning rate. Let's see that in practice in our toy example. Run the SGD example above with three other values for the learning rate: 1e1, 1e2, and 1e3, for just 10 training iterations. What happens with the loss for each of these learning rates? Does it decay faster, slower, or does it diverge (i.e., increase over the course of training)?\n",
    "\n",
    "Testing using 1000 iterations:\n",
    "- For learning rate = 1e1, the loss decays and stabilizes around a low value.\n",
    "- For learning rate = 1e2, the loss also decays and faster than 1e1, finally reaching 0 loss (which is surprising).\n",
    "- For learning rate = 1e3, the loss diverges, increasing rapidly over the course of training, reaching inf in about 25 iterations. But loss drops in about 200 iterations, eventually reaching 0 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb41fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 10, losses: [24.16925811767578, 15.468326568603516, 11.402588844299316, 8.921309471130371, 7.226260185241699, 5.991397857666016, 5.05294942855835, 4.3178887367248535, 3.72883677482605, 3.2482311725616455, 2.8503518104553223, 2.516951322555542, 2.234708547592163, 1.9936660528182983, 1.786230206489563, 1.6065125465393066, 1.4498776197433472, 1.312630295753479, 1.1917911767959595, 1.0849339962005615, 0.9900643825531006, 0.9055303335189819, 0.8299529552459717, 0.7621734738349915, 0.7012125253677368, 0.6462374925613403, 0.5965366363525391, 0.5514990091323853, 0.5105974674224854, 0.47337549924850464, 0.4394362270832062, 0.408433198928833, 0.38006314635276794, 0.3540596067905426, 0.33018791675567627, 0.30824047327041626, 0.2880336046218872, 0.26940399408340454, 0.25220635533332825, 0.23631088435649872, 0.22160156071186066, 0.20797444880008698, 0.19533604383468628, 0.18360237777233124, 0.17269763350486755, 0.16255345940589905, 0.1531079262495041, 0.1443050056695938, 0.13609379529953003, 0.12842810153961182, 0.12126585096120834, 0.11456871777772903, 0.10830172896385193, 0.10243289917707443, 0.0969330370426178, 0.09177535772323608, 0.08693531155586243, 0.08239037543535233, 0.07811984419822693, 0.0741046592593193, 0.07032732665538788, 0.06677164882421494, 0.06342272460460663, 0.060266781598329544, 0.05729111656546593, 0.054483938962221146, 0.0518343560397625, 0.04933226481080055, 0.04696831852197647, 0.044733814895153046, 0.0426206961274147, 0.040621448308229446, 0.03872910514473915, 0.036937166005373, 0.03523958846926689, 0.03363073617219925, 0.032105349004268646, 0.030658531934022903, 0.029285697266459465, 0.027982566505670547, 0.026745140552520752, 0.025569671764969826, 0.02445266768336296, 0.023390840739011765, 0.022381117567420006, 0.021420622244477272, 0.02050664648413658, 0.01963665708899498, 0.01880827359855175, 0.018019257113337517, 0.017267508432269096, 0.016551045700907707, 0.015868015587329865, 0.015216667205095291, 0.014595350250601768, 0.014002515934407711, 0.013436699286103249, 0.012896525673568249, 0.012380690313875675, 0.011887970380485058, 0.011417206376791, 0.010967306792736053, 0.010537237860262394, 0.010126023553311825, 0.009732742793858051, 0.009356523863971233, 0.008996539749205112, 0.008652012795209885, 0.008322199806571007, 0.008006405085325241, 0.007703964598476887, 0.007414249237626791, 0.007136664818972349, 0.006870646961033344, 0.006615659687668085, 0.006371195428073406, 0.006136772222816944, 0.005911931861191988, 0.005696241278201342, 0.005489286966621876, 0.005290675908327103, 0.0051000360399484634, 0.004917014390230179, 0.004741272423416376, 0.004572490695863962, 0.004410363268107176, 0.004254600964486599, 0.004104927182197571, 0.003961078356951475, 0.003822805592790246, 0.00368986907415092, 0.0035620415583252907, 0.0034391062799841166, 0.003320857649669051, 0.0032070972956717014, 0.003097638487815857, 0.0029923012480139732, 0.0028909153770655394, 0.0027933167293667793, 0.002699350006878376, 0.0026088666636496782, 0.00252172420732677, 0.002437787363305688, 0.00235692597925663, 0.0022790164221078157, 0.002203940413892269, 0.002131584333255887, 0.0020618403796106577, 0.001994604943320155, 0.001929778722114861, 0.0018672668375074863, 0.001806979184038937, 0.001748828450217843, 0.0016927319811657071, 0.001638609915971756, 0.0015863862354308367, 0.0015359879471361637, 0.001487345201894641, 0.0014403910608962178, 0.001395061262883246, 0.0013512942241504788, 0.0013090312713757157, 0.001268215593881905, 0.0012287930585443974, 0.0011907118605449796, 0.0011539217084646225, 0.0011183751048520207, 0.0010840259492397308, 0.001050830353051424, 0.0010187457082793117, 0.0009877318516373634, 0.00095774931833148, 0.0009287609718739986, 0.0009007307817228138, 0.000873624172527343, 0.0008474078495055437, 0.0008220501476898789, 0.0007975202752277255, 0.0007737890118733048, 0.0007508274866268039, 0.0007286090403795242, 0.0007071071886457503, 0.0006862969603389502, 0.0006661539664492011, 0.0006466549821197987, 0.0006277775974012911, 0.0006095001590438187, 0.0005918021779507399, 0.0005746633978560567, 0.0005580648430623114, 0.0005419877707026899, 0.0005264144856482744, 0.0005113279330544174, 0.0004967114073224366, 0.000482549105072394, 0.000468825688585639, 0.00045552634401246905, 0.0004426368686836213, 0.00043014358379878104, 0.0004180331889074296, 0.0004062930529471487, 0.00039491080678999424, 0.00038387469248846173, 0.00037317321402952075, 0.0003627954865805805, 0.0003527307417243719, 0.0003429688804317266, 0.0003334999782964587, 0.0003243144601583481, 0.0003154032165184617, 0.0003067573416046798, 0.0002983683079946786, 0.00029022779199294746, 0.00028232784825377166, 0.00027466079336591065, 0.00026721920585259795, 0.00025999589706771076, 0.0002529839694034308, 0.00024617669987492263, 0.00023956761287990957, 0.00023315053840633482, 0.0002269193937536329, 0.00022086832905188203, 0.0002149917563656345, 0.0002092842769343406, 0.00020374056475702673, 0.0001983555848710239, 0.0001931244187289849, 0.00018804229330271482, 0.00018310466839466244, 0.00017830707656685263, 0.0001736452686600387, 0.0001691150537226349, 0.00016471244452986866, 0.00016043357027228922, 0.0001562746474519372, 0.0001522320817457512, 0.0001483023661421612, 0.00014448209549300373, 0.00014076796651352197, 0.00013715682143811136, 0.00013364554615691304, 0.0001302312157349661, 0.00012691091978922486, 0.00012368187890388072, 0.00012054137187078595, 0.00011748678662115708, 0.00011451561294961721, 0.00011162539158249274, 0.00010881373600568622, 0.00010607834701659158, 0.00010341701272409409, 0.00010082756489282474, 9.830791532294825e-05, 9.585604129824787e-05, 9.346997103421018e-05, 9.11478346097283e-05, 8.888775482773781e-05, 8.668797090649605e-05, 8.454675116809085e-05, 8.246241486631334e-05, 8.043333946261555e-05, 7.845793152227998e-05, 7.653470674995333e-05, 7.466214447049424e-05, 7.283883314812556e-05, 7.106337579898536e-05, 6.93344118189998e-05, 6.765065336367115e-05, 6.601081986445934e-05, 6.441368896048516e-05, 6.285804556682706e-05, 6.134276191005483e-05, 5.986669930280186e-05, 5.842876635142602e-05, 5.702791168005206e-05, 5.566310210269876e-05, 5.4333359003067017e-05, 5.30376928509213e-05, 5.177518323762342e-05, 5.05449170304928e-05, 4.9346006562700495e-05, 4.8177604185184464e-05, 4.703886224888265e-05, 4.592899131239392e-05, 4.484720193431713e-05, 4.379272286314517e-05, 4.2764830141095445e-05, 4.1762796172406524e-05, 4.0785918827168643e-05, 3.9833532355260104e-05, 3.890497100655921e-05, 3.799960541073233e-05, 3.711680983542465e-05, 3.625597673817538e-05, 3.541653495631181e-05, 3.45979024132248e-05, 3.37995334120933e-05, 3.3020885894075036e-05, 3.226144690415822e-05, 3.152070348733105e-05, 3.0798160878475755e-05, 3.009334614034742e-05, 2.9405793611658737e-05, 2.873504490708001e-05, 2.8080670745112002e-05, 2.7442234568297863e-05, 2.6819327104021795e-05, 2.6211539079668e-05, 2.5618483050493523e-05, 2.503977702872362e-05, 2.447504630254116e-05, 2.3923932531033643e-05, 2.338608646823559e-05, 2.2861166144139133e-05, 2.2348833226715215e-05, 2.1848776668775827e-05, 2.1360674509196542e-05, 2.0884221157757565e-05, 2.0419123757164925e-05, 1.9965093088103458e-05, 1.9521841750247404e-05, 1.908910235215444e-05, 1.8666609321371652e-05, 1.8254098904435523e-05, 1.785132371878717e-05, 1.7458036381867714e-05, 1.7073998606065288e-05, 1.6698979379725643e-05, 1.6332751329173334e-05, 1.597509799466934e-05, 1.5625799278495833e-05, 1.5284651453839615e-05, 1.4951448974898085e-05, 1.4625998119299766e-05, 1.4308101526694372e-05, 1.399757729814155e-05, 1.3694238987227436e-05, 1.3397913789958693e-05, 1.3108424354868475e-05, 1.2825604244426358e-05, 1.2549292478070129e-05, 1.2279324437258765e-05, 1.2015548236377072e-05, 1.175781108031515e-05, 1.1505966540426016e-05, 1.1259870007052086e-05, 1.1019384146493394e-05, 1.078437071555527e-05, 1.0554696928011253e-05, 1.0330234545108397e-05, 1.0110857147083152e-05, 9.89644195215078e-06, 9.68686981650535e-06, 9.48202250583563e-06, 9.281789971282706e-06, 9.086059435503557e-06, 8.894724487618078e-06, 8.707676897756755e-06, 8.524817530997097e-06, 8.346045433427207e-06, 8.171264198608696e-06, 8.000379239092581e-06, 7.833295967429876e-06, 7.669925253139809e-06, 7.510181603720412e-06, 7.353976798185613e-06, 7.201229436759604e-06, 7.051855391182471e-06, 6.905777809151914e-06, 6.762918928870931e-06, 6.623202807531925e-06, 6.486555776064051e-06, 6.352906893880572e-06, 6.222186129889451e-06, 6.094324817240704e-06, 5.969256562821101e-06, 5.8469177020015195e-06, 5.7272427511634305e-06, 5.610172138403868e-06, 5.495644472830463e-06, 5.383601092034951e-06, 5.2739846978511196e-06, 5.166740720596863e-06, 5.061812771600671e-06, 4.959149464411894e-06, 4.8586975935904775e-06, 4.7604071369278245e-06, 4.66422943645739e-06, 4.5701144699705765e-06, 4.4780158532375935e-06, 4.387889475765405e-06, 4.299688953324221e-06, 4.213369265926303e-06, 4.128890395804774e-06, 4.046210051456001e-06, 3.965286850871053e-06, 3.8860821405251045e-06, 3.8085554479039274e-06, 3.7326703932194505e-06, 3.6583908240572782e-06, 3.5856792237609625e-06, 3.5145012589055113e-06, 3.4448219139449066e-06, 3.3766086744435597e-06, 3.3098278890975052e-06, 3.2444484077132074e-06, 3.1804386253497796e-06, 3.1177685286820633e-06, 3.0564081043849e-06, 2.9963280212541576e-06, 2.9375003123277565e-06, 2.8798976927646436e-06, 2.8234924229764147e-06, 2.7682585823640693e-06, 2.7141695682075806e-06, 2.6612012788973516e-06, 2.609328930702759e-06, 2.5585281946405303e-06, 2.508775196474744e-06, 2.460047880958882e-06, 2.412323283351725e-06, 2.3655800305277808e-06, 2.3197965219878824e-06, 2.2749516119802138e-06, 2.2310250642476603e-06, 2.1879970972804585e-06, 2.1458483843161957e-06, 2.1045595985924592e-06, 2.0641120954678627e-06, 2.0244876850483706e-06, 1.985669314308325e-06, 1.9476390207273653e-06, 1.9103797512798337e-06, 1.8738754761216114e-06, 1.838109596974391e-06, 1.8030665387414047e-06, 1.7687306126390467e-06, 1.735086812004738e-06, 1.7021203575495747e-06, 1.6698170384188415e-06, 1.6381623026973102e-06, 1.607142962711805e-06, 1.5767448076076107e-06, 1.546955331832578e-06, 1.5177614614003687e-06, 1.489150122324645e-06, 1.461109832234797e-06, 1.4336277445181622e-06, 1.4066927178646438e-06, 1.3802929288431187e-06, 1.3544172361434903e-06, 1.3290544984556618e-06, 1.3041940292168874e-06, 1.2798253692380968e-06, 1.2559382867038948e-06, 1.232522436112049e-06, 1.2095682677681907e-06, 1.187066345664789e-06, 1.165006779046962e-06, 1.14338058665453e-06, 1.1221786735404748e-06, 1.1013925131919677e-06, 1.0810131243488286e-06, 1.0610325489324168e-06, 1.041442033056228e-06, 1.0222339597021346e-06, 1.0034002571046585e-06, 9.849329671851592e-07, 9.668251550465357e-07, 9.490688057667285e-07, 9.316570412920555e-07, 9.145828698819969e-07, 8.97839186109195e-07, 8.814193392936431e-07, 8.653165650684969e-07, 8.495247243445192e-07, 8.340370527548657e-07, 8.188476954273938e-07, 8.039504564294475e-07, 7.893393103586277e-07, 7.75008686559886e-07, 7.609527301610797e-07, 7.471659841939982e-07, 7.336428211601742e-07, 7.203781819953292e-07, 7.073667234180903e-07, 6.946032158339221e-07, 6.820828275522217e-07, 6.698007268823858e-07, 6.577519684469735e-07, 6.459320047724759e-07, 6.343361746985465e-07, 6.229599875950953e-07, 6.117991233622888e-07, 6.008492050568748e-07, 5.901060831092764e-07, 5.795656079499167e-07, 5.692238005394756e-07, 5.590766249952139e-07, 5.491203296514868e-07, 5.393510491558118e-07, 5.29765031842544e-07, 5.20358753419714e-07, 5.111285759085149e-07, 5.02071031860396e-07, 4.931828812004824e-07, 4.844605427933857e-07, 4.759009470944875e-07, 4.6750076876378444e-07, 4.592569098349486e-07, 4.5116632918507094e-07, 4.432259856912424e-07, 4.3543298033910105e-07, 4.2778444253599446e-07, 4.202774732675607e-07, 4.1290940089311334e-07, 4.056774685068376e-07, 3.9857900446804706e-07, 3.916114792446024e-07, 3.8477224961752654e-07, 3.7805889974151796e-07, 3.7146898534956563e-07, 3.650001190180774e-07, 3.586499133234611e-07, 3.5241617979409057e-07, 3.4629655942808313e-07, 3.402888353321032e-07, 3.3439093272136233e-07, 3.286007199676533e-07, 3.2291617912960646e-07, 3.173350933138863e-07, 3.1185570037450816e-07, 3.0647592552668357e-07, 3.011938645158807e-07, 2.96007698352696e-07, 2.909155227825977e-07, 2.859156325030199e-07, 2.8100620852455904e-07, 2.7618554554464936e-07, 2.7145196668243443e-07, 2.6680379505705787e-07, 2.622393822093727e-07, 2.577571933670697e-07, 2.533556653361302e-07, 2.4903320650082605e-07, 2.447883957756858e-07, 2.4061972681010957e-07, 2.365257927294806e-07, 2.3250515823747264e-07, 2.2855645909203304e-07, 2.2467833105110913e-07, 2.2086942408350296e-07, 2.171284734231449e-07, 2.1345417167140113e-07, 2.0984531090562086e-07, 2.0630062635973445e-07, 2.0281895274365525e-07, 1.9939909634558717e-07, 1.960399060862983e-07, 1.9274023088655667e-07, 1.8949896229969454e-07, 1.8631504872246296e-07, 1.8318736749733944e-07, 1.8011495228620333e-07, 1.7709669464238686e-07, 1.741316424386241e-07, 1.712187867042303e-07, 1.6835718952279422e-07, 1.655458561344858e-07, 1.627838770446033e-07, 1.600703569692996e-07, 1.5740441483558243e-07, 1.5478512693789526e-07, 1.5221168325751933e-07, 1.496832027214623e-07, 1.471988753110054e-07, 1.4475789100742986e-07, 1.4235945400287164e-07, 1.400027827003214e-07, 1.3768712392447924e-07, 1.3541171028919052e-07, 1.3317581704086479e-07, 1.3097873363676626e-07, 1.2881972111244977e-07, 1.2669811155774369e-07, 1.2461322285162169e-07, 1.2256437287305744e-07, 1.2055092213358876e-07, 1.185722240393261e-07, 1.16627653312662e-07, 1.1471657046513428e-07, 1.1283839995712697e-07, 1.1099251651103259e-07, 1.0917836590351726e-07, 1.0739534417325558e-07, 1.0564291841319573e-07, 1.0392052729457646e-07, 1.0222763791034595e-07, 1.005636960371703e-07, 9.892820429513449e-08, 9.732065109346877e-08, 9.574051773597603e-08, 9.418732815902331e-08, 9.266059208812294e-08, 9.115983345964196e-08, 8.968461884251155e-08, 8.823444375138934e-08, 8.680889607148856e-08, 8.54075281608857e-08, 8.40299136939393e-08, 8.26756121341532e-08, 8.134423268302271e-08, 8.00353632257611e-08, 7.874857743672692e-08, 7.748352714997964e-08, 7.623980025073251e-08, 7.501702725676296e-08, 7.381482447499366e-08, 7.263284373948409e-08, 7.147072977886637e-08, 7.032812732177263e-08, 6.920468820226233e-08, 6.810007846524968e-08, 6.701397126107622e-08, 6.59460397400835e-08, 6.489595705261308e-08, 6.386342477071594e-08, 6.284811604473362e-08, 6.184974665757181e-08, 6.086801818128151e-08, 5.990262508248634e-08, 5.895330090766038e-08, 5.801975078156829e-08, 5.710170114525681e-08, 5.6198896203341064e-08, 5.5311055291440425e-08, 5.443792261417002e-08, 5.357925303428601e-08, 5.273478720368985e-08, 5.1904272879710334e-08, 5.108746847781731e-08, 5.0284146624335335e-08, 4.949407284016161e-08, 4.8717012646193325e-08, 4.7952738668755046e-08, 4.720103774502604e-08, 4.6461693159471906e-08, 4.5734481091130874e-08, 4.501920614075061e-08, 4.431565514551039e-08, 4.3623639811585235e-08, 4.294293987072706e-08, 4.227337768725192e-08, 4.1614747203766456e-08, 4.096688499544143e-08, 4.032958855759716e-08, 3.9702680254549705e-08, 3.90859860033288e-08, 3.8479328168250504e-08, 3.788253621905824e-08, 3.72954431782091e-08, 3.671788917358754e-08, 3.6149700122223294e-08, 3.559072681014186e-08, 3.504080936522769e-08, 3.449979146807891e-08, 3.396752390472102e-08, 3.3443861013893184e-08, 3.292865713433457e-08, 3.2421763052070673e-08, 3.1923047316695374e-08, 3.1432360714234164e-08, 3.0949578899708285e-08, 3.0474563317284264e-08, 3.0007182516555986e-08, 2.9547306823474173e-08, 2.9094815445773747e-08, 2.8649578709405432e-08, 2.8211479374817827e-08, 2.7780393097032174e-08, 2.7356206189210752e-08, 2.693880141180216e-08, 2.6528066854325516e-08, 2.612388705358626e-08, 2.5726155428174025e-08, 2.533476362032161e-08, 2.494960860133233e-08, 2.45705784607253e-08, 2.4197579051588036e-08, 2.383050734522385e-08, 2.3469265642006576e-08, 2.3113752689596367e-08, 2.2763881446508094e-08, 2.241954888404507e-08, 2.2080673289792685e-08, 2.174715518776793e-08, 2.1418911089199355e-08, 2.109585039988815e-08, 2.0777891407419702e-08, 2.046494707030888e-08, 2.0156933899784235e-08, 1.985377195978799e-08, 1.9555381314262377e-08, 1.9261685579863297e-08, 1.897260304417614e-08, 1.868806087657049e-08, 1.8407982693702252e-08, 1.813229566494101e-08, 1.7860930512370032e-08, 1.759381262900206e-08, 1.7330874513277195e-08, 1.7072046887278702e-08, 1.6817264025803524e-08, 1.6566460203648603e-08, 1.63195661428972e-08, 1.607652144741678e-08, 1.583726394471796e-08, 1.5601729685954524e-08, 1.536985827499393e-08, 1.514159109206048e-08, 1.4916869517378473e-08, 1.4695636707529047e-08, 1.4477834930914923e-08, 1.426341089683092e-08, 1.4052303320966075e-08, 1.3844466018042567e-08, 1.3639844809176793e-08, 1.3438381962771473e-08, 1.3240033069905621e-08, 1.3044743951695636e-08, 1.2852467534685275e-08, 1.2663153192704613e-08, 1.2476753852297406e-08, 1.2293224216364251e-08, 1.2112516323270484e-08, 1.1934583987738279e-08, 1.175938635356033e-08, 1.1586876347280395e-08, 1.1417012224512746e-08, 1.1249749576336399e-08, 1.1085048434722466e-08, 1.0922867943463643e-08, 1.0763167246352623e-08, 1.0605906375360519e-08, 1.0451048915172123e-08, 1.029855400958013e-08, 1.0148382578734072e-08, 1.0000501760032421e-08, 9.85487069726787e-09, 9.71145741601731e-09, 9.570223724608695e-09, 9.431136760440495e-09, 9.294161884554342e-09, 9.159265346170287e-09, 9.02641517086522e-09, 8.895577607859195e-09, 8.76672157090752e-09, 8.639815085587088e-09, 8.514828842010047e-09, 8.391729977574869e-09, 8.270490958750543e-09, 8.151083363827638e-09, 8.033474330204626e-09, 7.917638100707336e-09, 7.803547141804756e-09, 7.691173031787457e-09, 7.580489125302847e-09, 7.471467888819916e-09, 7.364086673788961e-09, 7.258315726232922e-09, 7.154131509423678e-09, 7.051509598454686e-09, 6.950424236151775e-09, 6.850851885786824e-09, 6.752769454720919e-09, 6.65615340622594e-09, 6.560981091752183e-09, 6.467229418660736e-09, 6.374876182491107e-09, 6.283900511050433e-09, 6.1942801998782215e-09, 6.105994376781609e-09, 6.019021725478524e-09, 5.933342706043732e-09, 5.848937334462789e-09, 5.765784738542834e-09, 5.683866710626262e-09, 5.60316326669863e-09, 5.523656199102334e-09, 5.44532685609056e-09, 5.368157030005705e-09, 5.292128513190164e-09, 5.217223542075544e-09, 5.14342612945029e-09, 5.070717179478379e-09, 4.9990807049482555e-09, 4.928501162737575e-09, 4.858961677456364e-09, 4.7904462618930665e-09, 4.722938484746919e-09, 4.6564236910739965e-09, 4.590885893662744e-09, 4.526311325747656e-09, 4.4626840001171786e-09, 4.399990594095016e-09, 4.338215120469613e-09, 4.277345588832304e-09, 4.217366456060745e-09, 4.158265287657059e-09, 4.100027872766532e-09, 4.042641332802077e-09, 3.9860932332658194e-09, 3.930369807392253e-09, 3.875459064772713e-09, 3.821349014998532e-09, 3.768026335393415e-09, 3.715480367816326e-09, 3.663698677769389e-09, 3.612669940977753e-09, 3.5623819449881466e-09, 3.5128244757487437e-09, 3.463985542850878e-09, 3.4158551542873283e-09, 3.368422207827848e-09, 3.321676267376006e-09, 3.2756064527461604e-09, 3.2302034380649047e-09, 3.1854565651912026e-09, 3.141356064162437e-09, 3.097892165015992e-09, 3.0550555418784597e-09, 3.012836424787224e-09, 2.971225709913483e-09, 2.93021407138383e-09, 2.8897926274140673e-09, 2.8499527182646034e-09, 2.810685462151241e-09, 2.7719824213789934e-09, 2.733834714163663e-09, 2.696234346899473e-09, 2.659172881891436e-09, 2.622642769622985e-09, 2.586635794443737e-09, 2.55114418479252e-09, 2.5161599470635565e-09, 2.4816759758294893e-09, 2.4476847215737507e-09, 2.4141786347797733e-09, 2.3811508320648045e-09, 2.3485937639122767e-09, 2.3165005469394373e-09, 2.2848645198081385e-09, 2.2536787991356277e-09, 2.222936723583757e-09, 2.192631409769774e-09, 2.1627570845339505e-09, 2.133306420404324e-09, 2.104274088310376e-09, 2.0756529828247494e-09, 2.0474379969215306e-09, 2.019622469262572e-09, 1.9922008487327503e-09, 1.9651671401277326e-09, 1.938515570287791e-09, 1.9122403660531972e-09, 1.886336642442643e-09, 1.860798293229493e-09, 1.8356203224101364e-09, 1.81079717886945e-09, 1.7863236445592179e-09, 1.762194834498132e-09, 1.738405419615674e-09, 1.7149507369751404e-09, 1.6918254575060132e-09, 1.6690252513384962e-09, 1.6465449004243737e-09, 1.6243799638715473e-09, 1.6025260007879183e-09, 1.580978237214481e-09, 1.5597322322591367e-09, 1.5387834340074846e-09, 1.5181277346343336e-09, 1.4977610263144925e-09, 1.4776786461112579e-09, 1.457876819266346e-09, 1.4383511048876585e-09, 1.4190980612838189e-09, 1.4001132475627287e-09, 1.3813928889661042e-09, 1.3629333217579642e-09, 1.34473054913542e-09, 1.3267807963401879e-09, 1.3090806216808915e-09, 1.291626250399247e-09, 1.2744143518261808e-09, 1.2574412622257114e-09, 1.2407032068395552e-09, 1.224197521132453e-09, 1.2079202083015161e-09, 1.1918682707445782e-09, 1.1760384888148678e-09, 1.1604276428656135e-09, 1.145032291205439e-09, 1.1298496582767825e-09, 1.1148766354551753e-09, 1.1001101141161485e-09, 1.0855473187021403e-09, 1.071184918544077e-09, 1.0570204711513043e-09, 1.0430509789216558e-09, 1.0292737773198724e-09, 1.0156858687437875e-09, 1.0022845886581422e-09, 9.890672725276772e-10, 9.76031588884041e-10, 9.631746511473693e-10, 9.504939058047057e-10, 9.37986910365396e-10, 9.256511668276346e-10, 9.134843992342212e-10, 9.014838875387454e-10, 8.89647411295158e-10, 8.779727500574097e-10, 8.664575723571488e-10, 8.550993801925699e-10, 8.438962861845312e-10, 8.328457368200759e-10, 8.219458447200623e-10, 8.11194389438441e-10, 8.005891505291629e-10, 7.901283516353885e-10, 7.798095502664637e-10, 7.696310255767003e-10, 7.595907791646539e-10, 7.496868126288803e-10, 7.39917127567935e-10, 7.302798921138276e-10, 7.207732743985673e-10, 7.113953315318611e-10, 7.021443426680207e-10, 6.930184759390556e-10, 6.840160660104289e-10, 6.751351699918473e-10, 6.663742890822277e-10, 6.577315914135795e-10, 6.492055781848194e-10, 6.407944175279567e-10, 6.324966661530596e-10, 6.243106032144397e-10, 6.162348409333163e-10, 6.082675474416988e-10, 6.0040750149426e-10, 5.926530377564632e-10, 5.850028239606786e-10, 5.774551947723694e-10, 5.70008817923906e-10, 5.626623611476589e-10, 5.554143256425448e-10, 5.482633791409341e-10, 5.412081893751974e-10, 5.342473685665539e-10, 5.273796954696763e-10, 5.206037823057841e-10, 5.139183523183988e-10, 5.073221842621933e-10, 5.008140568918407e-10, 4.943927489620137e-10, 4.880569837162341e-10, 4.818057064426284e-10, 4.756375848735672e-10, 4.695516753194795e-10, 4.635466177571601e-10, 4.576214129858869e-10, 4.517749785382108e-10, 4.460061209243804e-10, 4.403139242104004e-10, 4.3469719490651926e-10, 4.2915490605643924e-10, 4.236860862150138e-10, 4.182897639370964e-10, 4.1296480124408674e-10, 4.077103377131408e-10, 4.025253463879608e-10, 3.9740888357897575e-10, 3.9236000559661477e-10, 3.8737785201803376e-10, 3.8246142364251057e-10, 3.7760977678047425e-10, 3.7282213427580757e-10, 3.680975801945152e-10, 3.634352541137531e-10, 3.588342400995259e-10, 3.5429381650686764e-10, 3.4981303964620736e-10, 3.4539118787257905e-10, 3.4102740076313864e-10, 3.367209011617689e-10, 3.3247091191235256e-10, 3.282766281031968e-10]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 100, losses: [24.16925811767578, 24.16925811767578, 4.146788597106934, 0.09924199432134628, 1.336870115417992e-16, 1.490024632424686e-18, 5.017437842615996e-20, 2.9889220257678576e-21, 2.5640905147008167e-22, 2.848989460778685e-23, 3.848670524134663e-24, 6.065157562829815e-25, 1.0834361118534776e-25, 2.1483662247978107e-26, 4.654850956811464e-27, 1.0886350788492933e-27, 2.7215876971232333e-28, 7.216332510613265e-29, 2.0163385126547413e-29, 5.9051175004518164e-30, 1.804443637664781e-30, 5.730997164637395e-31, 1.8855854501243589e-31, 6.408263289489364e-32, 2.2439818345844898e-32, 8.07833586816059e-33, 2.983988368800546e-33, 1.128985549490019e-33, 4.36836338215365e-34, 1.7261576641186132e-34, 6.95704794699966e-35, 2.8566412571808355e-35, 1.193771192980318e-35, 5.072351752956363e-36, 2.189494187988713e-36, 9.593537708959453e-37, 4.263794736611094e-37, 1.9208916392095504e-37, 8.766516466583427e-38, 4.05058297136868e-38, 1.893827268955243e-38, 8.95526548427443e-39, 4.280837889053598e-39, 2.067768625643879e-39, 1.0088368034213656e-39, 4.969564873881531e-40, 2.4708114912514472e-40, 1.2394765176645872e-40, 6.271511277085719e-41, 3.1997249134392873e-41, 1.6456849165030652e-41, 8.529703752345162e-42, 4.4547278180885935e-42, 2.342971032351094e-42, 1.241550439391788e-42, 6.6141287516131366e-43, 3.5592980993850354e-43, 1.9197788961249994e-43, 1.0509738482436128e-43, 5.74532370373175e-44, 3.0828566215145976e-44, 1.6815581571897805e-44, 9.80908925027372e-45, 5.605193857299268e-45, 2.802596928649634e-45, 1.401298464324817e-45, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1000, losses: [24.16925811767578, 8725.1015625, 1506962.5, 167633488.0, 13578311680.0, 856946966528.0, 43992855085056.0, 1892760814616576.0, 6.976313231409152e+16, 2.240171378709889e+18, 6.351085266071349e+19, 1.6070264919090319e+21, 3.661823489359366e+22, 7.570902614668709e+23, 1.4294603343646416e+25, 2.4784995531227155e+26, 3.965599284996345e+27, 5.880184963460066e+28, 8.111203917475619e+29, 1.0443988735829056e+31, 1.2590986589802755e+32, 1.4251617580367637e+33, 1.5183371720036368e+34, 1.5260393873400625e+35, 1.4499969100123343e+36, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 7.179050783881025e+35, 1.4848447042407708e+35, 3.019551404971886e+34, 6.037362390042541e+33, 1.1868367912661454e+33, 2.293868462588467e+32, 4.358877305975131e+31, 8.143364819939099e+30, 1.4957201984961724e+30, 2.7008936634824452e+29, 4.794780667227913e+28, 8.368111916987789e+27, 1.4357409342129056e+27, 2.421624736262095e+26, 4.015243752943322e+25, 6.544594085006657e+24, 1.0486022125086639e+24, 1.651531232311693e+23, 2.5568259142872753e+22, 3.890849150744698e+21, 5.8197734518760355e+20, 8.556099540580775e+19, 1.2363531066294665e+19, 1.7558815866512998e+18, 2.450879965990748e+17, 3.362101687798989e+16, 4532620184518656.0, 600516488855552.0, 78185190391808.0, 10003092078592.0, 1257593765888.0, 155356184576.0, 18857494528.0, 2249011968.0, 263533808.0, 30339014.0, 3431397.5, 381266.1875, 41615.62109375, 4462.07861328125, 469.9515686035156, 48.616729736328125, 4.93987512588501, 0.49297425150871277, 0.048316020518541336, 0.004650467541068792, 0.0004395618161652237, 4.0798102418193594e-05, 3.7182157939241733e-06, 3.3272252153437876e-07, 2.9232127829459387e-08, 2.521424624646329e-09, 2.135085025445349e-10, 1.7747796288158924e-11, 1.448133822058717e-12, 1.159801014992759e-13, 9.116807040744114e-15, 7.033323783845098e-16, 5.32488718129294e-17, 3.956077719908867e-18, 2.8840078571664658e-19, 2.062892966507638e-20, 1.4476938043255718e-21, 9.96706505540661e-23, 6.731599384296944e-24, 4.459630377631014e-25, 2.897862970848294e-26, 1.8468158237597192e-27, 1.1542594083673384e-28, 7.074312330028273e-30, 4.251405716641564e-31, 2.5050337098367833e-32, 1.4470788207618418e-33, 8.194690821903166e-35, 4.548813373585557e-36, 2.4748679700459747e-37, 1.3196412995624492e-38, 6.895579548172776e-40, 3.530711610712809e-41, 1.7712412589065688e-42, 8.688050478813866e-44, 4.203895392974451e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAGJCAYAAADIVkprAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAARZpJREFUeJzt3Qd8FNX2wPGTRiDSawCjIBZElKYg2KUJPBRf49lAnuJfBB+KPgV9gKg0ffJsIIoiKCpYsSEQKVJEkSogoDRRJBQRAgRCSOb/ORdn3U02yW6YbJn9fT+fJezs7Ozk7GbnzL3n3omzLMsSAAAAB8U7uTEAAABFggEAABxHggEAABxHggEAABxHggEAABxHggEAABxHggEAABxHggEAABxHggEAABxHggHEkNdff10aNmwoSUlJUrly5VJ9rUceeUTi4uIk0k2aNMns57Zt28K9K4CrkGAgbF/oy5YtC/euxJQNGzbIrbfeKg0aNJAJEybISy+9FO5dggNmzJhhkjmn7dy5UwYOHChXXXWVVKhQwfzNzp8/v9D1v/zyS7n00kslJSVFUlNT5V//+pccOnSowHrZ2dny4IMPSp06daRcuXLSqlUrSU9PP6ltIjKRYAAxQg8OeXl58swzz5hE4+9//3u4dyki3HLLLXLkyBE5/fTTJVoTjGHDhjm+3Y0bN8ro0aNlx44dcv755xe57qpVq6Rt27aSlZUlY8aMkdtvv90ksH/7298KrKufPV3npptuMp/FhIQE6dy5syxatKjE20RkSgz3DgCxSg/2x44dk7Jly4bk9Xbv3m1+Otk1ol/+enYZSQ4fPiynnHJKwOvrAU5v0br/paVFixby66+/StWqVeXdd98t8sD+0EMPSZUqVUwSW7FiRbOsXr160rt3b5k9e7Z06NDBLFu6dKlMnTpVnnzySbn//vvNsh49ekjjxo3lgQceMC0WwW4TkYsWDESslStXSqdOncyXS/ny5c3ZzFdffeWzTk5Ojjl7O+uss8yBulq1aqZJ1bvJNSMjQ3r16iWnnnqqJCcnS+3ateW6664LqM9duxX0TL9GjRqmOfecc86Rhx9+2OdsTL/0Aqk/0Pv9+vWTN954Q8477zyzLx9//LH5Atf9yy8zM9P8TvYXsd28PHToUDnzzDPN89PS0swXsy4viu6jPk/p76L74t2sPm7cOM8+adN13759Zf/+/T7buPLKK82BYPny5XL55ZebxEIPAsGaMmWKOXhpPPV3/8c//iE//fSTzzoLFy40B7TTTjvN83vee++9pqXBm8ZfPxubN282Z8HalK9nxt7xnj59utlv3Y7+jjNnziy2BkPj9ac//cmcVbds2dK8D2eccYa89tprBX6fb7/9Vq644grz++hn7PHHH5dXX301oLqOovY/kBjo88eOHev5fe2bdxL79NNPm99bf4datWrJ//3f/8lvv/1W7Puk+6LvT3H0c6p/bzfffLMnEbATB/3d3n77bc8yTVQ0mbvjjjs8y3S/brvtNlmyZInncxDMNhG5aMFARFq3bp1cdtll5stFD6BalPjiiy+ag9wXX3xh+m2VHiRHjhxpmk/1QKBfTFrbsWLFCmnfvr1Z5y9/+YvZ3t13320OHHomr19e27dv95sceB84dB/0tfULUdfVA4EmBcOHDy/R7zV37lzz5agHvurVq5vE6Prrr5f333/f/H5lypTxrKsHRk0c9ABsHyyuvfZac9DT/Tn33HNlzZo18r///U++//57s35h9CCjB8cPPvhAXnjhBfMlfcEFF3hiqElau3btpE+fPqZpXNf55ptvZPHixeb3t+kZrSZ9uk/65a8HrGBo3AYPHmySNn3P9uzZI88995xJWDShtFtX3nnnHdM6ovujSaOe+ep6P//8s3nM2/Hjx6Vjx44msfzvf//r06KisdLY3nXXXeaA+eyzz5rPg773ut2ibNq0Sf7617+ag1/Pnj1l4sSJ5oCuyZEesJV2H2iNgh7UBw0aZFoeXn75ZZMQBKqw/Q8kBpos/PLLL+bzrAW8+enjmkBpAqv1C1u3bpXnn3/exDr/e1tS+hnU3+HCCy/0Wa6f5aZNm5rXsun/zz77bJ+kQenfrt0toolUMNtEBLOAEHv11Vct/eh98803ha7TrVs3q0yZMtbmzZs9y3755RerQoUK1uWXX+5Z1qRJE6tLly6Fbue3334zr/Xkk08GvZ/6Ovp6P/74o8/yvLw8z/979uxpnX766QWeO3ToUPO63vR+fHy8tW7dOp/ls2bNMo99/PHHPss7d+5snXHGGZ77r7/+unn+woULfdYbP368ef7ixYuL/H3sfdqzZ49n2e7du02cO3ToYOXm5nqWP//882bdiRMnepZdccUVZpm+XiDyx2Dbtm1WQkKCNXz4cJ/11qxZYyUmJvosz8rKKrC9kSNHWnFxcT7vh8ZfX2PgwIEF1tfl+rtt2rTJs2z16tVm+XPPPVfg87h161bPMn1PddmCBQt8YpWcnGzdd999nmV333232aeVK1d6lv36669W1apVC2zTn6L2P9AY9O3bt8BnTennRJe/8cYbPstnzpzpd3lR3nnnHfOcefPmFfqYd6xsf/vb36zU1FTP/fPOO8+6+uqrC6ynfxPen61gtonIRRcJIk5ubq7pY+3WrZtplrZp18aNN95ozkq1pULpGa+2Tvzwww9+t6XN1nrWo/24gTQL2/TMesGCBfLPf/7TNFF7O5mhl9qU3qhRI59lV199tWnNmDZtmmeZ7quelXbv3t2zTM9atdVCh5nu3bvXc9Pnq3nz5gW9P59//rmpA7nnnnskPv6PrwPt59azzE8//dRnfT0z99edEwhtSdBWGG298N5/HR2gLTne+6/vm3dNgq7Xpk0bPYr6PXvVs3x/tFVGR83YtNVGf68tW7YUu7/6PmkLlk27lrSLzPu52t3SunVrc1Zt024Fu5sjUP72P9gY5Kefl0qVKpmWPO94awuMtmCV5PPij91l46/VRrs/vLt09P+Free9rWC2icgV0wmGHkC6du1q+pz1oFFUE7M/2pSszaPaTGz30f7nP/8xdQHetC9b+7T1AKl/MNpEqJXfKPzgrk3D+mWenx5g9SBl99U++uijJr4aU610//e//226Nmwab62E/+yzz8z7pE3xTzzxhKnLKIp9ENG+eyfVr1+/wLLExETTbP/hhx96ain0YKyfI+8EQ5MoTab0QOd909/du4gzGD/++KP5mT/WmpTp59l+3Fa3bl2fbpxg6P7rwVGTify/w/r16332X7swtDtCD9Z6MNR1NDlTBw4cKBA/rX3wJ39yqLRwMJBkM5Dnany0HiY/f8sKU9j+BxODwuKt69WsWbNAvHWoZ0k+L/7YiZC/OqCjR4/6JEr6/8LW895WMNtE5IrpGgw9K2jSpIk5S/3zn/8c9PO1/1KLjpo3b27OpFevXm3O/PQAOGLECLOOnh3qGYT+kWuBk35B65dSaU9yFCs0YdC6CD04a6uH9n9rTcL48eNNH7/Ss3NNJDWBnDVrlqkB0LoNrYdo1qzZSb1+Ya0Z2grjT2FfjFrToDUYmghpy43WaWhLhX4+bfq50iRKh+z5o33Xpe1kvth1/zVe+jv6G7WhB1E7dvo3s2/fPjNfgsZBaxu03kEPuLodb5pEere+eCtsdMiJHpSincxzg+Fv/4ONgT+6jn7vaFGxP5poOEFPnOx5M/LTZXoC572u/g7+1lP2usFsE5ErphMMLVbTW2E0e9YRA2+99ZY5S9azWT0b1kJDpWd43k34Oo5em+K1+tumhWH6JaHDr+yCqqIKC3Hii08L3bSFyN+oDv0y9j6Y2qMw9KZnZpp0aOGinWAobSa/7777zE3P7LRJ+6mnnjIjGvyx39e1a9cWua96Rpt/tIXKf+ZfHN1n/VLVbhIt9tPkx3u0iv07aBKro2mcmiHTnvtBY+39WdbEWAsCtYvBKbr/enDWVhy71cUfLfDTotXJkyebBN5W2GRM4aTx02LQ/PwtC0YwMSjss6Dx1i6wSy65pFTP+PV7UVthtLjae24V/Qxp0ab3Mv27064Z7eL0LvT8+uuvPY8Hu01ErpjuIimOVvrr0Ckdt63N7jpk7Jprrim0v1+/VLRP1m7GVB999JHpo9UuEm2i1z8cbd0o7AwXJ84cdYy7tkp4D/PbtWuXvPnmm+YAbH856aiG/GfB2jxtN61qV4vd/Or9xasjCooa2qlJjh70NUHUpurCzmB1W9oM7d0to2dYOlojGJo06YgFHaGiowG0gt67e0Tpl6qe/eksnPlpn7S2yAVLEwjt8tDRFd6/1yuvvGJ+ry5duohTtJVQ31sdsZK/FUDv2++l3XLgvY7+XydlijQ6+kO/I/SgZ9MTisJaDQIVTAzsOTPyJ7r6edHvmccee6zAc/Tz5S8xLgmt89DPkSbrBw8e9CzXz7Em/N7zZ+hnXPfJexZZ/TvUYb06Msw+cQhmm4hcMd2CURQ9qOiHXn/azXE6H4EmELrc7gJRWnilwyL1D0WHD2pdgHdfvp6NatGX1l1oEqJD5rR/3Z6XIFbpwTv/nASqf//+Zi4BPVvTZELjpWcz2oWgMdYaCu9CPG1R0sI1bcnQMx7titLkUOlZoJ7x65etrqvb0YO/Jiv28M/C6EFXX1+7wPR91TNvTXi08NE+oOg2tAlbh5rqMEBNaHSIp56h62ciGJpQ6DBE/VxoV4jWm+SfcVK7Tu68805zFqhnpvplra06uly7f/IP6yuOJlI6vFIP+po86zBYbc3QeTEuuugiMxTVKZqM6fuqr6dx1K4gTfS0pUTfE42x/o1pd4Cuq//XhEqTyffeey+oIt1Q0SHUehDU7gwdBm0PU9X6DU00StrSFEwM9LOv9POnCY8mJ/q51BMdHaaq3YH6edWkXVtR9QRJC0A1WdEDflH0/VJa+2Mf4O0ZN7XezHv4sX4P6mvq+6hDabWFUF9TP1c2TSI0OdDPgNaA6MmAttLo50GTWm+BbhMRLNzDWCKFhuKDDz7w3P/kk0/MslNOOcXnpsPp/v73v/s8d/v27WaY1ZtvvmnVrVvXGj16tOexs846y0pLS7OOHz/uWfbUU0/F9DAre1hgYbeffvrJrLdixQqrY8eOVvny5a2UlBTrqquusr788kufbT3++ONWy5YtrcqVK1vlypWzGjZsaIY7Hjt2zDy+d+9eM4xPl+v7V6lSJatVq1bW22+/HdC+rl271rr++uvN9suWLWudc8451uDBg33WmT17ttW4cWMzJFIfnzJlSqHDVHVfCqPDX/Wzouvp7+WP/l76+dLhfjpkskqVKlaLFi2sYcOGWQcOHAh6mKr3sFSNUVJSklWrVi2rT58+ZoivNx2mqq8bKH8xUO+995516aWXev6m9HU1Lhs3bvSs891331nt2rUz73316tWt3r17e4aY6ufHe5inbsOfwuKtQ1D1ecUNU/U3/FljoDdvOkT1sssuM+/HqaeeaoaSPvvss2abGRkZRcaoqP0PNAb63aLDZWvUqGGGsOaP+UsvvWQ+I/r3ocOuzz//fOuBBx4ww76LU9Tfqb9hsW3atDF/J7ovGvvMzMwC6x05csS6//77zXegxuyiiy4yQ2f9CXSbiExx+k+4k5xIoGcaehalZ1VK+8K11UEz9/zFXtoMr0Pr/NGzGc22tVlPn6fZt541aF+oTYvcdNY+PRsvaUU+gMilhcXa4qbN+ZE0DTkQSnSRFEJHF2jzszbjeY+FD6RyW7s/9Kd+sWgzttYN6H27Ulyb7bWgj+QCiH5a/+JdRKm1JNqVoN1rJBeIZTGdYOjZhXe1t/YFa1+l9uVrH7q2YGgFt/b7acKh8zPMmTPHTNajxW9ayKWtE9pfrkPNtP9f+xa1L90eMaIT6OjUvFpXoH202v+p9RvaXwog+mkRt9YBac2M1vZoLYGOktDh0EBMs2KYTnvrr2/R7p/V/u4hQ4ZY9erVM33TtWvXNv3x3377rXl86tSpVvPmzU0fqfajNmrUyBoxYoTpY/SmdQPa76/9jTr1s9YIeNdkAIhegwYNMrVWWuOgtUJaX5Kenh7u3QLCjhoMAADgOObBAAAAjiPBAAAAjou5Ik8dzfHLL7+YCX6cmm4ZAIBYYFmWmYZBJ6As7BpAMZtgaHIRiotCAQDgVnpF68KuYhyzCYa2XNjB8b7YzsnQeS/0Sp72VLw4ecTUecTUWcTTecQ08mOqQ7D1JN0+lhYl5hIMu1tEkwsnEwy9+qdujz8KZxBT5xFTZxFP5xHT6IlpICUGFHkCAADHkWAAAADHkWAAAADHkWAAAADHkWAAAADHkWAAAADHxdww1XDKzbNk6dZ9svvgUalZoay0rF9VEuKZTRQA4D4kGCEyc+1OGfbxd7LzwFHPstqVysrQro3kmsa1w7pvAAA4jS6SECUXfaas8EkuVMaBo2a5Pg4AgJuQYISgW0RbLiw/j9nL9HFdDwAAtyDBKGVac5G/5cKbphX6uK4HAIBbkGCUMi3odHI9AACiAQlGKdPRIk6uBwBANCDBKGU6FFVHixQ2GFWX6+O6HgAAbkGCUcp0ngsdiuqPnXTo48yHAQBwExKMENB5Ll64ubmUT/addiS1UlmznHkwAABuE9YEY8GCBdK1a1epU6eOxMXFyfTp0wN+7uLFiyUxMVGaNm0q0UCTiN6X1Tf/b92gqrzV+2JZ9ODVJBcAAFcKa4Jx+PBhadKkiYwdOzao5+3fv1969Oghbdu2lWgSH3eiG6RetfLSukE1ukUAAK4V1qnCO3XqZG7BuvPOO+XGG2+UhISEoFo9wo2ptAAAsSLqrkXy6quvypYtW2TKlCny+OOPF7t+dna2udkyMzPNz5ycHHNzgr2d4raXm5trflpWnmOv7VaBxhSBI6bOIp7OI6aRH9NgthNVCcYPP/wgAwcOlIULF5r6i0CMHDlShg0bVmD57NmzJSUlxdH9S09PL/LxH37WLpEE2b59u8yYsc3R13ar4mKK4BFTZxFP5xHTyI1pVlaW+xIMPfvXbhFNFs4+++yAnzdo0CAZMGCATwtGWlqadOjQQSpWrOhYRqdvXvv27SUpKanQ9TbP2yzy02Y57bTTpHNn/0NXEVxMEThi6izi6TxiGvkxtXsBXJVgHDx4UJYtWyYrV66Ufv36mWV5eXliWZZpzdAWiauvvrrA85KTk80tPw200x/g4raZEJ/w+894/ngCVBrvU6wjps4ins4jppEb02C2ETUJhrY2rFmzxmfZuHHjZO7cufLuu+9K/fonhoBGMosyTwBAjAhrgnHo0CHZtGmT5/7WrVtl1apVUrVqVdONoN0bO3bskNdee03i4+OlcePGPs+vWbOmlC1btsDySPf7aFUAAFwrrAmGdnlcddVVnvt2rUTPnj1l0qRJsnPnTlMQ6RYWDRgAgBgR1gTjyiuvNDUUhdEkoyiPPPKIuUWbuEIvfQYAgDtwLRIAAOA4EowQoocEABArSDDCgCJPAIDbkWCEElWeAIAYQYIRBjRgAADcjgQjhGi/AADEChKMMIijCAMA4HIkGAAAwHEkGCFEjScAIFaQYAAAAMeRYIQQV1MFAMQKEowwoMYTAOB2JBghRA0GACBWkGCEAVdTBQC4HQlGCNGAAQCIFSQYYUANBgDA7UgwAACA40gwQogiTwBArCDBCAN6SAAAbkeCEUJMtAUAiBUkGGFAkScAwO1IMEKJBgwAQIwgwQiDOJowAAAuR4IBAAAcR4IRQvSQAABiBQlGGNBBAgBwOxKMELKYaQsAECNIMMKBJgwAgMuFNcFYsGCBdO3aVerUqWNGVkyfPr3I9d9//31p37691KhRQypWrCitW7eWWbNmSbSgAQMAECvCmmAcPnxYmjRpImPHjg04IdEEY8aMGbJ8+XK56qqrTIKycuVKiSZxNGEAAFwuMZwv3qlTJ3ML1NNPP+1zf8SIEfLhhx/Kxx9/LM2aNSuFPQQAAFGXYJysvLw8OXjwoFStWrXQdbKzs83NlpmZaX7m5OSYmxPs7RS3vdy8vN/3O9ex13arQGOKwBFTZxFP5xHTyI9pMNuJsyJkaIPWYHzwwQfSrVu3gJ/zxBNPyKhRo2TDhg1Ss2ZNv+s88sgjMmzYsALL33zzTUlJSZFQ+mBbvMzfGS/t6uRJ19NPJBsAAESLrKwsufHGG+XAgQOmFtKVLRiaIGjioF0khSUXatCgQTJgwACfFoy0tDTp0KFDscEJJqNLT0839SFJSUmFrrdyxgaRndulQYMG0rnDWY68tlsFGlMEjpg6i3g6j5hGfkztXoBARGWCMXXqVLn99tvlnXfekXbt2hW5bnJysrnlp4F2+gNc3Dbj4xPMz4SEeP54AlQa71OsI6bOIp7OI6aRG9NgthF182C89dZb0qtXL/OzS5cuEk0sJgsHAMSIsLZgHDp0SDZt2uS5v3XrVlm1apUp2jzttNNM98aOHTvktdde83SL9OzZU5555hlp1aqVZGRkmOXlypWTSpUqSbTgYqoAALcLawvGsmXLzPBSe4ip1kro/4cMGWLu79y5U7Zv3+5Z/6WXXpLjx49L3759pXbt2p5b//79w/Y7AACACGvBuPLKK4u8PsekSZN87s+fP1+iWWSM1wEAoPRFXQ2GGzCTJwDA7UgwAACA40gwwoAiTwCA25FghFCETJoKAECpI8EIAxowAABuR4IRQrRfAABiBQkGAABwHAlGOFDlCQBwORKMEKLGEwAQK0gwwoD2CwCA25FghBBXUwUAxAoSjDCgBAMA4HYkGCFEDQYAIFaQYIQBFzsDALgdCQYAAHAcCUYI0UMCAIgVJBhhQJEnAMDtSDBCiCJPAECsIMEIAxowAABuR4IRUjRhAABiAwlGGFCDAQBwOxIMAADgOBKMEKLIEwAQK0gwwiCOPhIAgMuRYIQQLRgAgFhBggEAABxHghFCFsNUAQAxggQjDCjBAAC4XVgTjAULFkjXrl2lTp06pvBx+vTpxT5n/vz50rx5c0lOTpYzzzxTJk2aFJJ9BQAAUZJgHD58WJo0aSJjx44NaP2tW7dKly5d5KqrrpJVq1bJPffcI7fffrvMmjVLogFFngCAWJEYzhfv1KmTuQVq/PjxUr9+fXnqqafM/XPPPVcWLVok//vf/6Rjx44SLeK4GgkAwOXCmmAEa8mSJdKuXTufZZpYaEtGYbKzs83NlpmZaX7m5OSYmxPs7RS3vdy8vBM/c3Mde223CjSmCBwxdRbxdB4xjfyYBrOdqEowMjIypFatWj7L9L4mDUeOHJFy5coVeM7IkSNl2LBhBZbPnj1bUlJSHN2/9PT0Ih//+WftkYqX7zdukBmH1jv62m5VXEwRPGLqLOLpPGIauTHNyspyZ4JREoMGDZIBAwZ47msykpaWJh06dJCKFSs6ltHpm9e+fXtJSkoqdL35768V2fOLnNOwoXS+rL4jr+1WgcYUgSOmziKeziOmkR9TuxfAdQlGamqq7Nq1y2eZ3tdEwV/rhdLRJnrLTwPt9Ae4uG3Gx52oqU1MSOCPJ0Cl8T7FOmLqLOLpPGIauTENZhtRNQ9G69atZc6cOT7LNDPT5dGAibYAALEirAnGoUOHzHBTvdnDUPX/27dv93Rv9OjRw7P+nXfeKVu2bJEHHnhANmzYIOPGjZO3335b7r333rD9DgAAIMISjGXLlkmzZs3MTWmthP5/yJAh5v7OnTs9yYbSIaqffvqpabXQ+TN0uOrLL78cVUNUFTN5AgDcLqw1GFdeeaVYRcw+5W+WTn3OypUrJSrRQwIAiBFRVYPhFky0BQBwOxKMEKIBAwAQK0gwwoAaDACA25FghFBR9SYAALgJCQYAAHAcCQYAAHAcCUYI0UECAIgVJBhhEEeVJwDA5UgwQogaTwBArCDBCAPaLwAAbkeCEUI0YAAAYgUJRhhQggEAcDsSDAAA4DgSjBBiJk8AQKwgwQgDekgAAG5HghFCtF8AAGIFCUYYMNEWAMDtSDBCiSYMAECMIMEIAxowAABuR4IBAAAcR4IRQhZ9JACAGEGCEQb0kAAA3I4EI4SYZwsAECtIMMKBKk8AgMuRYIQQLRgAgFhBghEGtF8AANyOBAMAALgvwRg7dqzUq1dPypYtK61atZKlS5cWuf7TTz8t55xzjpQrV07S0tLk3nvvlaNHj0o0YJgqACBWhDXBmDZtmgwYMECGDh0qK1askCZNmkjHjh1l9+7dftd/8803ZeDAgWb99evXyyuvvGK28dBDD0k0ocYTAOB2YU0wxowZI71795ZevXpJo0aNZPz48ZKSkiITJ070u/6XX34pl1xyidx4442m1aNDhw5yww03FNvqESko8gQAxIrEcL3wsWPHZPny5TJo0CDPsvj4eGnXrp0sWbLE73PatGkjU6ZMMQlFy5YtZcuWLTJjxgy55ZZbCn2d7Oxsc7NlZmaanzk5OebmBHs7xW3P+j3DyMvNc+y13SrQmCJwxNRZxNN5xDTyYxrMdsKWYOzdu1dyc3OlVq1aPsv1/oYNG/w+R1su9HmXXnqpOVgfP35c7rzzziK7SEaOHCnDhg0rsHz27NmmtcRJ6enpRT6ekaENRvGydu0ambHnW0df262KiymCR0ydRTydR0wjN6ZZWVmRn2CUxPz582XEiBEybtw4UxC6adMm6d+/vzz22GMyePBgv8/RFhKt8/BuwdDiUO1eqVixomMZnb557du3l6SkpELX+/i3lbLmtz1y/vnnS+cLT3Xktd0q0JgicMTUWcTTecQ08mNq9wJEdIJRvXp1SUhIkF27dvks1/upqal+n6NJhHaH3H777ea+HqgPHz4sd9xxhzz88MOmiyW/5ORkc8tPA+30B7jYbcad2D/9vfnjCUxpvE+xjpg6i3g6j5hGbkyD2UaJijx/+ukn+fnnnz33tSbinnvukZdeeingbZQpU0ZatGghc+bM8SzLy8sz91u3bl1o00z+JEIP1t71DdGAQSQAALcrUYKhtRDz5s0z/8/IyDBNL5pkaCvCo48+GvB2tOtiwoQJMnnyZDPstE+fPqZFQkeVqB49evgUgXbt2lVeeOEFmTp1qmzdutU0+2irhi63Ew0AABB+JeoiWbt2rRnFod5++21p3LixLF682BROatHlkCFDAtpO9+7dZc+ePWZ9TVSaNm0qM2fO9BR+bt++3afF4j//+Y/ExcWZnzt27JAaNWqY5GL48OESHaKnlQUAgJAnGFo0Ytc1fP7553Lttdea/zds2FB27twZ1Lb69etnboUVdfrsbGKimWRLb9GMibYAAG5Xoi6S8847z0yKtXDhQtNNcc0115jlv/zyi1SrVs3pfXSNKCoTAQAg9AnG6NGj5cUXX5Qrr7zSzKSpU3yrjz76yNN1gsLFUeYJAHC5EnWRaGKhE17peNgqVap4lutwUacnr3ITGjAAALGiRC0YR44cMdNv28nFjz/+aK5yunHjRqlZs6bT++g+NGAAAFyuRAnGddddJ6+99pr5//79+82smk899ZR069bNDCMFApWbZ8mSzb/Kh6t2mJ96HwAQo10kemn1//3vf+b/7777rhlWunLlSnnvvffMkFOdzwIFRdNkYKEwc+1OGfbxd7LzwFHPstqVysrQro2k7TnVw7pvAIAwtGDojJoVKlQw/9e5L/785z+b+Souvvhi012CotFDciK56DNlhU9yoTIOHDXLZ63znUIeABADCcaZZ54p06dPN1OGz5o1y1w4TO3evduxC4i5Ee0XJ2g3iLZc+IuHvWz4ZxuE3hIAiLEEQ7tB7r//fqlXr54ZlmpfO0RbM5o1a+b0PrqOzkYay5Zu3Veg5cKb5hU7D2TL5szYjhMAxFwNxl//+le59NJLzayd9hwYqm3btnL99dc7uX+uQgnGCbsPFp5ceMvMKfVdAQCUkhJfrl0vqa43+6qqp556KpNsBSjWz8trVigb0HoVuVozAMRWF4leVl2vmlqpUiU5/fTTza1y5cry2GOPmceAorSsX9WMFiks0dLltSslS4OKNPkAQEwlGHpZ9ueff15GjRplhqfqbcSIEfLcc8+Zy6fDPw6XJyTEx5mhqCp/kmHff7hTQ4mP9aYeAIi1LpLJkyfLyy+/7LmKqrrgggukbt26ctddd0XR5dPDI8ZrPI1rGteWF25uLkM/Wie7MrM9y1O95sGYwYhnAIitBGPfvn3m0uz56TJ9DP4x0VbBJKPVGdWk2aPp5v5r/7xILjmzhmnhyMmhwhMAYq6LREeOaBdJfrpMWzJQNFow/pDo1Q/Ssn41k1wAAGK0BeOJJ56QLl26yOeff+6ZA2PJkiVm4q0ZM2Y4vY9wsVifEwQA3KpELRhXXHGFfP/992bOC73Ymd50uvB169bJ66+/7vxeukxczA9U/YN3JOhBAgD3KPE8GHXq1ClQzLl69Wp55ZVX5KWXXnJi3xADvBswLMbZAEBst2CgZDhDL7o1h/gAgHuQYIQBZQeFtWAAANyCBCOE6AIoGsN4ASBGazC0kLMoWuwJBIMWDABwp6ASDL32SHGP9+jR42T3ybU4QS+IGgwAcKegEoxXX3219PYkhjD3wx98QkGCAQCuQQ1GCHGGXpBvfkGAAMAtSDDCgPYL/605JGAA4B4kGAgrekgAwJ3CnmCMHTtW6tWrJ2XLlpVWrVrJ0qVLix2p0rdvX6ldu7YkJyfL2WefHTXXP6ELoJhRJDRhAIBrlHiqcCdMmzZNBgwYIOPHjzfJxdNPPy0dO3aUjRs3Ss2aNQusf+zYMWnfvr157N1335W6devKjz/+KJUrV5ZoQo1nIV0kYd0TAIBrEowxY8ZI7969pVevXua+JhqffvqpTJw4UQYOHFhgfV2+b98++fLLLyUpKcks09aPaMEJetGIDwC4R9gSDG2NWL58uQwaNMizLD4+Xtq1a2cu/e7PRx99ZC4Pr10kH374odSoUUNuvPFGefDBByUhIcHvc7Kzs83NlpmZaX7m5OSYmxPs7RS3PbsLIC8317HXdgNtxNDQnHhP4oOKKQJHTJ1FPJ1HTCM/psFsJ2wJxt69eyU3N1dq1arls1zvb9iwwe9ztmzZInPnzpWbbrrJ1F1s2rRJ7rrrLvMLDx061O9zRo4cKcOGDSuwfPbs2ZKSkiJOSk9PL/LxX3/VJChOVqxcKdZ2Ttc9rBNx+XzOHKlUJriYInjE1FnE03nENHJjmpWVFR1dJMHKy8sz9Rd6OXhtsWjRooXs2LFDnnzyyUITDG0h0ToP7xaMtLQ06dChg1SsWNGR/dIER988rQ+xu278mbLzG9l88Ddp3qyZdGqc6shru8GAr9MlN8+Sq6++WmpVLBtUTBE4Yuos4uk8Yhr5MbV7ASI6wahevbpJEnbt2uWzXO+npvo/+OrIEQ2Qd3fIueeeKxkZGabLpUyZfKe/Imakid7y0+04/QEubpt2QWNiYiJ/PF7sMs/ExILxK433KdYRU2cRT+cR08iNaTDbCNswVU0GtAVizpw5Pi0Uel/rLPy55JJLTLeIrmf7/vvvTeLhL7mIOPSK+GUPJGEYLwC4R1jnwdCuiwkTJsjkyZNl/fr10qdPHzl8+LBnVIleOM27CFQf11Ek/fv3N4mFjjgZMWKEKfqMJoxS9X/BM0aRAIB7hLUGo3v37rJnzx4ZMmSI6eZo2rSpzJw501P4uX37djOyxKa1E7NmzZJ7771XLrjgAjMPhiYbOookGnCGXghPCwYAwC3CXuTZr18/c/Nn/vz5BZZp98lXX30l0YyJtnzZ4WAmTwBwj7BPFR5LOH4WU4NBfADANUgwwoImDH81GAAA9yDBQNjRggEA7kOCEUIcP4upwSBCAOAaJBhhQJGn/wnIaMEAAPcgwQghRkkU14IBAHALEowwoAEjH08NBikGALgFCUYIcfj0jxYMAHAfEoww1hzgBGowAMB9SDAQdn/kW2QYAOAWJBghxBl6cVOFh3lHAACOIcEIAzpICukiCfeOAAAcQ4IRQhxA/aMFAwDchwQjDKjxLGSqcFIwAHANEoxQ4hS9EIwiAQC3IcEIA1owfHGxMwBwHxKMEOL46R8XOwMA9yHBCIM4xpH4oAUDANyHBANhR8IFAO5DghFCnKH7RwsGALgPCUY4cMLugxoMAHAfEowQ4gDqHxc7AwD3IcEIAxowAABuR4IRQpyhF43wAIB7kGCEsUsA+Ys8STEAwC1IMBBB1yIBALgFCUYIcYJe9DwYxAcA3IMEIwzoIPH1R48RGQYAuEVEJBhjx46VevXqSdmyZaVVq1aydOnSgJ43depUU8/QrVs3iQYcPouZB4MAAYBrhD3BmDZtmgwYMECGDh0qK1askCZNmkjHjh1l9+7dRT5v27Ztcv/998tll10m0YYaz0LmwQj3jgAAHJMoYTZmzBjp3bu39OrVy9wfP368fPrppzJx4kQZOHCg3+fk5ubKTTfdJMOGDZOFCxfK/v37C91+dna2udkyMzPNz5ycHHNzgr2d4rZn5eWZn8eP5zr22q7we9NFTs7xArEkTs4hps4ins4jppEf02C2E9YE49ixY7J8+XIZNGiQZ1l8fLy0a9dOlixZUujzHn30UalZs6bcdtttJsEoysiRI00ikt/s2bMlJSVFnJSenl7k45kHE0yHwDdLl0rm95yv2w4fPhGXJV8tkT3fBRdTBI+YOot4Oo+YRm5Ms7KyoiPB2Lt3r2mNqFWrls9yvb9hwwa/z1m0aJG88sorsmrVqoBeQ5MX7YLxbsFIS0uTDh06SMWKFcWpjE7fvPbt20tSUlKh672w5Uv5JeuQtGzVUi5pUM2R13aDZ35YLLuPHpZWrS6WVvWrBhVTBI6YOot4Oo+YRn5M7V6AqOgiCcbBgwfllltukQkTJkj16tUDek5ycrK55aeBdvoDXOw2f681SExI5I/HS3z8ibgk+IlLabxPsY6YOot4Oo+YRm5Mg9lGWBMMTRISEhJk165dPsv1fmpqaoH1N2/ebIo7u3bt6lmW93tdQ2JiomzcuFEaNGggkY4iT19cTRUA3Ceso0jKlCkjLVq0kDlz5vgkDHq/devWBdZv2LChrFmzxnSP2Ldrr71WrrrqKvN/7fqIZAzDLCbhIj4A4Bph7yLR+oiePXvKhRdeKC1btpSnn35aDh8+7BlV0qNHD6lbt64p1tR5Mho3buzz/MqVK5uf+ZdHMhowCpnJM9w7AgBwT4LRvXt32bNnjwwZMkQyMjKkadOmMnPmTE/h5/bt283IEjegC6C4i52Fe08AAK5JMFS/fv3MzZ/58+cX+dxJkyZJ1KEJwy8SMABwD3c0DcAdM3mSXwCAa5BghBAHUP+o8QQA9yHBCGNRI/LXYJBiAIBbkGCEEIfPYhKMcO8IAMAxJBhhwERbhbTokGEAgGuQYIQQXQDFtWAQHwBwCxKMMKABo5AiT/ILAHANEowQ4vhZCIapAoDrkGCEcd4HnMAwVQBwHxIMhB3DVAHAfUgwQonjp1+0YACA+5BghAE9JL6YKhwA3IcEI4Q4fvr3R75FhADALUgwwoAGDF9crh0A3IcEI4QoYix6Jk+iAwDuQYIRBtRg5EMLBgC4DgkGImgUCRkGALgFCUYIcfj0jxoMAHAfEoywoI/EGzUYAOA+JBghxBm6f8zkCQDuQ4IRBhR5+iIeAOA+JBghRBFjMV0khAcAXIMEIww4YS+ki4QEDABcgwQDEYMWDABwDxKMEOIA6h8XOwMA9yHBCOMBFSdwuXYAcB8SjBDiDN0/hqkCgPtERIIxduxYqVevnpQtW1ZatWolS5cuLXTdCRMmyGWXXSZVqlQxt3bt2hW5fiSi/cIXLRgA4D5hTzCmTZsmAwYMkKFDh8qKFSukSZMm0rFjR9m9e7ff9efPny833HCDzJs3T5YsWSJpaWnSoUMH2bFjR8j3HQ53GZFhAIBrhD3BGDNmjPTu3Vt69eoljRo1kvHjx0tKSopMnDjR7/pvvPGG3HXXXdK0aVNp2LChvPzyy5KXlydz5syRaEEJhi8udgYA7pMYzhc/duyYLF++XAYNGuRZFh8fb7o9tHUiEFlZWZKTkyNVq1b1+3h2dra52TIzM81PfY7enGBvp7jt2TUGx48fd+y13eCPuOQWiCVxcg4xdRbxdB4xjfyYBrOdsCYYe/fuldzcXKlVq5bPcr2/YcOGgLbx4IMPSp06dUxS4s/IkSNl2LBhBZbPnj3btJQ4KT09vcjHs44kmPP1xYsXy0/lHX3pqLZ7tzakxcu3a9ZI+d3fBhVTBI+YOot4Oo+YRm5M9aQ+KhKMkzVq1CiZOnWqqcvQAlF/tHVEazy8WzDsuo2KFSs6ltHpm9e+fXtJSkoqfH+/WyD7jx2VSy+5VBrXdea13eDj31bK2t/2SOPG50vni04NKqYIHDF1FvF0HjGN/JjavQARn2BUr15dEhISZNeuXT7L9X5qamqRz/3vf/9rEozPP/9cLrjggkLXS05ONrf8NNBOf4AD3WZiYiJ/PF60W8z8TIgvEJfSeJ9iHTF1FvF0HjGN3JgGs42wFnmWKVNGWrRo4VOgaRdstm7dutDnPfHEE/LYY4/JzJkz5cILLwzR3jqHIs/C5sEI954AAFwzikS7L3Rui8mTJ8v69eulT58+cvjwYTOqRPXo0cOnCHT06NEyePBgM8pE587IyMgwt0OHDkkky82z5GhOrvn/tz/vN/eR72qq4d4RAIBjwl6D0b17d9mzZ48MGTLEJAo6/FRbJuzCz+3bt3ua0NULL7xgRp/89a9/9dmOzqPxyCOPSCSauXanDPv4O/kt60T17UMfrJXn5m6SoV0byTWNa0ss00Rr3+ETo3y27jlk7ifE08QDANEu7AmG6tevn7n5owWc3rZt2ybRRJOLPlNWFDg7zzhw1Cx/4ebmMZtk2InXzgNHzf2Ji7fJZ2szTOLV9pzq4d49AEA0d5G4mZ6N6wHUX9O/vUwfj8XuEjvxspOL/InXrHW+hb8AgOhCglGKlm7dV+AA6k3TCn1c14slgSRewz/bIDGYdwGAa5BglKLdB486ul5sJV7ZsjmTWgwAiFYkGKWoZoWyjq7nFoEmVJnMFgwAUYsEoxS1rF9ValcqW+jl2XW5Pq7rxZJAE6qKzLMDAFGLBKMU6XBLHRGh8icZ9n19PNaGZQaWeCVLg4oUYQBAtCLBKGU6BFWHoqZW8j1r1/uxOkQ1kMTr4U4NJcbyLgBwFRKMENAkYtGDV3sOmONuam7ux2JyEWji1fE83yvsAgCiCwlGiGhyYQ+71C6CWOsWKSrx6tz4xIXtujapE/OJFwC4BQlGiHjP6ZBIcuGhiVZatRTz/9SKySReAOASJBghcjwvz/N/DqK+kn6/1kxOLkWdAOAWJBgh4j0deKLXxdvwR8LlnYQBAKIbR7oQOe6VYNCC4Ssp4fcEgxYMAHANEowQyfU6eFKD4SsxIb5AEgYAiG4kGCGS83vzf1ycSDwJhg874TqeSxcJALgFCUaIazBovSjIjkkOLRgA4BokGCFi1xdQf1FEFwktGADgGiQYIW/BIOSFFXl6j7QBAEQ3jnYhYhcwJv5+MMUf7KSLeTAAwD1IMEKEGozC2UkX82AAgHuQYISIffCkBqMgWjAAwH1IMEKEGozC2TnXnoPZsmTzr9RiAIALJIZ7B2KtBoMWDF8z1+6Uhz5YY/6/de9huWHCV1K7Ull5uNM54d41AMBJ4HQ6RKjB8J9c9JmyQn7LyvFZnnHgqNw9dbWs/pVYAUC0IsEIkWM5J2owso4dpxvg94Rr2Mffib8o2Mve3xYf83ECgGhFghGiM/W7p640/8/IzDbdAJeOnmuWx6qlW/fJzgNHC31c04r9x+Jk2Y+/hXS/AADOIMEIUTfAvsPHCnQD6PJYTTJ2Hzwa4HrZpb4vAADnxVmWFVNt0JmZmVKpUiU5cOCAVKxY8aS3p034CzdkyLOffCMH4k6R347kmGnB8yxL4iVODh7L89sNoLTCILVSWVn04NUxV/yp3UTaklOcU5JEkhMTREewxoslcXFxPv/X4lmNdUJcvMTHFXycdX3/n519XBKTEkQ77KJhfyN63TxLjh07LmWTk0w8S3sfoiImJ7mufitmHc2RpDIJUbG/kb5uUkK8pFVJkbT4fTKyV0c5pVxySI+hETGKZOzYsfLkk09KRkaGNGnSRJ577jlp2bJloeu/8847MnjwYNm2bZucddZZMnr0aOncubOEmrY+DHh7tWQdy9WvaxHJf1ZedO6mj2o3gXYXtG5QTWJJy/pVpXJKkuzPV+CZ3+EcvWl8ixPIOqxrGi2PWVG0v5G+brwcOZob4n0IxWuEc914yTaf0WjZ30heN1f2Hj4gKyVBPh72udxxeX0Z1LmRxEwXybRp02TAgAEydOhQWbFihUkwOnbsKLt37/a7/pdffik33HCD3HbbbbJy5Urp1q2bua1duzbkycWdU1b8nlyEprvAbWKs8QwAwka/bV9csFVGzvgudrpIWrVqJRdddJE8//zz5n5eXp6kpaXJ3XffLQMHDiywfvfu3eXw4cPyySefeJZdfPHF0rRpUxk/fnxIuki0abTNyM9l10HfuoqSeqv3xTHXghFoFwkAwDnaG7/hsU5SJjHe3V0kx44dk+XLl8ugQYM8y+Lj46Vdu3ayZMkSv8/R5dri4U1bPKZPn+53/ezsbHPzDo7Kyckxt5L4eus+x5KL8skJ0uzUCiXel2i1c//hcO8CAMScPEtk0uLN0qtNvRI9P5hjVVgTjL1790pubq7UqlXLZ7ne37Bhg9/naJ2Gv/V1uT8jR46UYcOGFVg+e/ZsSUlJKdF+L98b93vNxclrcMoxmTXzM4k1Ww44F0MAQOAWrtwgtfaXrKskKysr4HUjosizNGnriHeLh7ZgaBdMhw4dStxFUm3rPnnth2WO7N9917aMue4Ru5vpjVHz5Lcjx8O9KwAQUy5r1lA6l7AFw+4FiPgEo3r16pKQkCC7du3yWa73U1NT/T5HlwezfnJysrnll5SUZG4l0frMmlKrQpmT7iapXC5JLjm7VswNUVUa+eHXny93vXliAjIAQOnTw82tlzSQpBLWYARz3AzrKJIyZcpIixYtZM6cOZ5lWuSp91u3bu33Obrce32Vnp5e6PqlQROCYdc1PuntjPrL+TGZXNg6X1BH/u/y+uHeDQCIGb0vq1/iAs+oG6aq3RcTJkyQyZMny/r166VPnz5mlEivXr3M4z169PApAu3fv7/MnDlTnnrqKVOn8cgjj8iyZcukX79+Id3vaxrXlvE3N5eUMsHXEZRJiDPP1W3EOh2TPe7G5pIUw4kWAJQ2/Yb9vxDPgxH2Ggwddrpnzx4ZMmSIKdTU4aaaQNiFnNu3bzcjS2xt2rSRN998U/7zn//IQw89ZCba0hEkjRuffItCsDRBaN8otdCZPO1Z1XT/kxITpEGNU+SOyxvIpWfViOmWi/w6X1BbOjZOlUUb98j4BZtk0+6DciQnV47n5Eq55MSImhkv2tdlJk9m8ozkdZnJUyJ+Js+omgcj2qcKt4ftzJgxw8wmWtK6Dvgips4jps4ins4jppEf02COoWHvIgEAAO5DggEAABxHggEAABxHggEAABxHggEAABxHggEAANw3D0ao2aNyg5lPPZBhQHoBGN0mQ6ucQUydR0ydRTydR0wjP6b2sTOQGS5iLsE4ePCg+akXPAMAACU7lup8GEWJuYm29Fonv/zyi1SoUMHMduYE+wqtP/30k2OTd8U6Yuo8Yuos4uk8Yhr5MdWUQZOLOnXq+Myy7U/MtWBoQE499dRS2ba+efxROIuYOo+YOot4Oo+YRnZMi2u5sFHkCQAAHEeCAQAAHEeC4YDk5GQZOnSo+QlnEFPnEVNnEU/nEVN3xTTmijwBAEDpowUDAAA4jgQDAAA4jgQDAAA4jgQDAAA4jgTDAWPHjpV69epJ2bJlpVWrVrJ06dJw71JEGjlypFx00UVmFtWaNWtKt27dZOPGjT7rHD16VPr27SvVqlWT8uXLy1/+8hfZtWuXzzrbt2+XLl26SEpKitnOv//9bzl+/LjEulGjRpnZae+55x7PMuIZvB07dsjNN99sYlauXDk5//zzZdmyZZ7HtS5+yJAhUrt2bfN4u3bt5IcffvDZxr59++Smm24yExtVrlxZbrvtNjl06JDEotzcXBk8eLDUr1/fxKtBgwby2GOP+VzLgpgWbcGCBdK1a1cze6b+jU+fPt3ncafi9+2338pll11mjmU6++cTTzwhJ0VHkaDkpk6dapUpU8aaOHGitW7dOqt3795W5cqVrV27doV71yJOx44drVdffdVau3attWrVKqtz587WaaedZh06dMizzp133mmlpaVZc+bMsZYtW2ZdfPHFVps2bTyPHz9+3GrcuLHVrl07a+XKldaMGTOs6tWrW4MGDbJi2dKlS6169epZF1xwgdW/f3/PcuIZnH379lmnn366deutt1pff/21tWXLFmvWrFnWpk2bPOuMGjXKqlSpkjV9+nRr9erV1rXXXmvVr1/fOnLkiGeda665xmrSpIn11VdfWQsXLrTOPPNM64YbbrBi0fDhw61q1apZn3zyibV161brnXfescqXL28988wznnWIadH07/Lhhx+23n//fc3KrA8++MDncSfid+DAAatWrVrWTTfdZL6j33rrLatcuXLWiy++aJUUCcZJatmypdW3b1/P/dzcXKtOnTrWyJEjw7pf0WD37t3mj+WLL74w9/fv328lJSWZLyDb+vXrzTpLlizx/KHFx8dbGRkZnnVeeOEFq2LFilZ2drYViw4ePGidddZZVnp6unXFFVd4EgziGbwHH3zQuvTSSwt9PC8vz0pNTbWefPJJzzKNc3JysvlCVt99952J8TfffONZ57PPPrPi4uKsHTt2WLGmS5cu1j//+U+fZX/+85/NgUwR0+DkTzCcit+4ceOsKlWq+Pzd69/DOeecY5UUXSQn4dixY7J8+XLTHOV9rRO9v2TJkrDuWzQ4cOCA+Vm1alXzU2Oplxb2jmfDhg3ltNNO88RTf2qTda1atTzrdOzY0VzQZ926dRKLtAtEuzi846aIZ/A++ugjufDCC+Vvf/ub6S5q1qyZTJgwwfP41q1bJSMjwyemel0G7Rr1jqk2Qet2bLq+fjd8/fXXEmvatGkjc+bMke+//97cX716tSxatEg6depk7hPTk+NU/HSdyy+/XMqUKePzXaDd2L/99luJ9i3mLnbmpL1795r+Re8vZ6X3N2zYELb9ipar2mqtwCWXXCKNGzc2y/SPRD/c+oeQP576mL2Ov3jbj8WaqVOnyooVK+Sbb74p8BjxDN6WLVvkhRdekAEDBshDDz1k4vqvf/3LxLFnz56emPiLmXdMNTnxlpiYaBLpWIzpwIEDTcKqyW1CQoL5zhw+fLipB1DE9OQ4FT/9qXUy+bdhP1alSpWg940EA2E76167dq05k0HJ6OWX+/fvL+np6aYoC84kvnqWN2LECHNfWzD0czp+/HiTYCB4b7/9trzxxhvy5ptvynnnnSerVq0yJxdasEhM3Y0ukpNQvXp1k5Hnr8rX+6mpqWHbr0jXr18/+eSTT2TevHly6qmnepZrzLTbaf/+/YXGU3/6i7f9WCzRLpDdu3dL8+bNzdmI3r744gt59tlnzf/17IN4Bker8Bs1auSz7NxzzzUjbbxjUtTfvP7U98WbjsrRKv5YjKmOStJWjH/84x+mO+6WW26Re++914wqU8T05DgVv9L4LiDBOAnabNqiRQvTv+h9BqT3W7duHdZ9i0Ran6TJxQcffCBz584t0BynsUxKSvKJp/b/6Ze7HU/9uWbNGp8/Fj2D16FX+Q8Mbte2bVsTCz0jtG969q1Nz/b/iWdwtMsu/9BprR04/fTTzf/1M6tftt4x1eZ/7cf2jqkmdZoA2vTzrt8N2i8ea7Kyskxfvzc9MdN4KGJ6cpyKn66jw2G1bsv7u+Ccc84pUfeIUeLyUHiGqWq17qRJk0yl7h133GGGqXpX5eOEPn36mKFU8+fPt3bu3Om5ZWVl+Qyr1KGrc+fONcMqW7dubW75h1V26NDBDHWdOXOmVaNGjZgdVpmf9ygSRTyDH+6bmJhohlb+8MMP1htvvGGlpKRYU6ZM8RkSqH/jH374ofXtt99a1113nd8hgc2aNTNDXRctWmRG+cTKkMr8evbsadWtW9czTFWHWupQ6AceeMCzDjEtfqSYDiPXmx62x4wZY/7/448/OhY/HXmiw1RvueUWM0xVj2362WeYapg999xz5ktc58PQYas6zhgF6R+Gv5vOjWHTP4i77rrLDJfSD/f1119vkhBv27Ztszp16mTGaOsX1X333Wfl5OSE4TeK/ASDeAbv448/NkmXnjg0bNjQeumll3we12GBgwcPNl/Guk7btm2tjRs3+qzz66+/mi9vne9Bh/z26tXLHCRiUWZmpvlM6ndk2bJlrTPOOMPM6eA9HJKYFm3evHl+vzs1eXMyfjqHhg7T1m1oUqiJy8ngcu0AAMBx1GAAAADHkWAAAADHkWAAAADHkWAAAADHkWAAAADHkWAAAADHkWAAAADHkWAAAADHkWAAiEr16tWTp59+Oty7AaAQJBgAinXrrbdKt27dzP+vvPJKc7ntUJk0aZJUrly5wPJvvvlG7rjjjpDtB4DgJAa5PgA4Qi8lr1ckLqkaNWo4uj8AnEULBoCgWjK++OILeeaZZyQuLs7ctm3bZh5bu3atdOrUScqXLy+1atWSW265Rfbu3et5rrZ89OvXz7R+VK9eXTp27GiWjxkzRs4//3w55ZRTJC0tTe666y45dOiQeWz+/PnSq1cvOXDggOf1HnnkEb9dJHoZ+uuuu868vl5u/u9//7vs2rXL87g+r2nTpvL666+b51aqVEn+8Y9/yMGDB0MWPyCWkGAACJgmFq1bt5bevXvLzp07zU2Tgv3798vVV18tzZo1k2XLlsnMmTPNwV0P8t4mT55sWi0WL14s48ePN8vi4+Pl2WeflXXr1pnH586dKw888IB5rE2bNiaJ0ITBfr3777+/wH7l5eWZ5GLfvn0mAUpPT5ctW7ZI9+7dfdbbvHmzTJ8+XT755BNz03VHjRpVqjEDYhVdJAACpmf9miCkpKRIamqqZ/nzzz9vkosRI0Z4lk2cONEkH99//72cffbZZtlZZ50lTzzxhM82ves5tGXh8ccflzvvvFPGjRtnXktfU1suvF8vvzlz5siaNWtk69at5jXVa6+9Juedd56p1bjooos8iYjWdFSoUMHc11YWfe7w4cMdixGAE2jBAHDSVq9eLfPmzTPdE/atYcOGnlYDW4sWLQo89/PPP5e2bdtK3bp1zYFfD/q//vqrZGVlBfz669evN4mFnVyoRo0ameJQfcw7gbGTC1W7dm3ZvXt3iX5nAEWjBQPASdOaia5du8ro0aMLPKYHcZvWWXjT+o0//elP0qdPH9OKULVqVVm0aJHcdtttpghUW0qclJSU5HNfW0a0VQOA80gwAARFuy1yc3N9ljVv3lzee+8900KQmBj418ry5cvNAf6pp54ytRjq7bffLvb18jv33HPlp59+Mje7FeO7774ztSHakgEg9OgiARAUTSK+/vpr0/qgo0Q0Qejbt68psLzhhhtMzYN2i8yaNcuMACkqOTjzzDMlJydHnnvuOVOUqSM87OJP79fTFhKtldDX89d10q5dOzMS5aabbpIVK1bI0qVLpUePHnLFFVfIhRdeWCpxAFA0EgwAQdFRHAkJCaZlQOei0OGhderUMSNDNJno0KGDOdhr8abWQNgtE/40adLEDFPVrpXGjRvLG2+8ISNHjvRZR0eSaNGnjgjR18tfJGp3dXz44YdSpUoVufzyy03CccYZZ8i0adNKJQYAihdnWZYVwHoAAAABowUDAAA4jgQDAAA4jgQDAAA4jgQDAAA4jgQDAAA4jgQDAAA4jgQDAAA4jgQDAAA4jgQDAAA4jgQDAAA4jgQDAACI0/4f+xyDb55wJJcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from cs336_basics.transformer.optimizer import SGD\n",
    "\n",
    "lrs = [1e1, 1e2, 1e3]\n",
    "loss_histories = {}\n",
    "\n",
    "for lr in lrs:\n",
    "    torch.manual_seed(42)  # for reproducibility\n",
    "\n",
    "    weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "    opt = SGD([weights], lr=lr)\n",
    "    losses = []\n",
    "    for t in range(1000):\n",
    "        opt.zero_grad()  # Reset the gradients for all learnable parameters.\n",
    "        loss = (weights**2).mean()  # Compute a scalar loss value.\n",
    "        losses.append(loss.cpu().item())\n",
    "        loss.backward()  # Run backward pass, which computes gradients.\n",
    "        opt.step()  # Run optimizer step.\n",
    "    loss_histories[lr] = losses\n",
    "\n",
    "for lr, losses in loss_histories.items():\n",
    "    print(f\"Learning rate: {lr:g}, losses: {losses}\")\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(losses, marker=\"o\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Loss curve for learning rate {lr:g}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529930c2",
   "metadata": {},
   "source": [
    "### Problem (adamwAccounting): Resource accounting for training with AdamW\n",
    "Let us compute how much memory and compute running AdamW requires. Assume we are using float32 for every tensor.  \n",
    "\n",
    "[WARNING: 🆘 I HAVE NO IDEA WHAT I AM DOING!!!!]\n",
    "\n",
    "#### (a) How much peak memory does running AdamW require? Decompose your answer based on the memory usage of the parameters, activations, gradients, and optimizer state. Express your answer in terms of the batch_size and the model hyperparameters (vocab_size, context_length, num_layers, d_model, num_heads). Assume d_ff = 4 × d_model.\n",
    "For simplicity, when calculating memory usage of activations, consider only the following components:\n",
    "- Transformer block\n",
    "  - RMSNorm(s)\n",
    "  - Multi-head self-attention sublayer: $QKV$ projections, $Q^{T}K$ matrix multiply, softmax, weighted sum of values, output projection.\n",
    "  - Position-wise feed-forward: $W_1$ matrix multiply, SiLU, $W_2$ matrix multiply\n",
    "- final RMSNorm\n",
    "- output embedding\n",
    "- cross-entropy on logits\n",
    "Deliverable: An algebraic expression for each of parameters, activations, gradients, and optimizer state, as well as the total.\n",
    "\n",
    "1. parameters:\n",
    "- Embedding: $vocab\\_size \\times d\\_model$\n",
    "- Transformer blocks($num\\_layers$):\n",
    "  - RMSNorm: $d\\_model$\n",
    "  - RotaryPositionalEmbedding: $ d\\_model // num\\_heads \\times context\\_length$\n",
    "  - CausalMultiHeadSelfAttention: $d\\_model \\times (3 \\times d\\_model) + d\\_model \\times d\\_model = 4 \\times d\\_model \\times d\\_model$\n",
    "  - RMSNorm: $d\\_model$\n",
    "  - SiLUFeedForward: $d\\_model \\times d\\_ff + d\\_ff \\times d\\_model$\n",
    "- final RMSNorm: $d\\_model$\n",
    "- output embedding: $d\\_model \\times vocab\\_size$\n",
    "\n",
    "2. activations:\n",
    "NOTE: not sure this is correct\n",
    "- Transformer block($num\\_layers$):\n",
    "  - RMSNorm: $d\\_model$\n",
    "  - Multi-head self-attention sublayer: \n",
    "    - $QKV$ projections: $3 \\times context\\_length \\times d\\_model$\n",
    "    - $Q^{T}K$ matrix multiply: $num\\_heads \\times context\\_length \\times context\\_length$\n",
    "    - softmax & weighted sum of values: $num\\_heads \\times context\\_length \\times context\\_length$\n",
    "    - output projection: $context\\_length \\times d\\_model$\n",
    "  - RMSNorm: $d\\_model$\n",
    "  - SiLUFeedForward: $d\\_ff + d\\_model$\n",
    "- final RMSNorm: $d\\_model$\n",
    "- output embedding: $vocab\\_size$\n",
    "- cross-entropy on logits: $vocab\\_size$\n",
    "\n",
    "3. gradients\n",
    "Each parameter has one gradient, so total memory of gradients is the same as parameters.\n",
    "\n",
    "4. optimizer\n",
    "AdamW maintains two state variables for each parameter: the first moment (m) and the second moment (v). Both are of the same shape as the parameter, so their memory usage is twice the parameter memory usage. Additionally, AdamW maintains a single scalar state variable (t) for each parameter.\n",
    "\n",
    "#### (b) Instantiate your answer for a GPT-2 XL-shaped model to get an expression that only depends on the batch_size. What is the maximum batch size you can use and still fit within 80GB memory?\n",
    "Deliverable: An expression that looks like a · batch_size + b for numerical values a, b, and a number representing the maximum batch size.  \n",
    "- Parameters memory: $2*d\\_model*vocab\\_size + d\\_model + num\\_layers*(context\\_length*floor(d\\_model/num\\_heads) + 12*d\\_model**2 + 2*d\\_model)$  \n",
    "- Activations memory: $d\\_model + num\\_layers*(2*context\\_length**2*num\\_heads + 4*context\\_length*d\\_model + 7*d\\_model) + 2*vocab\\_size$  \n",
    "- Gradients memory: $2*d\\_model*vocab\\_size + d\\_model + num\\_layers*(context\\_length*floor(d\\_model/num\\_heads) + 12*d\\_model**2 + 2*d\\_model)$  \n",
    "- Optimizer memory: $4*d\\_model*vocab\\_size + 2*d\\_model + 2*num\\_layers*(context\\_length*floor(d\\_model/num\\_heads) + 12*d\\_model**2 + 2*d\\_model)$  \n",
    "Total memory: $batch\\_size*(d\\_model + num\\_layers*(2*context\\_length**2*num\\_heads + 4*context\\_length*d\\_model + 7*d\\_model) + 2*vocab\\_size) + 8*d\\_model*vocab\\_size + 4*d\\_model + 4*num\\_layers*(context\\_length*floor(d\\_model/num\\_heads) + 12*d\\_model**2 + 2*d\\_model)$  \n",
    "Instantiate for a GPT-2 XL-shaped model: 2831794914*batch_size + 6554733312  \n",
    "approximately 5 can use and still fit within 80GB memory\n",
    "\n",
    "#### (c) How many FLOPs does running one step of AdamW take?\n",
    "Deliverable: An algebraic expression, with a brief justification.\n",
    "\n",
    "Thrice the FLOPs of the forward pass.\n",
    "\n",
    "#### (d) Model FLOPs utilization (MFU) is defined as the ratio of observed throughput (tokens per second) relative to the hardware's theoretical peak FLOP throughput [Chowdhery et al., 2022]. An NVIDIA A100 GPU has a theoretical peak of 19.5 teraFLOP/s for float32 operations. Assuming you are able to get 50% MFU, how long would it take to train a GPT-2 XL for 400K steps and a batch size of 1024 on a single A100? Following Kaplan et al. [2020] and Hoffmann et al. [2022], assume that the backward pass has twice the FLOPs of the forward pass.\n",
    "Deliverable: The number of days training would take, with a brief justification.\n",
    "\n",
    "Let:\n",
    "- Forward pass FLOPs per batch: $F = 3,506,703,564,800$\n",
    "- Total FLOPs per step (forward + backward): $3F = 10,520,110,694,400$\n",
    "- Number of steps: $N = 400,000$\n",
    "- Batch size: $B = 1024$\n",
    "- Total FLOPs: $T = 3F \\times N \\times B$\n",
    "- A100 theoretical peak: $19.5 \\times 10^{12}$ FLOP/s\n",
    "- Actual throughput (50% MFU): $9.75 \\times 10^{12}$ FLOP/s\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "Total FLOPs:\n",
    "$$\n",
    "T = 10,520,110,694,400 \\times 400,000 \\times 1024 \\approx 4.310 \\times 10^{18}\n",
    "$$\n",
    "\n",
    "Total training time (seconds):\n",
    "$$\n",
    "\\text{Time} = \\frac{T}{9.75 \\times 10^{12}} \\approx 442,051 \\text{ seconds}\n",
    "$$\n",
    "\n",
    "Convert to days:\n",
    "$$\n",
    "\\frac{442,051}{86,400} \\approx 5.1 \\text{ days}\n",
    "$$\n",
    "\n",
    "**Justification:**  \n",
    "Training GPT-2 XL for 400K steps with batch size 1024 on a single A100 at 50% MFU would take about **5.1 days**. This is based on the total compute required (forward + backward) and the effective hardware throughput.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fbe3d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters memory: 2*d_model*vocab_size + d_model + num_layers*(context_length*floor(d_model/num_heads) + 12*d_model**2 + 2*d_model)\n",
      "Activations memory: d_model + num_layers*(2*context_length**2*num_heads + 4*context_length*d_model + 7*d_model) + 2*vocab_size\n",
      "Gradients memory: 2*d_model*vocab_size + d_model + num_layers*(context_length*floor(d_model/num_heads) + 12*d_model**2 + 2*d_model)\n",
      "Optimizer memory: 4*d_model*vocab_size + 2*d_model + 2*num_layers*(context_length*floor(d_model/num_heads) + 12*d_model**2 + 2*d_model)\n",
      "Total memory: batch_size*(d_model + num_layers*(2*context_length**2*num_heads + 4*context_length*d_model + 7*d_model) + 2*vocab_size) + 8*d_model*vocab_size + 4*d_model + 4*num_layers*(context_length*floor(d_model/num_heads) + 12*d_model**2 + 2*d_model)\n",
      "Config: ModelConfig(name='GPT2_XL', vocab_size=50257, context_length=1024, num_layers=48, d_model=1600, num_heads=25, d_ff=6400)\n",
      "Result: 2831794914*batch_size + 6554733312\n",
      "Max batch size: 7460051584/1415897457, approximately 5\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "ModelConfig = namedtuple(\n",
    "    \"ModelConfig\", [\"name\", \"vocab_size\", \"context_length\", \"num_layers\", \"d_model\", \"num_heads\", \"d_ff\"]\n",
    ")\n",
    "GPT2_XL = ModelConfig(\n",
    "    name=\"GPT2_XL\", vocab_size=50257, context_length=1024, num_layers=48, d_model=1600, num_heads=25, d_ff=6400\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_parameters_memory(vocab_size, d_model, num_layers, num_heads, d_ff, context_length):\n",
    "    embedding = vocab_size * d_model\n",
    "\n",
    "    rmsnorm1 = d_model\n",
    "    rotary_pos_emb = (d_model // num_heads) * context_length\n",
    "    causal_attn = 4 * d_model * d_model\n",
    "    rmsnorm2 = d_model\n",
    "    silu_ff = d_model * d_ff + d_ff * d_model\n",
    "    transformer_block = rmsnorm1 + rotary_pos_emb + causal_attn + rmsnorm2 + silu_ff\n",
    "\n",
    "    final_rmsnorm = d_model\n",
    "    output_emb = d_model * vocab_size\n",
    "\n",
    "    total_params = embedding + num_layers * transformer_block + final_rmsnorm + output_emb\n",
    "    return total_params\n",
    "\n",
    "\n",
    "# activation memory is associated with batch size\n",
    "def calculate_activations_memory(vocab_size, d_model, num_layers, num_heads, d_ff, context_length):\n",
    "    rmsnorm1 = d_model\n",
    "    qkv_projections = 3 * context_length * d_model\n",
    "    qk_t = num_heads * context_length * context_length\n",
    "    attention_scores = num_heads * context_length * context_length\n",
    "    output_projection = context_length * d_model\n",
    "    rmsnorm2 = d_model\n",
    "    silu_ff = d_ff + d_model\n",
    "\n",
    "    transformer_block = rmsnorm1 + qkv_projections + qk_t + attention_scores + output_projection + rmsnorm2 + silu_ff\n",
    "\n",
    "    final_rmsnorm = d_model\n",
    "    output_emb = vocab_size\n",
    "    cross_entropy = vocab_size\n",
    "\n",
    "    total_activations = num_layers * transformer_block + final_rmsnorm + output_emb + cross_entropy\n",
    "    return total_activations\n",
    "\n",
    "\n",
    "def calculate_gradients_memory(vocab_size, d_model, num_layers, num_heads, d_ff, context_length):\n",
    "    return calculate_parameters_memory(vocab_size, d_model, num_layers, num_heads, d_ff, context_length)\n",
    "\n",
    "\n",
    "def calculate_optimizer_memory(vocab_size, d_model, num_layers, num_heads, d_ff, context_length):\n",
    "    param_memory = calculate_parameters_memory(vocab_size, d_model, num_layers, num_heads, d_ff, context_length)\n",
    "    return 2 * param_memory  # memory for `t` is left out\n",
    "\n",
    "\n",
    "vocab_size, d_model, num_layers, num_heads, d_ff, context_length = sp.symbols(\n",
    "    \"vocab_size d_model num_layers num_heads d_ff context_length\"\n",
    ")\n",
    "d_ff = 4 * d_model\n",
    "parameters_memory = calculate_parameters_memory(vocab_size, d_model, num_layers, num_heads, d_ff, context_length)\n",
    "print(f\"Parameters memory: {sp.simplify(parameters_memory)}\")\n",
    "\n",
    "activations_memory = calculate_activations_memory(vocab_size, d_model, num_layers, num_heads, d_ff, context_length)\n",
    "print(f\"Activations memory: {sp.simplify(activations_memory)}\")\n",
    "\n",
    "gradients_memory = calculate_gradients_memory(vocab_size, d_model, num_layers, num_heads, d_ff, context_length)\n",
    "print(f\"Gradients memory: {sp.simplify(gradients_memory)}\")\n",
    "\n",
    "optimizer_memory = calculate_optimizer_memory(vocab_size, d_model, num_layers, num_heads, d_ff, context_length)\n",
    "print(f\"Optimizer memory: {sp.simplify(optimizer_memory)}\")\n",
    "\n",
    "batch_size = sp.symbols(\"batch_size\")\n",
    "total_memory = parameters_memory + batch_size * activations_memory + gradients_memory + optimizer_memory\n",
    "print(f\"Total memory: {sp.simplify(total_memory)}\")\n",
    "\n",
    "for config in [GPT2_XL]:\n",
    "    print(f\"Config: {config}\")\n",
    "    result = total_memory.subs(\n",
    "        {\n",
    "            vocab_size: config.vocab_size,\n",
    "            d_model: config.d_model,\n",
    "            num_layers: config.num_layers,\n",
    "            num_heads: config.num_heads,\n",
    "            d_ff: config.d_ff,\n",
    "            context_length: config.context_length,\n",
    "        }\n",
    "    )\n",
    "    print(f\"Result: {result}\")\n",
    "    max_mem_bytes = 80 * 1024**3\n",
    "    eq = sp.Eq(4 * result, max_mem_bytes)\n",
    "    sol = sp.solve(eq, batch_size)\n",
    "    print(f\"Max batch size: {sol[0]}, approximately {int(sol[0])}\")\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from cs336_basics.bpe_tokenizer.encoder import load_bpe_tokenizer\n",
    "\n",
    "\n",
    "data_path = f\"data/TinyStoriesV2-GPT4-train_encoded_10k.npy\"\n",
    "\n",
    "vocab_filepath = \"data/TinyStoriesV2-GPT4_vocab.json\"\n",
    "merge_filepath = \"data/TinyStoriesV2-GPT4_merges.txt\"\n",
    "\n",
    "tokenizer = load_bpe_tokenizer(\n",
    "    vocab_path=vocab_filepath,\n",
    "    merge_path=merge_filepath,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "array = np.memmap(data_path, dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "assert array.max() < 10000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
