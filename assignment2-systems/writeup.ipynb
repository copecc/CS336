{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03dd4a2d",
   "metadata": {},
   "source": [
    "### Problem (benchmarking_script)\n",
    "\n",
    "#### (b) Time the forward and backward passes for the model sizes described in §1.1.2. Use 5 warmup steps and compute the average and standard deviation of timings over 10 measurement steps. How long does a forward pass take? How about a backward pass? Do you see high variability across measurements, or is the standard deviation small?\n",
    "\n",
    "    model  forward_mean  forward_std  backward_mean  backward_std\n",
    "0   small      0.162622     0.000616       0.325898      0.000811\n",
    "1  medium      0.520552     0.002540       1.003874      0.004929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a816536",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 cs336-basics/benchmark/benchmarking.py\n",
    "\n",
    "!nsys profile -o result python cs336-basics/benchmark/benchmark.py\n",
    "!nsys profile -o result_annotated python cs336-basics/benchmark/benchmarking.py --use_annotated\n",
    "\n",
    "!python3 cs336-basics/benchmark/benchmarking.py --profile_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc7c76",
   "metadata": {},
   "source": [
    "### Problem (mixed_precision_accumulation)\n",
    "Run the following code and commment on the (accuracy of the) results.\n",
    "\n",
    "The results show that using float16 for accumulation leads to significant precision loss (resulting in 9.9531 instead of 10), while float32 accumulation maintains high accuracy (close to 10). When float16 values are accumulated in float32, the accuracy is nearly as good as pure float32, demonstrating why mixed-precision training uses float32 accumulators to preserve numerical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7879ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0001)\n",
      "tensor(9.9531, dtype=torch.float16)\n",
      "tensor(10.0021)\n",
      "tensor(10.0021)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float32)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float16)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01, dtype=torch.float16)\n",
    "print(s)\n",
    "\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    x = torch.tensor(0.01, dtype=torch.float16)\n",
    "    s += x.type(torch.float32)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83af566",
   "metadata": {},
   "source": [
    "### Problem (benchmarking_mixed_precision)\n",
    "\n",
    "#### (a) Consider the following model:\n",
    "```python\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.ln(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "Suppose we are training the model on a GPU and that the model parameters are originally in\n",
    "FP32. We’d like to use autocasting mixed precision with FP16. What are the data types of:\n",
    "• the model parameters within the autocast context,\n",
    "• the output of the first feed-forward layer (ToyModel.fc1),\n",
    "• the output of layer norm (ToyModel.ln),\n",
    "• the model’s predicted logits,\n",
    "• the loss,\n",
    "• and the model’s gradients?\n",
    "\n",
    "#### You should have seen that FP16 mixed precision autocasting treats the layer normalization layer differently than the feed-forward layers. What parts of layer normalization are sensitive to mixed precision? If we use BF16 instead of FP16, do we still need to treat layer normalization differently? Why or why not?\n",
    "\n",
    "Layer normalization is sensitive to mixed precision because its mean and variance computations can suffer significant numerical errors in FP16 due to limited precision, which can destabilize training. Therefore, these operations are typically performed in FP32 even during mixed precision training. If using BF16, which has a much larger dynamic range than FP16, layer normalization is generally stable and does not require special treatment, so it can safely run in BF16.\n",
    "\n",
    "#### (c) Modify your benchmarking script to optionally run the model using mixed precision with BF16. Time the forward and backward passes with and without mixed-precision for each language model size described in §1.1.2. Compare the results of using full vs. mixed precision, and comment on any trends as model size changes. You may find the nullcontext no-op context manager to be useful.\n",
    "Deliverable: A 2-3 sentence response with your timings and commentary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a1cc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model param dtype (in autocast): torch.float32\n",
      "fc1.weight dtype (model param): torch.float32\n",
      "fc1 output dtype: torch.float16\n",
      "layernorm output dtype: torch.float32\n",
      "logits dtype: torch.float16\n",
      "loss dtype: torch.float32\n",
      "model param grad dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"fc1.weight dtype (model param):\", self.fc1.weight.dtype)\n",
    "        x1 = self.fc1(x)\n",
    "        print(\"fc1 output dtype:\", x1.dtype)\n",
    "        x2 = self.relu(x1)\n",
    "        x3 = self.ln(x2)\n",
    "        print(\"layernorm output dtype:\", x3.dtype)\n",
    "        x4 = self.fc2(x3)\n",
    "        print(\"logits dtype:\", x4.dtype)\n",
    "        return x4\n",
    "\n",
    "\n",
    "model = ToyModel(5, 2).cuda()\n",
    "x = torch.randn(3, 5, device=\"cuda\")\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    print(\"model param dtype (in autocast):\", model.fc1.weight.dtype)\n",
    "    logits = model(x)\n",
    "    loss = logits.sum()\n",
    "    print(\"loss dtype:\", loss.dtype)\n",
    "\n",
    "    loss.backward()\n",
    "    print(\"model param grad dtype:\", model.fc1.weight.grad.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-systems (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
